{"cells":[{"cell_type":"markdown","metadata":{"id":"bpJ7s_SIVu_I"},"source":["# **Trabajo Práctico Final**: Linear/Quadratic Discriminant Analysis (LDA/QDA)\n","___________________________________________________________________________\n","## *Analísis Mátematico para Inteligencia Artificial*                                   \n","## *Facultad de Ingeniería de la Universidad de Buenos Aires*                                                 \n","## *B. Massoller, J. D. Canal, J. C. Ferreyra,S. Rodríguez, Y. P. Arrieta Echavez*\n"]},{"cell_type":"markdown","metadata":{"id":"BsDYvwFEEKV5"},"source":["---\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"1Yb1V7_yXRfO"},"source":["# 1 Implementación base\n","1. Entrenar un modelo QDA sobre el dataset *iris* utilizando las distribuciones *a priori* a continuación ¿Se observan diferencias?¿Por qué cree? _Pista: comparar con las distribuciones del dataset completo, **sin splitear**_.\n","    1. Uniforme (cada clase tiene probabilidad 1/3)\n","    2. Una clase con probabilidad 0.9, las demás 0.05 (probar las 3 combinaciones)\n","2. Repetir el punto anterior para el dataset *penguin*.\n","3. Implementar el modelo LDA, entrenarlo y testearlo contra los mismos sets que QDA (no múltiples prioris) ¿Se observan diferencias? ¿Podría decirse que alguno de los dos es notoriamente mejor que el otro?\n","4. Utilizar otros 2 (dos) valores de *random seed* para obtener distintos splits de train y test, y repetir la comparación del punto anterior ¿Las conclusiones previas se mantienen?\n","5. Estimar y comparar los tiempos de predicción de las clases `QDA` y `TensorizedQDA`. De haber diferencias ¿Cuáles pueden ser las causas?\n","\n","\n","**Sugerencia:** puede resultar de utilidad para cada inciso de comparación utilizar tablas del siguiente estilo:\n","\n","<center>\n","\n","Modelo | Dataset | Seed | Error (train) | Error (test)\n",":---: | :---: | :---: | :---: | :---:\n","QDA | Iris | 125 | 0.55 | 0.85\n","LDA | Iris | 125 | 0.22 | 0.8\n","\n","</center>\n"]},{"cell_type":"markdown","metadata":{"id":"MnOZk1L9MsgM"},"source":["***Resolución***\n","\n","---"]},{"cell_type":"markdown","metadata":{"id":"fkkZBz-BP0ha"},"source":["## ***Definición herramientas***"]},{"cell_type":"code","execution_count":1,"metadata":{"executionInfo":{"elapsed":1884,"status":"ok","timestamp":1713478588535,"user":{"displayName":"Simón Rodríguez","userId":"08248044722851115137"},"user_tz":-120},"id":"FtpNMJSzP1mb"},"outputs":[],"source":["import numpy as np\n","import pandas as pd\n","from numpy.linalg import det, inv\n","from sklearn.datasets import load_iris, fetch_openml\n","from sklearn.model_selection import train_test_split\n","\n","class ClassEncoder:\n","  def fit(self, y):\n","    self.names = np.unique(y)\n","    self.name_to_class = {name:idx for idx, name in enumerate(self.names)}\n","    self.fmt = y.dtype\n","    # Q1: por que no hace falta definir un class_to_name para el mapeo inverso?\n","\n","  def _map_reshape(self, f, arr):\n","    return np.array([f(elem) for elem in arr.flatten()]).reshape(arr.shape)\n","    # Q2: por que hace falta un reshape?\n","\n","  def transform(self, y):\n","    return self._map_reshape(lambda name: self.name_to_class[name], y)\n","\n","  def fit_transform(self, y):\n","    self.fit(y)\n","    return self.transform(y)\n","\n","  def detransform(self, y_hat):\n","    return self._map_reshape(lambda idx: self.names[idx], y_hat)\n","\n","class BaseBayesianClassifier:\n","  def __init__(self):\n","    self.encoder = ClassEncoder()\n","\n","  def _estimate_a_priori(self, y):\n","    a_priori = np.bincount(y.flatten().astype(int)) / y.size\n","    # Q3: para que sirve bincount?\n","    return np.log(a_priori)\n","\n","  def _fit_params(self, X, y):\n","    # estimate all needed parameters for given model\n","    raise NotImplementedError()\n","\n","  def _predict_log_conditional(self, x, class_idx):\n","    # predict the log(P(x|G=class_idx)), the log of the conditional probability of x given the class\n","    # this should depend on the model used\n","    raise NotImplementedError()\n","\n","  def fit(self, X, y, a_priori=None):\n","    # first encode the classes\n","    y = self.encoder.fit_transform(y)\n","\n","    # if it's needed, estimate a priori probabilities\n","    self.log_a_priori = self._estimate_a_priori(y) if a_priori is None else np.log(a_priori)\n","\n","    # check that a_priori has the correct number of classes\n","    assert len(self.log_a_priori) == len(self.encoder.names), \"A priori probabilities do not match number of classes\"\n","\n","    # now that everything else is in place, estimate all needed parameters for given model\n","    self._fit_params(X, y)\n","    # Q4: por que el _fit_params va al final? no se puede mover a, por ejemplo, antes de la priori?\n","\n","  def predict(self, X):\n","    # this is actually an individual prediction encased in a for-loop\n","    m_obs = X.shape[1]\n","    y_hat = np.empty(m_obs, dtype=self.encoder.fmt)\n","\n","    for i in range(m_obs):\n","      # Para cada observación hace:\n","      # Obtiene la predicción de dicha observación y la guarda en un arreglo y_hat.\n","      # La predicción se obtiene calculando, para cada clase, la función log f_i y tomando el\n","      # mayor.\n","      encoded_y_hat_i = self._predict_one(X[:,i].reshape(-1,1))\n","      y_hat[i] = self.encoder.names[encoded_y_hat_i]\n","\n","    # return prediction as a row vector (matching y)\n","    return y_hat.reshape(1,-1)\n","\n","  def _predict_one(self, x):\n","    # calculate all log posteriori probabilities (actually, +C)\n","    log_posteriori = [ log_a_priori_i + self._predict_log_conditional(x, idx) for idx, log_a_priori_i\n","                  in enumerate(self.log_a_priori) ]\n","\n","    # return the class that has maximum a posteriori probability\n","    return np.argmax(log_posteriori)\n","\n","\n","class QDA(BaseBayesianClassifier):\n","\n","  def _fit_params(self, X, y):\n","    # estimate each covariance matrix\n","    self.inv_covs = [inv(np.cov(X[:,y.flatten()==idx], bias=True))\n","                      for idx in range(len(self.log_a_priori))]\n","    # Q5: por que hace falta el flatten y no se puede directamente X[:,y==idx]?\n","    # Q6: por que se usa bias=True en vez del default bias=False?\n","    \n","    self.means = [X[:,y.flatten()==idx].mean(axis=1, keepdims=True)\n","                  for idx in range(len(self.log_a_priori))]\n","    # Q7: que hace axis=1? por que no axis=0?\n","\n","  def _predict_log_conditional(self, x, class_idx):\n","    # predict the log(P(x|G=class_idx)), the log of the conditional probability of x given the class\n","    # this should depend on the model used\n","    inv_cov = self.inv_covs[class_idx]\n","    unbiased_x =  x - self.means[class_idx]\n","    return 0.5*np.log(det(inv_cov)) -0.5 * unbiased_x.T @ inv_cov @ unbiased_x\n","\n","class TensorizedQDA(QDA):\n","\n","    def _fit_params(self, X, y):\n","        # ask plain QDA to fit params\n","        super()._fit_params(X,y)\n","\n","        # stack onto new dimension\n","        self.tensor_inv_cov = np.stack(self.inv_covs)\n","        self.tensor_means = np.stack(self.means)\n","\n","    def _predict_log_conditionals(self,x):\n","        unbiased_x = x - self.tensor_means\n","        inner_prod = unbiased_x.transpose(0,2,1) @ self.tensor_inv_cov @ unbiased_x\n","\n","        return 0.5*np.log(det(self.tensor_inv_cov)) - 0.5 * inner_prod.flatten()\n","\n","    def _predict_one(self, x):\n","        # return the class that has maximum a posteriori probability\n","        return np.argmax(self.log_a_priori + self._predict_log_conditionals(x))"]},{"cell_type":"code","execution_count":2,"metadata":{"executionInfo":{"elapsed":6,"status":"ok","timestamp":1713478588536,"user":{"displayName":"Simón Rodríguez","userId":"08248044722851115137"},"user_tz":-120},"id":"FTu1FE-6P6OM"},"outputs":[],"source":["# hiperparámetros\n","rng_seed = 6543"]},{"cell_type":"code","execution_count":3,"metadata":{"executionInfo":{"elapsed":6,"status":"ok","timestamp":1713478588536,"user":{"displayName":"Simón Rodríguez","userId":"08248044722851115137"},"user_tz":-120},"id":"DuYDUMwEP8eb"},"outputs":[],"source":["#Descarga de datos\n","\n","def get_iris_dataset():\n","  data = load_iris()\n","  X_full = data.data\n","  y_full = np.array([data.target_names[y] for y in data.target.reshape(-1,1)])\n","  return X_full, y_full\n","\n","def get_penguins():\n","    # get data\n","    df, tgt = fetch_openml(name=\"penguins\", return_X_y=True, as_frame=True)\n","\n","    # drop non-numeric columns\n","    df.drop(columns=[\"island\",\"sex\"], inplace=True)\n","\n","    # drop rows with missing values\n","    mask = df.isna().sum(axis=1) == 0\n","    df = df[mask]\n","    tgt = tgt[mask]\n","\n","    return df.values, tgt.to_numpy().reshape(-1,1)"]},{"cell_type":"code","execution_count":4,"metadata":{"executionInfo":{"elapsed":5,"status":"ok","timestamp":1713478588536,"user":{"displayName":"Simón Rodríguez","userId":"08248044722851115137"},"user_tz":-120},"id":"hMJDnCQwP-gV"},"outputs":[],"source":["# preparing data, train - test validation\n","# 70-30 split\n","\n","def split_transpose(X, y, test_sz, random_state):\n","    # split\n","    X_train, X_test, y_train, y_test = train_test_split(X_full, y_full, test_size=0.4, random_state=random_state)\n","\n","    # transpose so observations are column vectors\n","    return X_train.T, y_train.T, X_test.T, y_test.T\n","\n","def accuracy(y_true, y_pred):\n","  return (y_true == y_pred).mean()"]},{"cell_type":"markdown","metadata":{"id":"ho-jvdLTQQLN"},"source":["## ***1.1 Punto 1***"]},{"cell_type":"code","execution_count":5,"metadata":{"executionInfo":{"elapsed":5,"status":"ok","timestamp":1713478588536,"user":{"displayName":"Simón Rodríguez","userId":"08248044722851115137"},"user_tz":-120},"id":"tSyCzW4sQpoh"},"outputs":[],"source":["def try_prioris(X, y, random_seed, a_prioris, classf, data):\n","  # Utiliza el método train_test_split pero devuelve los datos de forma traspuesta. O sea, las observaciones\n","  # están a lo largo de una columna y las filas indican los valores por clase.\n","  train_x, train_y, test_x, test_y = split_transpose(X, y, 0.4, random_seed)\n","\n","  if classf == 'QDA':\n","    print(\"QDA\")\n","    M = QDA()\n","  else:\n","    print(\"LDA\")\n","    M = LDA()\n","\n","  # Iterate over each priori distribution and save results in a dataframe.\n","  results = []\n","  for k, a_priori in a_prioris.items():\n","    M.fit(train_x, train_y, a_priori=a_priori)\n","    acc_train = accuracy(train_y, M.predict(train_x))\n","    acc_test = accuracy(test_y, M.predict(test_x))\n","\n","    res = {\n","      \"Modelo\": classf,\n","      \"Dataset\": data,\n","      \"Seed\": random_seed,\n","      \"Priori\":k,\n","      \"Error(Train)\":1-acc_train,\n","      \"Error(Test)\":1-acc_test\n","    }\n","\n","    test_y_encoded = M.encoder.transform(test_y)\n","\n","    for class_id in range(len(M.encoder.names)):\n","      mask = test_y_encoded == class_id\n","      acc_class = accuracy(test_y[:, mask.flatten()], M.predict(test_x[:, mask.flatten()]))\n","      res[f\"ErrorClase{class_id}\"] = 1-acc_class\n","\n","    results.append(res)\n","\n","  df_results = pd.DataFrame(results)\n","\n","  return df_results"]},{"cell_type":"code","execution_count":6,"metadata":{"executionInfo":{"elapsed":5,"status":"ok","timestamp":1713478588536,"user":{"displayName":"Simón Rodríguez","userId":"08248044722851115137"},"user_tz":-120},"id":"Z8bMZR3zQs_a"},"outputs":[],"source":["# Descargamos los datos de iris dataset y spliteamos en train y test\n","random_seed = 42\n","\n","a_prioris = {\n","    \"sin_priori\" : None,\n","    \"uniforme\" : [(1/3), (1/3), (1/3)],\n","    \"clase0_90\": [.9, .05, .05],\n","    \"clase1_90\": [.05, .9, .05],\n","    \"clase2_90\": [.05, .05, .9]\n","}"]},{"cell_type":"code","execution_count":7,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":223},"executionInfo":{"elapsed":417,"status":"ok","timestamp":1713478588949,"user":{"displayName":"Simón Rodríguez","userId":"08248044722851115137"},"user_tz":-120},"id":"iwVepU6WQvgh","outputId":"698646d1-30be-42c1-e41f-634b3176f854"},"outputs":[{"name":"stdout","output_type":"stream","text":["QDA\n"]},{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Modelo</th>\n","      <th>Dataset</th>\n","      <th>Seed</th>\n","      <th>Priori</th>\n","      <th>Error(Train)</th>\n","      <th>Error(Test)</th>\n","      <th>ErrorClase0</th>\n","      <th>ErrorClase1</th>\n","      <th>ErrorClase2</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>QDA</td>\n","      <td>Iris</td>\n","      <td>42</td>\n","      <td>sin_priori</td>\n","      <td>0.011111</td>\n","      <td>0.016667</td>\n","      <td>0.0</td>\n","      <td>0.000000</td>\n","      <td>0.055556</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>QDA</td>\n","      <td>Iris</td>\n","      <td>42</td>\n","      <td>uniforme</td>\n","      <td>0.011111</td>\n","      <td>0.016667</td>\n","      <td>0.0</td>\n","      <td>0.000000</td>\n","      <td>0.055556</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>QDA</td>\n","      <td>Iris</td>\n","      <td>42</td>\n","      <td>clase0_90</td>\n","      <td>0.011111</td>\n","      <td>0.016667</td>\n","      <td>0.0</td>\n","      <td>0.000000</td>\n","      <td>0.055556</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>QDA</td>\n","      <td>Iris</td>\n","      <td>42</td>\n","      <td>clase1_90</td>\n","      <td>0.044444</td>\n","      <td>0.050000</td>\n","      <td>0.0</td>\n","      <td>0.000000</td>\n","      <td>0.166667</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>QDA</td>\n","      <td>Iris</td>\n","      <td>42</td>\n","      <td>clase2_90</td>\n","      <td>0.044444</td>\n","      <td>0.016667</td>\n","      <td>0.0</td>\n","      <td>0.052632</td>\n","      <td>0.000000</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["  Modelo Dataset  Seed      Priori  Error(Train)  Error(Test)  ErrorClase0  \\\n","0    QDA    Iris    42  sin_priori      0.011111     0.016667          0.0   \n","1    QDA    Iris    42    uniforme      0.011111     0.016667          0.0   \n","2    QDA    Iris    42   clase0_90      0.011111     0.016667          0.0   \n","3    QDA    Iris    42   clase1_90      0.044444     0.050000          0.0   \n","4    QDA    Iris    42   clase2_90      0.044444     0.016667          0.0   \n","\n","   ErrorClase1  ErrorClase2  \n","0     0.000000     0.055556  \n","1     0.000000     0.055556  \n","2     0.000000     0.055556  \n","3     0.000000     0.166667  \n","4     0.052632     0.000000  "]},"execution_count":7,"metadata":{},"output_type":"execute_result"}],"source":["X_full, y_full = get_iris_dataset()\n","\n","try_prioris(X_full, y_full, random_seed, a_prioris, 'QDA','Iris')"]},{"cell_type":"code","execution_count":8,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5,"status":"ok","timestamp":1713478588949,"user":{"displayName":"Simón Rodríguez","userId":"08248044722851115137"},"user_tz":-120},"id":"gj2eNIVgQ2qZ","outputId":"52128acf-faff-4378-c0df-934b5ddcf4ea"},"outputs":[{"name":"stdout","output_type":"stream","text":["\n","Distribución original:  [0.33333333 0.33333333 0.33333333]\n"]}],"source":["# Observamos cuál es la distribución del dataset completo.\n","encoder = ClassEncoder()\n","\n","y_full_encoded = encoder.fit_transform(y_full)\n","original_distribution = np.bincount(y_full_encoded.flatten().astype(int)) / y_full_encoded.size\n","\n","print(\"\\nDistribución original: \", original_distribution)"]},{"cell_type":"markdown","metadata":{"id":"AEcbJHujR-BF"},"source":["## ***1.2 Punto 2***"]},{"cell_type":"code","execution_count":9,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":278},"executionInfo":{"elapsed":5063,"status":"ok","timestamp":1713478594009,"user":{"displayName":"Simón Rodríguez","userId":"08248044722851115137"},"user_tz":-120},"id":"_OlVp5M-R_Vt","outputId":"0939fb81-d9d7-49b6-f87a-6a194bc1368f"},"outputs":[{"name":"stdout","output_type":"stream","text":["QDA\n"]},{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Modelo</th>\n","      <th>Dataset</th>\n","      <th>Seed</th>\n","      <th>Priori</th>\n","      <th>Error(Train)</th>\n","      <th>Error(Test)</th>\n","      <th>ErrorClase0</th>\n","      <th>ErrorClase1</th>\n","      <th>ErrorClase2</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>QDA</td>\n","      <td>Penguins</td>\n","      <td>42</td>\n","      <td>sin_priori</td>\n","      <td>0.009756</td>\n","      <td>0.007299</td>\n","      <td>0.015625</td>\n","      <td>0.000000</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>QDA</td>\n","      <td>Penguins</td>\n","      <td>42</td>\n","      <td>uniforme</td>\n","      <td>0.009756</td>\n","      <td>0.007299</td>\n","      <td>0.015625</td>\n","      <td>0.000000</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>QDA</td>\n","      <td>Penguins</td>\n","      <td>42</td>\n","      <td>clase0_90</td>\n","      <td>0.024390</td>\n","      <td>0.007299</td>\n","      <td>0.000000</td>\n","      <td>0.041667</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>QDA</td>\n","      <td>Penguins</td>\n","      <td>42</td>\n","      <td>clase1_90</td>\n","      <td>0.029268</td>\n","      <td>0.029197</td>\n","      <td>0.062500</td>\n","      <td>0.000000</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>QDA</td>\n","      <td>Penguins</td>\n","      <td>42</td>\n","      <td>clase2_90</td>\n","      <td>0.009756</td>\n","      <td>0.007299</td>\n","      <td>0.015625</td>\n","      <td>0.000000</td>\n","      <td>0.0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["  Modelo   Dataset  Seed      Priori  Error(Train)  Error(Test)  ErrorClase0  \\\n","0    QDA  Penguins    42  sin_priori      0.009756     0.007299     0.015625   \n","1    QDA  Penguins    42    uniforme      0.009756     0.007299     0.015625   \n","2    QDA  Penguins    42   clase0_90      0.024390     0.007299     0.000000   \n","3    QDA  Penguins    42   clase1_90      0.029268     0.029197     0.062500   \n","4    QDA  Penguins    42   clase2_90      0.009756     0.007299     0.015625   \n","\n","   ErrorClase1  ErrorClase2  \n","0     0.000000          0.0  \n","1     0.000000          0.0  \n","2     0.041667          0.0  \n","3     0.000000          0.0  \n","4     0.000000          0.0  "]},"execution_count":9,"metadata":{},"output_type":"execute_result"}],"source":["# Descargamos los datos de penguin dataset y spliteamos en train y test\n","X_full, y_full = get_penguins()\n","\n","try_prioris(X_full, y_full, random_seed, a_prioris,\"QDA\",\"Penguins\")"]},{"cell_type":"code","execution_count":10,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":404,"status":"ok","timestamp":1713478594411,"user":{"displayName":"Simón Rodríguez","userId":"08248044722851115137"},"user_tz":-120},"id":"TsQOaggrSE5F","outputId":"e59752ff-39df-40ec-b2dc-a4b74f64d7b1"},"outputs":[{"name":"stdout","output_type":"stream","text":["\n","Distribución original:  [0.44152047 0.19883041 0.35964912]\n"]}],"source":["# Observamos cuál es la distribución del dataset completo.\n","encoder = ClassEncoder()\n","\n","y_full_encoded = encoder.fit_transform(y_full)\n","original_distribution = np.bincount(y_full_encoded.flatten().astype(int)) / y_full_encoded.size\n","\n","print(\"\\nDistribución original: \", original_distribution)"]},{"cell_type":"markdown","metadata":{"id":"PjsgX0b1SiFT"},"source":["## ***1.3 Punto 3***\n","### Implementación del LDA\n","\n","En el caso de LDA se hizo la suposición extra, de que es $X|_{G=j} \\sim \\mathcal{N}_p(\\mu_j, \\Sigma)$, es decir que las poblaciones no sólo siguen una distribución normal sino que son de igual matriz de covarianzas. Trabajando algebráicamente se obtuvo lo siguiente:\n","\n","$$\n","\\log{f_j(x)} =  \\mu_j^T \\Sigma^{-1} (x- \\frac{1}{2} \\mu_j) + C'\n","$$\n","\n","Por lo cuál, ajustar los datos con LDA implica estimar los parámetros $(\\mu_j, \\Sigma)$. Estos parámetros se estimarón por máxima verosimilitud, de manera que los estimadores resultaron:\n","\n","* $\\hat{\\mu}_j = \\bar{x}_j$ el promedio de los $x$ de la clase *j*\n","* $\\hat{\\Sigma}_j = s^2_j$ la matriz de covarianzas estimada para cada clase *j*\n","* $\\hat{\\pi}_j = f_{R_j} = \\frac{n_j}{n}$ la frecuencia relativa de la clase *j* en la muestra\n","* $\\hat{\\Sigma} = \\frac{1}{n} \\sum_{j=1}^k n_j \\cdot s^2_j$ el promedio ponderado (por frecs. relativas) de las matrices de covarianzas de todas las clases. *Observar que se utiliza el estimador de MV y no el insesgado*\n","\n","Considerando estas consideraciones, se procedió a crear la \"class LDA\" expuesta a continuación:\n","\n"]},{"cell_type":"markdown","metadata":{"id":"ksU-MYRON1jx"},"source":["#### **Clase LDA implementada**"]},{"cell_type":"code","execution_count":11,"metadata":{"executionInfo":{"elapsed":3,"status":"ok","timestamp":1713478594411,"user":{"displayName":"Simón Rodríguez","userId":"08248044722851115137"},"user_tz":-120},"id":"UK2812zbSGtO"},"outputs":[],"source":["class LDA(BaseBayesianClassifier):\n","    def _fit_params(self, X, y):\n","        # Computing the means of each class\n","        self.means = [X[:, y.flatten() == idx].mean(axis=1, keepdims=True)\n","                      for idx in range(len(self.log_a_priori))]\n","\n","        # Computing the covariance matrices for each class\n","        covariances = [np.cov(X[:, y.flatten() == idx], bias=True)\n","                       for idx in range(len(self.log_a_priori))]\n","\n","        # Computing the clustered covariance matrix as a weighted average\n","        # for the relative frequencies of the class\n","        freqs = np.bincount(y.flatten().astype(int)) / y.size\n","        pooled_cov = sum([cov * freq for cov, freq in zip(covariances, freqs)])\n","\n","        # Computing the inverse of the clustered covariance matrix.\n","        self.inv_pooled_cov = inv(pooled_cov)\n","\n","    def _predict_log_conditional(self, x, class_idx):\n","        # Computing the logarithm of the conditional probability P(x|G=class_idx)\n","        # For LDA, it's assumed that all the classes share the same coavariance matrix\n","        unbiased_x = x - 0.5*self.means[class_idx]\n","        return self.means[class_idx].T @ self.inv_pooled_cov @ unbiased_x"]},{"cell_type":"markdown","metadata":{"id":"Doyw5KhlWAeU"},"source":["#### **Validación rápida con LDA de SciKit-Learn**\n","Por un lado, se compararon los accuracy de la implementación del modelo LDA y el modelo de scikit learn. Para la implementación del LDA se obtuvo un accuracy similar al LDA de scikit learn."]},{"cell_type":"code","execution_count":12,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":414,"status":"ok","timestamp":1713478594823,"user":{"displayName":"Simón Rodríguez","userId":"08248044722851115137"},"user_tz":-120},"id":"R7zFNdGzV_P7","outputId":"c2a39865-db7a-43a1-8475-5a2268607d24"},"outputs":[{"name":"stdout","output_type":"stream","text":["Precisión del clasificador LDA - class LDA: 0.9833333333333333\n","Precisión del clasificador LDA - SciKit-learn: 0.9833333333333333\n"]}],"source":["# Validation\n","from sklearn.datasets import load_iris\n","from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n","from sklearn.metrics import accuracy_score as acc\n","\n","def accuracy_score(y_true, y_pred):\n","  return (y_true == y_pred).mean()\n","\n","rng_seed = 42\n","\n","# Cargar y dividir los datos (en train y test) para usar en la implementación de LDA\n","# Considerando que se debe hacer la transposición.\n","\n","X_full, y_full = get_iris_dataset()\n","train_x, train_y, test_x, test_y = split_transpose(X_full, y_full, 0.4, rng_seed)\n","\n","# Cargar los datos y dividir los datos (en train y test) para la LDA de SciKit-Learn\n","iris = load_iris()\n","X = iris.data\n","y = iris.target\n","\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=rng_seed)\n","\n","# Crear un clasificador LDA para ambas opciones\n","lda = LDA()\n","lda2 = LinearDiscriminantAnalysis()\n","\n","# Ajustar los clasificadores LDA con los datos de entrenamiento respectivos\n","lda.fit(train_x, train_y)\n","lda2.fit(X_train, y_train)\n","\n","# Hacer predicciones con los datos de prueba respectivos\n","y_pred1 = lda.predict(test_x)\n","y_pred2 = lda2.predict(X_test)\n","\n","\n","# Calcular la precisión de los clasificadores\n","accuracy = accuracy_score(test_y, y_pred1)\n","print(\"Precisión del clasificador LDA - class LDA:\", accuracy)\n","\n","accuracy2 = accuracy_score(y_test, y_pred2)\n","print(\"Precisión del clasificador LDA - SciKit-learn:\", accuracy2)\n","\n"]},{"cell_type":"markdown","metadata":{"id":"MCx3Ag84Y4H-"},"source":["### **Entrenamiento con dataset**\n","\n"]},{"cell_type":"code","execution_count":13,"metadata":{"executionInfo":{"elapsed":4,"status":"ok","timestamp":1713478594824,"user":{"displayName":"Simón Rodríguez","userId":"08248044722851115137"},"user_tz":-120},"id":"Xt_QP71tZVcS"},"outputs":[],"source":["# Preparamos algunas definiciones previas\n","random_seed = 43\n","\n","a_prioris = {\n","    \"sin_priori\" : None,\n","}\n","\n","def accuracy(y_true, y_pred):\n","  return (y_true == y_pred).mean()"]},{"cell_type":"code","execution_count":14,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":147},"executionInfo":{"elapsed":457,"status":"ok","timestamp":1713478595278,"user":{"displayName":"Simón Rodríguez","userId":"08248044722851115137"},"user_tz":-120},"id":"jnx3ZcEjZS3B","outputId":"7a1d0990-212b-4cd9-b4c5-2059ca57fc62"},"outputs":[{"name":"stdout","output_type":"stream","text":["QDA\n","LDA\n"]},{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Modelo</th>\n","      <th>Dataset</th>\n","      <th>Seed</th>\n","      <th>Priori</th>\n","      <th>Error(Train)</th>\n","      <th>Error(Test)</th>\n","      <th>ErrorClase0</th>\n","      <th>ErrorClase1</th>\n","      <th>ErrorClase2</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>QDA</td>\n","      <td>Iris</td>\n","      <td>43</td>\n","      <td>sin_priori</td>\n","      <td>0.000000</td>\n","      <td>0.033333</td>\n","      <td>0.0</td>\n","      <td>0.076923</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <th>0</th>\n","      <td>LDA</td>\n","      <td>Iris</td>\n","      <td>43</td>\n","      <td>sin_priori</td>\n","      <td>0.011111</td>\n","      <td>0.050000</td>\n","      <td>0.0</td>\n","      <td>0.076923</td>\n","      <td>0.058824</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["  Modelo Dataset  Seed      Priori  Error(Train)  Error(Test)  ErrorClase0  \\\n","0    QDA    Iris    43  sin_priori      0.000000     0.033333          0.0   \n","0    LDA    Iris    43  sin_priori      0.011111     0.050000          0.0   \n","\n","   ErrorClase1  ErrorClase2  \n","0     0.076923     0.000000  \n","0     0.076923     0.058824  "]},"execution_count":14,"metadata":{},"output_type":"execute_result"}],"source":["# Descargamos los datos de iris dataset y spliteamos en train y test\n","X_full, y_full = get_iris_dataset()\n","\n","# Obtenemos los resultados de las pruebas\n","resultado_qda = try_prioris(X_full, y_full, random_seed, a_prioris,\"QDA\",\"Iris\")\n","resultado_lda = try_prioris(X_full, y_full, random_seed, a_prioris,\"LDA\",\"Iris\")\n","\n","# Comparamos\n","comparar = pd.concat([resultado_qda, resultado_lda])\n","comparar"]},{"cell_type":"code","execution_count":15,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":201},"executionInfo":{"elapsed":417,"status":"ok","timestamp":1713478595689,"user":{"displayName":"Simón Rodríguez","userId":"08248044722851115137"},"user_tz":-120},"id":"VGzvKuMKZTkp","outputId":"f369781d-efdc-4f3d-aa59-65aa0e3e3aae"},"outputs":[{"name":"stdout","output_type":"stream","text":["QDA\n","LDA\n"]},{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Modelo</th>\n","      <th>Dataset</th>\n","      <th>Seed</th>\n","      <th>Priori</th>\n","      <th>Error(Train)</th>\n","      <th>Error(Test)</th>\n","      <th>ErrorClase0</th>\n","      <th>ErrorClase1</th>\n","      <th>ErrorClase2</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>QDA</td>\n","      <td>Penguins</td>\n","      <td>43</td>\n","      <td>sin_priori</td>\n","      <td>0.004878</td>\n","      <td>0.014599</td>\n","      <td>0.028986</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>0</th>\n","      <td>LDA</td>\n","      <td>Penguins</td>\n","      <td>43</td>\n","      <td>sin_priori</td>\n","      <td>0.004878</td>\n","      <td>0.014599</td>\n","      <td>0.028986</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["  Modelo   Dataset  Seed      Priori  Error(Train)  Error(Test)  ErrorClase0  \\\n","0    QDA  Penguins    43  sin_priori      0.004878     0.014599     0.028986   \n","0    LDA  Penguins    43  sin_priori      0.004878     0.014599     0.028986   \n","\n","   ErrorClase1  ErrorClase2  \n","0          0.0          0.0  \n","0          0.0          0.0  "]},"execution_count":15,"metadata":{},"output_type":"execute_result"}],"source":["# Descargamos los datos de iris dataset y spliteamos en train y test\n","X_full, y_full = get_penguins()\n","\n","# Obtenemos los resultados de las pruebas\n","resultado_qda = try_prioris(X_full, y_full, random_seed, a_prioris,\"QDA\",\"Penguins\")\n","resultado_lda = try_prioris(X_full, y_full, random_seed, a_prioris,\"LDA\",\"Penguins\")\n","\n","# Comparamos\n","comparar = pd.concat([resultado_qda, resultado_lda])\n","comparar"]},{"cell_type":"markdown","metadata":{"id":"qJP2OXDlbR7A"},"source":["### Respuesta:\n","¿Se observan diferencias? ¿Podría decirse que alguno de los dos es notoriamente mejor que el otro?\n","- No se observan diferencias notorias entre ambos modelos. Sin embargo, dada la *seed* utilizada, para el dataset de **Iris**, se observó que QDA tenía menor error general para el conjunto de prueba y para la *Clase 2*, en comparación con LDA."]},{"cell_type":"markdown","metadata":{"id":"TewGEtRjYj8D"},"source":["## ***1.4 Punto 4***"]},{"cell_type":"markdown","metadata":{"id":"adnP6lsubvkb"},"source":["### **1ª Random seed**"]},{"cell_type":"code","execution_count":16,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5,"status":"ok","timestamp":1713478595689,"user":{"displayName":"Simón Rodríguez","userId":"08248044722851115137"},"user_tz":-120},"id":"yjozqnG5b5tQ","outputId":"a5e53f2b-28e9-468b-f199-54d44b458e46"},"outputs":[{"name":"stdout","output_type":"stream","text":["8083\n"]}],"source":["import random\n","\n","random_seed = random.randint(42, 10000)\n","print(random_seed)"]},{"cell_type":"code","execution_count":17,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":164},"executionInfo":{"elapsed":426,"status":"ok","timestamp":1713478596112,"user":{"displayName":"Simón Rodríguez","userId":"08248044722851115137"},"user_tz":-120},"id":"VIegNZfSnJQF","outputId":"3adb4b0a-a6a9-4682-ebf2-447d2fb37653"},"outputs":[{"name":"stdout","output_type":"stream","text":["QDA\n","LDA\n","Usando la seed: 8083\n"]},{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Modelo</th>\n","      <th>Dataset</th>\n","      <th>Seed</th>\n","      <th>Priori</th>\n","      <th>Error(Train)</th>\n","      <th>Error(Test)</th>\n","      <th>ErrorClase0</th>\n","      <th>ErrorClase1</th>\n","      <th>ErrorClase2</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>QDA</td>\n","      <td>Iris</td>\n","      <td>8083</td>\n","      <td>sin_priori</td>\n","      <td>0.011111</td>\n","      <td>0.05</td>\n","      <td>0.0</td>\n","      <td>0.125000</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <th>0</th>\n","      <td>LDA</td>\n","      <td>Iris</td>\n","      <td>8083</td>\n","      <td>sin_priori</td>\n","      <td>0.000000</td>\n","      <td>0.05</td>\n","      <td>0.0</td>\n","      <td>0.083333</td>\n","      <td>0.058824</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["  Modelo Dataset  Seed      Priori  Error(Train)  Error(Test)  ErrorClase0  \\\n","0    QDA    Iris  8083  sin_priori      0.011111         0.05          0.0   \n","0    LDA    Iris  8083  sin_priori      0.000000         0.05          0.0   \n","\n","   ErrorClase1  ErrorClase2  \n","0     0.125000     0.000000  \n","0     0.083333     0.058824  "]},"execution_count":17,"metadata":{},"output_type":"execute_result"}],"source":["# Obtenemos los datos del dataset de Iris\n","X_full, y_full = get_iris_dataset()\n","\n","# Obtenemos los resultados de las pruebas\n","resultado_qda = try_prioris(X_full, y_full, random_seed, a_prioris,\"QDA\",\"Iris\")\n","resultado_lda = try_prioris(X_full, y_full, random_seed, a_prioris,\"LDA\",\"Iris\")\n","\n","print(f\"Usando la seed: {random_seed}\")\n","\n","# Comparamos\n","comparar = pd.concat([resultado_qda, resultado_lda])\n","comparar"]},{"cell_type":"code","execution_count":18,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":219},"executionInfo":{"elapsed":909,"status":"ok","timestamp":1713478597018,"user":{"displayName":"Simón Rodríguez","userId":"08248044722851115137"},"user_tz":-120},"id":"Do7O_HPCnwBe","outputId":"663eb6cb-9645-41b3-e4ad-f8d42a819e47"},"outputs":[{"name":"stdout","output_type":"stream","text":["QDA\n","LDA\n","Usando la seed: 8083\n"]},{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Modelo</th>\n","      <th>Dataset</th>\n","      <th>Seed</th>\n","      <th>Priori</th>\n","      <th>Error(Train)</th>\n","      <th>Error(Test)</th>\n","      <th>ErrorClase0</th>\n","      <th>ErrorClase1</th>\n","      <th>ErrorClase2</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>QDA</td>\n","      <td>Penguins</td>\n","      <td>8083</td>\n","      <td>sin_priori</td>\n","      <td>0.004878</td>\n","      <td>0.007299</td>\n","      <td>0.016667</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>0</th>\n","      <td>LDA</td>\n","      <td>Penguins</td>\n","      <td>8083</td>\n","      <td>sin_priori</td>\n","      <td>0.009756</td>\n","      <td>0.007299</td>\n","      <td>0.016667</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["  Modelo   Dataset  Seed      Priori  Error(Train)  Error(Test)  ErrorClase0  \\\n","0    QDA  Penguins  8083  sin_priori      0.004878     0.007299     0.016667   \n","0    LDA  Penguins  8083  sin_priori      0.009756     0.007299     0.016667   \n","\n","   ErrorClase1  ErrorClase2  \n","0          0.0          0.0  \n","0          0.0          0.0  "]},"execution_count":18,"metadata":{},"output_type":"execute_result"}],"source":["# Obtenemos los datos del dataset de Penguins\n","X_full, y_full = get_penguins()\n","\n","# Obtenemos los resultados de las pruebas\n","resultado_qda = try_prioris(X_full, y_full, random_seed, a_prioris,\"QDA\",\"Penguins\")\n","resultado_lda = try_prioris(X_full, y_full, random_seed, a_prioris,\"LDA\",\"Penguins\")\n","\n","print(f\"Usando la seed: {random_seed}\")\n","\n","# Comparamos\n","comparar = pd.concat([resultado_qda, resultado_lda])\n","comparar"]},{"cell_type":"markdown","metadata":{"id":"5w-qg-gieWRr"},"source":["###**2ª Random seed**"]},{"cell_type":"code","execution_count":19,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":9,"status":"ok","timestamp":1713478597018,"user":{"displayName":"Simón Rodríguez","userId":"08248044722851115137"},"user_tz":-120},"id":"zVhKltEReaoC","outputId":"66168b41-67e0-44b8-f482-bebebaf0ecf3"},"outputs":[{"name":"stdout","output_type":"stream","text":["3325\n"]}],"source":["random_seed = random.randint(42, 10000)\n","print(random_seed)"]},{"cell_type":"code","execution_count":20,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":164},"executionInfo":{"elapsed":7,"status":"ok","timestamp":1713478597018,"user":{"displayName":"Simón Rodríguez","userId":"08248044722851115137"},"user_tz":-120},"id":"J1D4Wx3yo2t6","outputId":"9264fd65-58be-45ff-c965-e56dc3b7202d"},"outputs":[{"name":"stdout","output_type":"stream","text":["QDA\n","LDA\n","Usando la seed: 3325\n"]},{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Modelo</th>\n","      <th>Dataset</th>\n","      <th>Seed</th>\n","      <th>Priori</th>\n","      <th>Error(Train)</th>\n","      <th>Error(Test)</th>\n","      <th>ErrorClase0</th>\n","      <th>ErrorClase1</th>\n","      <th>ErrorClase2</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>QDA</td>\n","      <td>Iris</td>\n","      <td>3325</td>\n","      <td>sin_priori</td>\n","      <td>0.011111</td>\n","      <td>0.033333</td>\n","      <td>0.0</td>\n","      <td>0.052632</td>\n","      <td>0.045455</td>\n","    </tr>\n","    <tr>\n","      <th>0</th>\n","      <td>LDA</td>\n","      <td>Iris</td>\n","      <td>3325</td>\n","      <td>sin_priori</td>\n","      <td>0.011111</td>\n","      <td>0.033333</td>\n","      <td>0.0</td>\n","      <td>0.052632</td>\n","      <td>0.045455</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["  Modelo Dataset  Seed      Priori  Error(Train)  Error(Test)  ErrorClase0  \\\n","0    QDA    Iris  3325  sin_priori      0.011111     0.033333          0.0   \n","0    LDA    Iris  3325  sin_priori      0.011111     0.033333          0.0   \n","\n","   ErrorClase1  ErrorClase2  \n","0     0.052632     0.045455  \n","0     0.052632     0.045455  "]},"execution_count":20,"metadata":{},"output_type":"execute_result"}],"source":["# Obtenemos los datos del dataset de Iris\n","X_full, y_full = get_iris_dataset()\n","\n","# Obtenemos los resultados de las pruebas\n","resultado_qda = try_prioris(X_full, y_full, random_seed, a_prioris,\"QDA\",\"Iris\")\n","resultado_lda = try_prioris(X_full, y_full, random_seed, a_prioris,\"LDA\",\"Iris\")\n","\n","print(f\"Usando la seed: {random_seed}\")\n","\n","# Comparamos\n","comparar = pd.concat([resultado_qda, resultado_lda])\n","comparar"]},{"cell_type":"code","execution_count":21,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":219},"executionInfo":{"elapsed":955,"status":"ok","timestamp":1713478597969,"user":{"displayName":"Simón Rodríguez","userId":"08248044722851115137"},"user_tz":-120},"id":"IDKN2qVgppaY","outputId":"5fa30f7f-00d8-4182-a1fc-390cfa6d1c93"},"outputs":[{"name":"stdout","output_type":"stream","text":["QDA\n","LDA\n","Usando la seed: 3325\n"]},{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Modelo</th>\n","      <th>Dataset</th>\n","      <th>Seed</th>\n","      <th>Priori</th>\n","      <th>Error(Train)</th>\n","      <th>Error(Test)</th>\n","      <th>ErrorClase0</th>\n","      <th>ErrorClase1</th>\n","      <th>ErrorClase2</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>QDA</td>\n","      <td>Penguins</td>\n","      <td>3325</td>\n","      <td>sin_priori</td>\n","      <td>0.019512</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>0</th>\n","      <td>LDA</td>\n","      <td>Penguins</td>\n","      <td>3325</td>\n","      <td>sin_priori</td>\n","      <td>0.014634</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["  Modelo   Dataset  Seed      Priori  Error(Train)  Error(Test)  ErrorClase0  \\\n","0    QDA  Penguins  3325  sin_priori      0.019512          0.0          0.0   \n","0    LDA  Penguins  3325  sin_priori      0.014634          0.0          0.0   \n","\n","   ErrorClase1  ErrorClase2  \n","0          0.0          0.0  \n","0          0.0          0.0  "]},"execution_count":21,"metadata":{},"output_type":"execute_result"}],"source":["# Obtenemos los datos del dataset de Penguins\n","X_full, y_full = get_penguins()\n","\n","# Obtenemos los resultados de las pruebas\n","resultado_qda = try_prioris(X_full, y_full, random_seed, a_prioris,\"QDA\",\"Penguins\")\n","resultado_lda = try_prioris(X_full, y_full, random_seed, a_prioris,\"LDA\",\"Penguins\")\n","\n","print(f\"Usando la seed: {random_seed}\")\n","\n","# Comparamos\n","comparar = pd.concat([resultado_qda, resultado_lda])\n","comparar"]},{"cell_type":"markdown","metadata":{"id":"H-TDIBFBeqHG"},"source":["### Respuesta:\n","¿Las conclusiones previas se mantienen?\n","\n","- Se mantiene que el modelo QDA es mejor en algunos casos que el modelo LDA, pero esto depende de la seed que se esté utilizando. Es decir, la distribución del dataset cuando se hace el split.\n","- Vale la pena resaltar que cuando los resultados no son similares, QDA presenta un error que es aproximadamente la mitad del error de LDA para esa *Seed*."]},{"cell_type":"markdown","metadata":{"id":"Ftdu-eMUYmAY"},"source":["## ***1.5 Punto 5***\n","Estimar y comparar los tiempos de predicción de las clases `QDA` y `TensorizedQDA`. De haber diferencias ¿Cuáles pueden ser las causas?"]},{"cell_type":"code","execution_count":22,"metadata":{"executionInfo":{"elapsed":7,"status":"ok","timestamp":1713478597969,"user":{"displayName":"Simón Rodríguez","userId":"08248044722851115137"},"user_tz":-120},"id":"ewNbKjBxfUdC"},"outputs":[],"source":["qda     = QDA()\n","ten_qda =  TensorizedQDA ()\n","\n","X_full, y_full = get_iris_dataset()\n","train_x, train_y, test_x, test_y = split_transpose(X_full, y_full, 0.4, random_seed)"]},{"cell_type":"code","execution_count":23,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3330,"status":"ok","timestamp":1713478601294,"user":{"displayName":"Simón Rodríguez","userId":"08248044722851115137"},"user_tz":-120},"id":"2puI6yVeiH4T","outputId":"4dc7c938-47f1-4793-93e2-8cf3069d4136"},"outputs":[{"name":"stdout","output_type":"stream","text":["5.87 ms ± 365 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"]}],"source":["%%timeit\n","\n","qda.fit(train_x, train_y)\n","qda.predict(test_x)\n"]},{"cell_type":"code","execution_count":24,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":12977,"status":"ok","timestamp":1713478614267,"user":{"displayName":"Simón Rodríguez","userId":"08248044722851115137"},"user_tz":-120},"id":"pD1kdR5ujYaT","outputId":"e1b5c874-0441-485c-e1b1-194b27b26b5f"},"outputs":[{"data":{"text/plain":["array([['versicolor', 'versicolor', 'setosa', 'setosa', 'virginica',\n","        'versicolor', 'setosa', 'versicolor', 'versicolor', 'setosa',\n","        'setosa', 'setosa', 'versicolor', 'versicolor', 'setosa',\n","        'versicolor', 'virginica', 'virginica', 'virginica', 'virginica',\n","        'versicolor', 'setosa', 'setosa', 'virginica', 'setosa',\n","        'virginica', 'setosa', 'setosa', 'setosa', 'setosa', 'virginica',\n","        'virginica', 'versicolor', 'versicolor', 'versicolor',\n","        'virginica', 'virginica', 'virginica', 'virginica', 'setosa',\n","        'virginica', 'virginica', 'setosa', 'setosa', 'versicolor',\n","        'virginica', 'setosa', 'versicolor', 'virginica', 'versicolor',\n","        'setosa', 'virginica', 'virginica', 'versicolor', 'versicolor',\n","        'virginica', 'versicolor', 'virginica', 'virginica',\n","        'versicolor']], dtype='<U10')"]},"execution_count":24,"metadata":{},"output_type":"execute_result"}],"source":["# %%timeit\n","\n","ten_qda.fit(train_x, train_y)\n","ten_qda.predict(test_x)"]},{"cell_type":"markdown","metadata":{"id":"4x34TvqQjggt"},"source":["Se disminuye a menos de la mitad la latencia del modelo. Esto se debe a que la tensorización del modelo nos permite calcular la probabilidad para las tres clases en una sola operación en el modelo TensorizedQDA, en lugar de secuencialmente como en el modelo QDA. Cabe destacar que esto se logra al tensorizar el ciclo que itera el número de clases en un dataset con sólo tres clases. En un dataset con mucha mayor cantidad de clases cabe esperar que esta mejora sea aún más significativa."]},{"cell_type":"markdown","metadata":{},"source":["# **2** **Optimización matemática**"]},{"cell_type":"markdown","metadata":{"id":"PTgroAMKj37H"},"source":["**Sugerencia:** considerar combinaciones adecuadas de `transpose`, `reshape` y, ocasionalmente, `flatten`. Explorar la dimensionalidad de cada elemento antes de implementar las clases.\n","\n","### QDA\n","\n","Debido a la forma cuadrática de QDA, no se puede predecir para *n* observaciones en una sola pasada (utilizar $X \\in \\mathbb{R}^{p \\times n}$ en vez de $x \\in \\mathbb{R}^p$) sin pasar por una matriz de *n x n* en donde se computan todas las interacciones entre observaciones. Se puede acceder al resultado recuperando sólo la diagonal de dicha matriz, pero resulta ineficiente en tiempo y (especialmente) en memoria. Aún así, es *posible* que el modelo funcione más rápido.\n","\n","1. Implementar el modelo `FasterQDA` (se recomienda heredarlo de TensorizedQDA) de manera de eliminar el ciclo for en el método predict.\n","2. Comparar los tiempos de predicción de `FasterQDA` con `TensorizedQDA` y `QDA`.\n","3. Mostrar (puede ser con un print) dónde aparece la mencionada matriz de *n x n*, donde *n* es la cantidad de observaciones a predecir.\n","4.Demostrar que\n","$$\n","diag(A \\cdot B) = \\sum_{cols} A \\odot B^T = np.sum(A \\odot B^T, axis=1)\n","$$ es decir, que se puede \"esquivar\" la matriz de *n x n* usando matrices de *n x p*.\n","5.Utilizar la propiedad antes demostrada para reimplementar la predicción del modelo `FasterQDA` de forma eficiente. ¿Hay cambios en los tiempos de predicción?\n","\n","\n","### LDA\n","\n","1. \"Tensorizar\" el modelo LDA y comparar sus tiempos de predicción con el modelo antes implementado. *Notar que, en modo tensorizado, se puede directamente precomputar $\\mu^T \\cdot \\Sigma^{-1} \\in \\mathbb{R}^{k \\times 1 \\times p}$ y guardar eso en vez de $\\Sigma^{-1}$.*\n","2. LDA no sufre del problema antes descrito de QDA debido a que no computa productos internos, por lo que no tiene un verdadero costo extra en memoria predecir \"en batch\". Implementar el modelo `FasterLDA` y comparar sus tiempos de predicción con las versiones anteriores de LDA."]},{"cell_type":"markdown","metadata":{"id":"QjMpuAlOkdQk"},"source":["***Resolución***\n","\n","---\n","\n"]},{"cell_type":"markdown","metadata":{"id":"Ujp7mzU_nEvp"},"source":["## ***2.1 QDA***"]},{"cell_type":"markdown","metadata":{"id":"5Bvr8UTkKohI"},"source":["### *2.1.1 Punto 1*"]},{"cell_type":"code","execution_count":25,"metadata":{"executionInfo":{"elapsed":8,"status":"ok","timestamp":1713478614268,"user":{"displayName":"Simón Rodríguez","userId":"08248044722851115137"},"user_tz":-120},"id":"81SOVaJj9f6h"},"outputs":[],"source":["class FasterQDA(TensorizedQDA):\n","    def _predict_log_conditionals(self, X, debug):\n","        unbiased_X = X - self.tensor_means # (k, n, m)\n","\n","        inner_prod = unbiased_X.transpose(0,2,1) @ self.tensor_inv_cov @ unbiased_X # shape: (k, m, m)\n","\n","        if debug:\n","          print((\n","              \"\\nInner product between:\\n\"\n","              f\"- unbiased X tensor transposed (shape: {unbiased_X.transpose(0,2,1).shape})\\n\"\n","              f\"- inverse of cov matrix tensor (shape: {self.tensor_inv_cov.shape})\\n\"\n","              f\"- unbiased X tensor (shape: {unbiased_X.shape})\\n\"\n","              f\"shape: {inner_prod.shape}\\n\"\n","              ))\n","\n","        inner_prod_diagonal = np.diagonal(inner_prod, axis1=1, axis2=2) # shape: (k, m)\n","\n","        log_likelihood_constant = np.log(det(self.tensor_inv_cov)).reshape(3, 1) # add dimension to allow broadcasting\n","\n","        return 0.5 * log_likelihood_constant - 0.5 * inner_prod_diagonal # shape: (k, m)\n","\n","    def predict(self, X, debug=False):\n","        log_a_priori_reshaped = self.log_a_priori.reshape(3, 1) # add dimension to allow broadcasting\n","\n","        # We dont need _predict_one method anymore. Now we predict all at once!\n","        encoded_y_hat = np.argmax(log_a_priori_reshaped + self._predict_log_conditionals(X, debug), axis=0)\n","        y_hat = self.encoder.names[encoded_y_hat]\n","\n","        return y_hat.reshape(1,-1)"]},{"cell_type":"markdown","metadata":{"id":"R0tqjsIGDerc"},"source":["### *2.1.2 Punto 2*"]},{"cell_type":"code","execution_count":26,"metadata":{"executionInfo":{"elapsed":5,"status":"ok","timestamp":1713478614268,"user":{"displayName":"Simón Rodríguez","userId":"08248044722851115137"},"user_tz":-120},"id":"_OPdpshjpNTZ"},"outputs":[],"source":["qda = QDA()\n","qda.fit(train_x, train_y)\n","\n","ten_qda = TensorizedQDA ()\n","ten_qda.fit(train_x, train_y)\n","\n","fast_qda = FasterQDA()\n","fast_qda.fit(train_x, train_y)"]},{"cell_type":"code","execution_count":27,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1050,"status":"ok","timestamp":1713478615313,"user":{"displayName":"Simón Rodríguez","userId":"08248044722851115137"},"user_tz":-120},"id":"D-tGFDk7vEH9","outputId":"6c72313e-1859-42e2-98a7-dfb236837083"},"outputs":[{"name":"stdout","output_type":"stream","text":["5.43 ms ± 1.48 ms per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"]}],"source":["%%timeit\n","qda.predict(test_x)"]},{"cell_type":"code","execution_count":28,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1896,"status":"ok","timestamp":1713478617207,"user":{"displayName":"Simón Rodríguez","userId":"08248044722851115137"},"user_tz":-120},"id":"sKiJuNMRvG3d","outputId":"4e0965fb-0ebf-4be1-8809-fa8e989eb79d"},"outputs":[{"name":"stdout","output_type":"stream","text":["2.13 ms ± 630 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"]}],"source":["%%timeit\n","ten_qda.predict(test_x)"]},{"cell_type":"code","execution_count":29,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4221,"status":"ok","timestamp":1713478621426,"user":{"displayName":"Simón Rodríguez","userId":"08248044722851115137"},"user_tz":-120},"id":"cduzmWsKvTnx","outputId":"f5a9e333-5dc7-4925-c493-56539d414c6f"},"outputs":[{"name":"stdout","output_type":"stream","text":["46.6 µs ± 4.96 µs per loop (mean ± std. dev. of 7 runs, 10,000 loops each)\n"]}],"source":["%%timeit\n","fast_qda.predict(test_x)"]},{"cell_type":"code","execution_count":30,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":11,"status":"ok","timestamp":1713478621426,"user":{"displayName":"Simón Rodríguez","userId":"08248044722851115137"},"user_tz":-120},"id":"fRDLOedFzQ08","outputId":"f6d28407-a7de-4b86-a9ea-ee7834ec451c"},"outputs":[{"data":{"text/plain":["True"]},"execution_count":30,"metadata":{},"output_type":"execute_result"}],"source":["# Verify that we obtained the same predictions that we obtained previously.\n","(fast_qda.predict(test_x) == ten_qda.predict(test_x)).all()"]},{"cell_type":"markdown","metadata":{"id":"CDYKOELeDiWD"},"source":["### *2.1.3 Punto 3*"]},{"cell_type":"code","execution_count":31,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":9,"status":"ok","timestamp":1713478621427,"user":{"displayName":"Simón Rodríguez","userId":"08248044722851115137"},"user_tz":-120},"id":"vlWuUFjWAKBI","outputId":"3c476c7f-00c7-411c-8996-ee5b02246e32"},"outputs":[{"name":"stdout","output_type":"stream","text":["\n","Inner product between:\n","- unbiased X tensor transposed (shape: (3, 60, 4))\n","- inverse of cov matrix tensor (shape: (3, 4, 4))\n","- unbiased X tensor (shape: (3, 4, 60))\n","shape: (3, 60, 60)\n","\n"]}],"source":["fast_qda.predict(test_x, debug=True);"]},{"cell_type":"markdown","metadata":{"id":"ySvzQOtPD0rn"},"source":["### *2.1.4 Punto 4: \"Demostración\"*"]},{"cell_type":"markdown","metadata":{"id":"QjR1FYa9Lvtb"},"source":["Sean\n","$$\n","A \\in \\mathbb{R}^{m \\times n}, B \\in \\mathbb{R}^{n \\times m}\n","$$\n","<br>\n","- Por definición del producto de Hadamard de matrices tenemos:\n","$$\n","(A \\odot B^T)_{ij} = A_{ij}B^T_{ij} = A_{ij}B_{ji}\n","$$\n","Cuya suma a través de columnas para un vector fila $i$ es:\n","$$\n","  \\sum_{cols} (A \\odot B^T)_{i.} = \\sum_{j=1}^{n}A_{ij}B^T_{ij} = \\sum_{j=1}^{n}A_{ij}B_{ji}\n","$$\n","Podemos generalizar entonces a todas las filas:\n","$$\n","\\sum_{cols} A \\odot B^T = {[\\sum_{j=1}^{n}A_{1j}B_{j1},\\sum_{j=1}^{n}A_{2j}B_{j2},...,\\sum_{j=1}^{n}A_{mj}B_{jm}]^T} _{(1)}\n","$$\n","<br>\n","- Por definición del producto de matrices:\n","$$\n","(A \\cdot B)_{ij} = \\sum_{r=1}^{n}A_{ir}B_{rj}\n","$$\n","Dónde cada elemento $i$ de la diagonal es:\n","$$\n","(A \\cdot B)_{ii} = \\sum_{r=1}^{n}A_{ir}B_{ri}\n","$$\n","Por lo cual:\n","$$\n","diag(A \\cdot B) = {[\\sum_{j=1}^{n}A_{1j}B_{j1},\\sum_{j=1}^{n}A_{2j}B_{j2},...,\\sum_{j=1}^{n}A_{mj}B_{jm}]^T} _{(2)}\n","$$\n","<br>\n","- Comparando $(1)$ y $(2)$, obtenemos:\n","\n","$$\n","diag(A \\cdot B) = \\sum_{cols} A \\odot B^T = np.sum(A \\odot B^T, axis=1)\n","$$\n"]},{"cell_type":"markdown","metadata":{"id":"iHCz9Ng0L6_g"},"source":["###*2.1.5 Punto 5*"]},{"cell_type":"code","execution_count":32,"metadata":{"executionInfo":{"elapsed":6,"status":"ok","timestamp":1713478621427,"user":{"displayName":"Simón Rodríguez","userId":"08248044722851115137"},"user_tz":-120},"id":"SV_GTegLGxQU"},"outputs":[],"source":["class FasterQDA(TensorizedQDA):\n","    def _predict_log_conditionals(self, X, debug):\n","        unbiased_X = X - self.tensor_means # (k, n, m)\n","\n","        # We obtain inner_prod_diagonal avoiding nxn matrix!\n","        first_inner_prod = unbiased_X.transpose(0,2,1) @ self.tensor_inv_cov # shape: (k, m, n)\n","        inner_prod_diagonal = np.sum(first_inner_prod * unbiased_X.transpose(0,2,1), axis=2) # shape: (3, 90)\n","\n","        if debug:\n","          print((\n","              \"\\nInner product between:\\n\"\n","              f\"- unbiased X tensor transposed (shape: {unbiased_X.transpose(0,2,1).shape})\\n\"\n","              f\"- inverse of cov matrix tensor (shape: {self.tensor_inv_cov.shape})\\n\"\n","              f\"shape: {first_inner_prod.shape}\\n\"\n","              \"Sum of Hadamard product between:\"\n","              f\"- inner product obtained previously (shape: {unbiased_X.shape})\\n\"\n","              f\"- unbiased X tensor trasnposed (shape: {unbiased_X.shape})\\n\"\n","              f\"shape: {inner_prod_diagonal.shape}\\n\"\n","              ))\n","\n","        log_likelihood_constant = np.log(det(self.tensor_inv_cov)).reshape(3, 1) # add dimension to allow broadcasting\n","\n","        return 0.5 * log_likelihood_constant - 0.5 * inner_prod_diagonal # shape: (k, m)\n","\n","    def predict(self, X, debug=False):\n","        log_a_priori_reshaped = self.log_a_priori.reshape(3, 1) # add dimension to allow broadcasting\n","\n","        # We dont need _predict_one method anymore. Now we predict all at once!\n","        encoded_y_hat = np.argmax(log_a_priori_reshaped + self._predict_log_conditionals(X, debug), axis=0)\n","        y_hat = self.encoder.names[encoded_y_hat]\n","\n","        return y_hat.reshape(1,-1)"]},{"cell_type":"code","execution_count":33,"metadata":{"executionInfo":{"elapsed":5,"status":"ok","timestamp":1713478621427,"user":{"displayName":"Simón Rodríguez","userId":"08248044722851115137"},"user_tz":-120},"id":"-FCw7CAxI5y9"},"outputs":[],"source":["fast_qda = FasterQDA()\n","fast_qda.fit(train_x, train_y)"]},{"cell_type":"code","execution_count":34,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5444,"status":"ok","timestamp":1713478626867,"user":{"displayName":"Simón Rodríguez","userId":"08248044722851115137"},"user_tz":-120},"id":"Q2-azKYEJD9k","outputId":"0e5136ae-b4ca-4aab-e269-e3d4427758c5"},"outputs":[{"name":"stdout","output_type":"stream","text":["51 µs ± 16.1 µs per loop (mean ± std. dev. of 7 runs, 10,000 loops each)\n"]}],"source":["%%timeit\n","fast_qda.predict(test_x)"]},{"cell_type":"markdown","metadata":{"id":"0e42FVkgJVwF"},"source":["Se observan mejoras no significativas en esta prueba de la nueva implementación de FasterQDA para n=60 observaciones. Sin embargo, a medida que aumente el número de observaciones, la ventaja de no computar la matriz $n \\times n$ se volverá mucho más relevante."]},{"cell_type":"markdown","metadata":{"id":"QvEJIhOgm1uZ"},"source":["##***2.2 LDA***"]},{"cell_type":"markdown","metadata":{"id":"XMyyg5gtK9pq"},"source":["### *2.2.1 Punto 1*"]},{"cell_type":"markdown","metadata":{"id":"8jTqdT9G_XM3"},"source":["LDA - como referencia\n","\n","```Python\n","class LDA(BaseBayesianClassifier):\n","    def _fit_params(self, X, y):\n","        # Computing the means of each class\n","        self.means = [X[:, y.flatten() == idx].mean(axis=1, keepdims=True)\n","                      for idx in range(len(self.log_a_priori))]\n","\n","        # Computing the covariance matrices for each class\n","        covariances = [np.cov(X[:, y.flatten() == idx], bias=True)\n","                       for idx in range(len(self.log_a_priori))]\n","\n","        # Computing the clustered covariance matrix as a weighted average\n","        # for the relative frequencies of the class\n","        freqs = np.bincount(y.flatten().astype(int)) / y.size\n","        pooled_cov = sum([cov * freq for cov, freq in zip(covariances, freqs)])\n","\n","        # Computing the inverse of the clustered covariance matrix.\n","        self.inv_pooled_cov = inv(pooled_cov)\n","\n","    def _predict_log_conditional(self, x, class_idx):\n","        # Computing the logarithm of the conditional probability P(x|G=class_idx)\n","        # For LDA, it's assumed that all the classes share the same coavariance matrix\n","        unbiased_x = x - 0.5*self.means[class_idx]\n","        return self.means[class_idx].T @ self.inv_pooled_cov @ unbiased_x\n","```"]},{"cell_type":"code","execution_count":35,"metadata":{"executionInfo":{"elapsed":5,"status":"ok","timestamp":1713478626867,"user":{"displayName":"Simón Rodríguez","userId":"08248044722851115137"},"user_tz":-120},"id":"ijJBndI_krot"},"outputs":[],"source":["class TensorizedLDA(LDA):\n","\n","    def _fit_params(self, X, y):\n","        # ask plain LDA to fit params\n","        super()._fit_params(X,y)\n","\n","        # stack onto new dimension\n","        self.tensor_inv_cov_pooled = self.inv_pooled_cov #No es necesario el stack, ya que solo tenemos una matriz inversa agrupada a diferencia de QDA\n","        self.tensor_means = np.stack(self.means)\n","\n","        #Precómputo de mu^T * Sigma^{-1}\n","        self.precomp = self.tensor_means.transpose(0,2,1) @ self.tensor_inv_cov_pooled\n","\n","    def _predict_log_conditionals(self,x):\n","        unbiased_x = x - 0.5*self.tensor_means\n","        calc = self.precomp @ unbiased_x\n","        return calc.flatten()\n","\n","    def _predict_one(self, x):\n","        # return the class that has maximum a posteriori probability\n","        max_class = np.argmax(self.log_a_priori + self._predict_log_conditionals(x))\n","        return max_class"]},{"cell_type":"code","execution_count":36,"metadata":{"executionInfo":{"elapsed":4,"status":"ok","timestamp":1713478626868,"user":{"displayName":"Simón Rodríguez","userId":"08248044722851115137"},"user_tz":-120},"id":"mPeJFwUflCjy"},"outputs":[],"source":["# Iniciamos los modelos:\n","lda     = LDA()\n","ten_lda =  TensorizedLDA()"]},{"cell_type":"code","execution_count":37,"metadata":{"executionInfo":{"elapsed":3,"status":"ok","timestamp":1713478626868,"user":{"displayName":"Simón Rodríguez","userId":"08248044722851115137"},"user_tz":-120},"id":"-EO7wHlsHuzu"},"outputs":[],"source":["# Obtenemos los datos del primer dataset: Iris\n","X_full, y_full = get_iris_dataset()\n","train_x, train_y, test_x, test_y = split_transpose(X_full, y_full, 0.4, rng_seed)"]},{"cell_type":"code","execution_count":38,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3989,"status":"ok","timestamp":1713478630854,"user":{"displayName":"Simón Rodríguez","userId":"08248044722851115137"},"user_tz":-120},"id":"vjd59a4OletG","outputId":"d6158592-184e-43c1-c852-db3fe26c0bcf"},"outputs":[{"name":"stderr","output_type":"stream","text":["UsageError: Line magic function `%%timeit` not found.\n"]}],"source":["# Medimos el tiempo de LDA\n","%%timeit\n","\n","lda.fit(train_x, train_y)\n","lda.predict(test_x)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1781,"status":"ok","timestamp":1713478632633,"user":{"displayName":"Simón Rodríguez","userId":"08248044722851115137"},"user_tz":-120},"id":"rSdEUDd5lhDG","outputId":"190e58d9-e75a-4eaa-ea6a-3f19e7717ea8"},"outputs":[{"name":"stdout","output_type":"stream","text":["2.15 ms ± 185 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"]}],"source":["# Medimos el tiempo de LDA Tensorizado\n","%%timeit\n","\n","ten_lda.fit(train_x, train_y)\n","ten_lda.predict(test_x)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":7,"status":"ok","timestamp":1713478632634,"user":{"displayName":"Simón Rodríguez","userId":"08248044722851115137"},"user_tz":-120},"id":"hJXTTD0VFPUF","outputId":"4f249179-2559-43e0-af44-55b8e0673b30"},"outputs":[{"name":"stdout","output_type":"stream","text":["Tensorized LDA Accuracy: 0.9833333333333333\n","LDA Accuracy: 0.9833333333333333\n"]}],"source":["# Comparamos con LDA\n","\n","def accuracy_score(y_true, y_pred):\n","  return (y_true == y_pred).mean()\n","\n","ten_lda_acc = accuracy_score(test_y, ten_lda.predict(test_x))\n","lda_acc = accuracy_score(test_y, lda.predict(test_x))\n","\n","print(f\"Tensorized LDA Accuracy: {ten_lda_acc}\")\n","print(f\"LDA Accuracy: {lda_acc}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5,"status":"ok","timestamp":1713478632634,"user":{"displayName":"Simón Rodríguez","userId":"08248044722851115137"},"user_tz":-120},"id":"5YRhvaU1IJ0U","outputId":"a3471bf5-5a9e-4039-cd93-11870c8867ff"},"outputs":[{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.10/dist-packages/sklearn/datasets/_openml.py:968: FutureWarning: The default value of `parser` will change from `'liac-arff'` to `'auto'` in 1.4. You can set `parser='auto'` to silence this warning. Therefore, an `ImportError` will be raised from 1.4 if the dataset is dense and pandas is not installed. Note that the pandas parser may return different data types. See the Notes Section in fetch_openml's API doc for details.\n","  warn(\n"]}],"source":["# Repetimos con el dataset Penguins:\n","X_full, y_full = get_penguins()\n","train_x, train_y, test_x, test_y = split_transpose(X_full, y_full, 0.4, rng_seed)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":8918,"status":"ok","timestamp":1713478641549,"user":{"displayName":"Simón Rodríguez","userId":"08248044722851115137"},"user_tz":-120},"id":"-OtxtUOuIbpD","outputId":"5aad402b-133e-4b49-b47f-20124abf8622"},"outputs":[{"name":"stdout","output_type":"stream","text":["11.3 ms ± 3.05 ms per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"]}],"source":["# Medimos el tiempo de LDA\n","%%timeit\n","\n","lda.fit(train_x, train_y)\n","lda.predict(test_x)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3018,"status":"ok","timestamp":1713478644566,"user":{"displayName":"Simón Rodríguez","userId":"08248044722851115137"},"user_tz":-120},"id":"tXhDae8pIdUE","outputId":"e4b4d963-5c08-4d45-af16-83ef0e8fdc7b"},"outputs":[{"name":"stdout","output_type":"stream","text":["3.65 ms ± 141 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"]}],"source":["# Medimos el tiempo de LDA Tensorizado\n","%%timeit\n","\n","ten_lda.fit(train_x, train_y)\n","ten_lda.predict(test_x)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5,"status":"ok","timestamp":1713478644566,"user":{"displayName":"Simón Rodríguez","userId":"08248044722851115137"},"user_tz":-120},"id":"mv30_Z-dIfqB","outputId":"024d0352-4c91-4fd2-c942-59889ea55828"},"outputs":[{"name":"stdout","output_type":"stream","text":["Tensorized LDA Accuracy: 0.9927007299270073\n","LDA Accuracy: 0.9927007299270073\n","Matches? True\n"]}],"source":["# Comparamos con LDA\n","\n","def accuracy_score(y_true, y_pred):\n","  return (y_true == y_pred).mean()\n","\n","ten_lda_acc = accuracy_score(test_y, ten_lda.predict(test_x))\n","lda_acc = accuracy_score(test_y, lda.predict(test_x))\n","matches = (ten_lda.predict(test_x) == lda.predict(test_x)).all()\n","\n","print(f\"Tensorized LDA Accuracy: {ten_lda_acc}\")\n","print(f\"LDA Accuracy: {lda_acc}\")\n","print(f\"Matches? {matches}\")"]},{"cell_type":"markdown","metadata":{"id":"GnV9wq1IH6Ny"},"source":["#### Respuesta:\n","El método LDA Tensorizado es más rápido que el LDA implementado antes y con un nivel de accuracy similar."]},{"cell_type":"markdown","metadata":{"id":"sUcSY9HWDvod"},"source":["### *2.2.2 Punto 2*"]},{"cell_type":"markdown","metadata":{"id":"4JTwe9uOKIV1"},"source":["LDA Tensorized - Referencia\n","\n","```Python\n","class TensorizedLDA(LDA):\n","\n","    def _fit_params(self, X, y):\n","        # ask plain LDA to fit params\n","        super()._fit_params(X,y)\n","\n","        # stack onto new dimension\n","        self.tensor_inv_cov_pooled = self.inv_pooled_cov\n","        #No es necesario el stack, ya que solo tenemos una\n","        #matriz inversa agrupada a diferencia de QDA\n","        self.tensor_means = np.stack(self.means)\n","\n","        #Precómputo de mu^T * Sigma^{-1}\n","        self.precomp = self.tensor_means.transpose(0,2,1) @ self.tensor_inv_cov_pooled\n","\n","    def _predict_log_conditionals(self,x):\n","        unbiased_x = x - 0.5*self.tensor_means\n","        calc = self.precomp @ unbiased_x\n","        return calc.flatten()\n","\n","    def _predict_one(self, x):\n","        # return the class that has maximum a posteriori probability\n","        max_class = np.argmax(self.log_a_priori + self._predict_log_conditionals(x))\n","        return max_class\n","  ```"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":4,"status":"ok","timestamp":1713478644567,"user":{"displayName":"Simón Rodríguez","userId":"08248044722851115137"},"user_tz":-120},"id":"VJ-6nAv9I1OQ"},"outputs":[],"source":["class FasterLDA(TensorizedLDA):\n","  #Based on FasterQDA with diagonal\n","\n","    def _predict_log_conditionals(self,x):\n","      unbiased_x = x - 0.5*self.tensor_means\n","      calc = self.precomp @ unbiased_x\n","      return calc.squeeze(axis=1)\n","\n","    def predict(self, X):\n","\n","      log_priors_reshaped = self.log_a_priori.reshape(-1, 1)  # Añadir una dimensión para broadcasting\n","\n","      # We dont need _predict_one method anymore. Now we predict all at once!\n","      encoded_y_hat = np.argmax(log_priors_reshaped + self._predict_log_conditionals(X), axis=0)\n","      y_hat = self.encoder.names[encoded_y_hat]\n","\n","      # return prediction as a row vector (matching y)\n","      return y_hat.reshape(1,-1)"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":2,"status":"ok","timestamp":1713478644567,"user":{"displayName":"Simón Rodríguez","userId":"08248044722851115137"},"user_tz":-120},"id":"R_HK8bRbNtSd"},"outputs":[],"source":["# Iniciamos los modelos:\n","lda     = LDA()\n","ten_lda =  TensorizedLDA()\n","faster_lda = FasterLDA()"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":510,"status":"ok","timestamp":1713478645075,"user":{"displayName":"Simón Rodríguez","userId":"08248044722851115137"},"user_tz":-120},"id":"NvTVABXgNzN_"},"outputs":[],"source":["# Obtenemos los datos del primer dataset: Iris\n","X_full, y_full = get_iris_dataset()\n","train_x, train_y, test_x, test_y = split_transpose(X_full, y_full, 0.4, rng_seed)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3505,"status":"ok","timestamp":1713478648578,"user":{"displayName":"Simón Rodríguez","userId":"08248044722851115137"},"user_tz":-120},"id":"Pjf-y7P1N3DP","outputId":"9c0c0be3-1190-453f-fef1-099f62b2e8bd"},"outputs":[{"name":"stdout","output_type":"stream","text":["4.48 ms ± 103 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"]}],"source":["# Medimos el tiempo de LDA\n","%%timeit\n","\n","lda.fit(train_x, train_y)\n","lda.predict(test_x)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":19736,"status":"ok","timestamp":1713478668311,"user":{"displayName":"Simón Rodríguez","userId":"08248044722851115137"},"user_tz":-120},"id":"LFK2nFO3N3hK","outputId":"b5f33fa7-61d5-4e1d-b908-f942c1c197ca"},"outputs":[{"name":"stdout","output_type":"stream","text":["2.49 ms ± 562 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)\n"]}],"source":["# Medimos el tiempo de Tensorized LDA\n","%%timeit\n","\n","ten_lda.fit(train_x, train_y)\n","ten_lda.predict(test_x)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":7101,"status":"ok","timestamp":1713478675410,"user":{"displayName":"Simón Rodríguez","userId":"08248044722851115137"},"user_tz":-120},"id":"jSQLSDdNN7GQ","outputId":"23119388-3a82-401e-cc44-601d15b675ab"},"outputs":[{"name":"stdout","output_type":"stream","text":["831 µs ± 37.3 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)\n"]}],"source":["# Medimos el tiempo de FasterLDA\n","%%timeit\n","\n","faster_lda.fit(train_x, train_y)\n","faster_lda.predict(test_x)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":15,"status":"ok","timestamp":1713478675411,"user":{"displayName":"Simón Rodríguez","userId":"08248044722851115137"},"user_tz":-120},"id":"aiNN2CBFRYIg","outputId":"7ce8a3bc-debd-4193-bfc5-800b33611963"},"outputs":[{"name":"stdout","output_type":"stream","text":["LDA Accuracy: 0.9833333333333333\n","Tensorized LDA Accuracy: 0.9833333333333333\n","Faster LDA Accuracy: 0.9833333333333333\n","Matches? True\n"]}],"source":["# Comparamos con LDA y Tensorized LDA\n","\n","def accuracy_score(y_true, y_pred):\n","  return (y_true == y_pred).mean()\n","\n","lda_acc = accuracy_score(test_y, lda.predict(test_x))\n","print(f\"LDA Accuracy: {lda_acc}\")\n","ten_lda_acc = accuracy_score(test_y, ten_lda.predict(test_x))\n","print(f\"Tensorized LDA Accuracy: {ten_lda_acc}\")\n","faster_lda_acc = accuracy_score(test_y, faster_lda.predict(test_x))\n","print(f\"Faster LDA Accuracy: {faster_lda_acc}\")\n","\n","matches = (lda.predict(test_x) == faster_lda.predict(test_x)).all()\n","print(f\"Matches? {matches}\")"]},{"cell_type":"markdown","metadata":{"id":"yEA55jYbZq2I"},"source":["### Respuesta:\n","Se puede observar que FasterLDA es efectivamente más rápido que las otras dos versiones (LDA y TensorizedLDA)."]},{"cell_type":"markdown","metadata":{"id":"ruqWrAFjj-UB"},"source":["# 3 Preguntas teóricas"]},{"cell_type":"markdown","metadata":{"id":"U7oPeQ0z-pux"},"source":["1. En LDA se menciona que la función a maximizar puede ser, mediante operaciones, convertida en:\n","$$\n","\\log{f_j(x)} =  \\mu_j^T \\Sigma^{-1} (x- \\frac{1}{2} \\mu_j) + C'\n","$$\n","Mostrar los pasos por los cuales se llega a dicha expresión.\n","2. Explicar, utilizando las respectivas funciones a maximizar, por qué QDA y LDA son \"quadratic\" y \"linear\".\n","3. La implementación de QDA estima la probabilidad condicional utilizando `0.5*np.log(det(inv_cov)) -0.5 * unbiased_x.T @ inv_cov @ unbiased_x` que no es *exactamente* lo descrito en el apartado teórico ¿Cuáles son las diferencias y por qué son expresiones equivalentes?\n","\n","El espíritu de esta componente práctica es la de establecer un mínimo de trabajo aceptable para su entrega; se invita al alumno a explorar otros aspectos que generen curiosidad, sin sentirse de ninguna manera limitado por la consigna."]},{"cell_type":"markdown","metadata":{"id":"xqB8s29bM9rh"},"source":["### *3.1 Punto 1*\n","\n","Para LDA se asume que $X|_{G=j} \\sim \\mathcal{N}_p(\\mu_j, \\Sigma)$, es decir que las poblaciones no sólo siguen una distribución normal sino que son de igual matriz de covarianzas. Esto nos da la función de densidad:\n","\n","Esto nos da que la función de densidad para una clase $j$ es:\n","$$\n","f_j(x) = \\frac{1}{(2 \\pi)^\\frac{p}{2} \\cdot |\\Sigma|^\\frac{1}{2}} e^{- \\frac{1}{2}(x-\\mu_j)^T \\Sigma^{-1} (x- \\mu_j)}\n","$$\n","\n","Aplicando logaritmo (que al ser una función estrictamente creciente no afecta el cálculo de máximos/mínimos), queda:\n","\n","$$\n","\\log{f_j(x)} = \\log{(\\frac{1}{(2 \\pi)^\\frac{p}{2} \\cdot |\\Sigma|^\\frac{1}{2}} e^{- \\frac{1}{2}(x-\\mu_j)^T \\Sigma^{-1} (x- \\mu_j)})}\n","$$\n","\n","> Recordando propiedades de los logaritmos:\n","<br>\n","> $ \\log(a.b) = \\log(a) + \\log(b) $\n","\n","$$\n","\\log{f_j(x)} = \\log{(\\frac{1}{(2 \\pi)^\\frac{p}{2} \\cdot |\\Sigma|^\\frac{1}{2}})} - \\frac{1}{2}(x-\\mu_j)^T \\Sigma^{-1} (x- \\mu_j)\n","$$\n","\n","Podemos simplificar y agrupar las constantes, ya que son iguales para todas las clases. Es decir, aquellos términos que no dependan de $x$, por ejemplo: $ \\log{(\\frac{1}{(2 \\pi)^\\frac{p}{2} \\cdot |\\Sigma|^\\frac{1}{2}})} $. Entonces nos queda:\n","\n","$$\n","\\log{f_j(x)} = {- \\frac{1}{2}(x-\\mu_j)^T \\Sigma^{-1} (x- \\mu_j) + C}  _{(1)}\n","$$\n","\n","Donde C es una constante.\n","\n","Ahora consideremos el término: $(x-\\mu_j)^T \\Sigma^{-1} (x- \\mu_j)$\n","\n","\n","Este se puede expandir de la siguiente forma. Considerando la propiedad distributiva de matrices transpuestas $(A+B)^T = A^T + B^T $, tenemos:\n","\n","$(x-\\mu_j)^T \\Sigma^{-1} (x- \\mu_j) = (x^T-\\mu_j^T) \\Sigma^{-1} (x- \\mu_j)\n","$\n","\n","Ahora, por propiedad distributiva de la multiplicación de matrices, nos queda:\n","\n","$  = x^T \\Sigma^{-1}(x- \\mu_j) - \\mu_j^T \\Sigma^{-1}(x- \\mu_j) $\n","\n","$  = {(x^T \\Sigma^{-1} x) - (x^T \\Sigma^{-1}\\mu_j) - (\\mu_j^T \\Sigma^{-1} x) + (\\mu_j^T \\Sigma^{-1} \\mu_j)} _{(2)} $\n","\n","Nótese que $x$ y $\\mu_j$ son vectores columna y $\\Sigma^{-1}$ es una matriz simétrica. Por esta razón, la multiplicación del término $x^T \\Sigma^{-1}\\mu_j$ resulta en un escalar (vector fila por matriz por vector columna). La transpuesta de un escalar es el mismo escalar y por lo tanto:\n","\n","$x^T \\Sigma^{-1}\\mu_j = (x^T \\Sigma^{-1}\\mu_j)^T$\n","\n","\n","Ahora, considerando la propiedad de la transposición de un producto de matrices: $(ABC)^T = C^T B^T A^T$, podemos aplicar que:\n","\n","$(x^T \\Sigma^{-1}\\mu_j)^T = \\mu_j^T (\\Sigma^{-1})^T (x^T)^T $\n","\n","Además, la transpuesta de una matriz simétrica es ella misma, y $(A^T)^T = A$. Entonces:\n","\n","$ {x^T \\Sigma^{-1}\\mu_j = \\mu_j^T (\\Sigma^{-1})^T (x^T)^T = \\mu_j^T \\Sigma^{-1} x} _{(3)}\n","$\n","\n","Reemplazando (3) en (2), tenemos:\n","\n","$ = {(x^T \\Sigma^{-1} x) - (\\mu_j^T \\Sigma^{-1} x) - (\\mu_j^T \\Sigma^{-1} x) + (\\mu_j^T \\Sigma^{-1} \\mu_j)}  $\n","\n","$ (x-\\mu_j)^T \\Sigma^{-1} (x- \\mu_j) =  {(x^T \\Sigma^{-1} x) - 2(\\mu_j^T \\Sigma^{-1} x) + (\\mu_j^T \\Sigma^{-1} \\mu_j)} _{(4)}$\n","\n","Esto nos quedaría entonces si volvemos a (1) y reemplazamos por (4):\n","\n","$$\n","\\log{f_j(x)} = {- \\frac{1}{2} [(x^T \\Sigma^{-1} x) - 2(\\mu_j^T \\Sigma^{-1} x) + (\\mu_j^T \\Sigma^{-1} \\mu_j)]  + C}\n","$$\n","\n","Nótese que el término $ (x^T \\Sigma^{-1} x) $ no depende de la clase $j$ (es igual para todas las clases) y no afecta a la maximización, por lo que podemos agregarlo a la constante.\n","\n","> Nota: Este término corresponde a la forma cuadrática y es una diferencia clave en comparación con QDA, en dónde no se puede simplificar ni agregar a la constante ya que sí depende de la clase e influye en la maximización.\n","\n","$$\n","\\log{f_j(x)} = {- \\frac{1}{2} [- 2(\\mu_j^T \\Sigma^{-1} x) + (\\mu_j^T \\Sigma^{-1} \\mu_j)]  + C'}\n","$$\n","\n","Por propiedad distributiva:\n","\n","$$\n","\\log{f_j(x)} = {(\\mu_j^T \\Sigma^{-1} x) - \\frac{1}{2} (\\mu_j^T \\Sigma^{-1} \\mu_j)  + C'}\n","$$\n","\n","Por último, sacamos como factor comun a $ (\\mu_j^T \\Sigma^{-1}) $ y nos queda:\n","\n","$$\n","\\log{f_j(x)} = {(\\mu_j^T \\Sigma^{-1}) (x - \\frac{1}{2} \\mu_j)  + C'}\n","$$\n","\n","-------"]},{"cell_type":"markdown","metadata":{"id":"ulUrnOJVUM9F"},"source":["### *3.2 Punto 2*\n","El logaritmo de la función de densidad de QDA y LDA a maximizar nos permite entender la razón detrás del término \"quadratic\" y \"linear\" en el nombre de ambos modelos respectivamente.\n","\n","En el caso de QDA, con logaritmo de pdf a maximizar:\n","\n","$$\n","\\log{f_j(x)} = -\\frac{1}{2}\\log |\\Sigma_j| - \\frac{1}{2} (x-\\mu_j)^T \\Sigma_j^{-1} (x- \\mu_j) + C\n","$$\n","\n","Observamos que $x$ aparece en la forma cuadrática $(x-\\mu_j)^T \\Sigma_j^{-1} (x- \\mu_j)$. Esto nos indica que al momento de buscar una función para realizar el borde de decisión con respecto al input $x$, este va a tomar forma cuadrática.\n","\n","Por otro lado, en el caso de LDA, con logaritmo de pdf a maximizar:\n","\n","$$\n","\\log{f_j(x)} =  \\mu_j^T \\Sigma^{-1} (x- \\frac{1}{2} \\mu_j) + C'\n","$$\n","\n","Observamos que $x$ aparece en forma linear $\\mu_j^T \\Sigma^{-1} (x- \\frac{1}{2} \\mu_j)$. Por lo tanto, al buscar una función para realizar el borde de decisión con respecto al input $x$, este va a tomar forma linear."]},{"cell_type":"markdown","metadata":{"id":"SRrvmDUCUPHK"},"source":["### *3.3 Punto 3*\n","\n","La implementación de QDA estima la probabilidad condicional utilizando `0.5*np.log(det(inv_cov)) - 0.5 * unbiased_x.T @ inv_cov @ unbiased_x` a diferencia del apartado teórico, dónde la función a maximizar es:\n","\n","$$\n","f_j(x) = \\frac{1}{(2 \\pi)^\\frac{p}{2} \\cdot |\\Sigma_j|^\\frac{1}{2}} e^{- \\frac{1}{2}(x-\\mu_j)^T \\Sigma_j^{-1} (x- \\mu_j)}\n","$$\n","\n","Como se explicó en el apartado teórico, podemos aplicar logaritmo (que al ser una función estrictamente creciente no afecta el cálculo de máximos/mínimos), queda algo mucho más práctico de trabajar:\n","\n","$$\n","\\log{f_j(x)} = -\\frac{1}{2}\\log |\\Sigma_j| - \\frac{1}{2} (x-\\mu_j)^T \\Sigma_j^{-1} (x- \\mu_j) + C\n","$$\n","\n","Observar que en este caso $C=-\\frac{p}{2} \\log(2\\pi)$, pero no se tiene en cuenta ya que al tener una constante aditiva en todas las clases, no afecta al cálculo del máximo.\n","\n","Al comparar la implementación en código con la función descrita en el apartado teórico, vemos en primer lugar que no se incorpora la constante $C$ mencionada en el párrafo anterior.\n","\n","Además vemos que en la transformación de $f_j(x)$ a $\\log{f_j(x)}$, la expresión:\n","\n","$$\n","\\frac{1}{|\\Sigma_j|^\\frac{1}{2}} = |\\Sigma_j|^{-\\frac{1}{2}}\n","$$\n","\n","se desarrolló como:\n","\n","$$\n","-\\frac{1}{2}\\log |\\Sigma_j|\n","$$\n","\n","mientras que en el código se desarrolló como:\n","\n","$$\n","\\frac{1}{2}\\log |\\Sigma_j^{-1}|\n","$$\n","\n","es decir `0.5*np.log(det(inv_cov))`.\n","\n","Por otro lado, el segundo término de la función aparece en el código como representación de su expresión matemática `0.5 * unbiased_x.T @ inv_cov @ unbiased_x`."]},{"cell_type":"markdown","metadata":{"id":"B8u9DjoTkHzq"},"source":["# 4 Ejercicio teórico"]},{"cell_type":"markdown","metadata":{"id":"CpOoxE2d-mOY"},"source":["\n","\n","Sea una red neuronal de dos capas, la primera de 3 neuronas y la segunda de 1 con los parámetros inicializados con los siguientes valores:\n","$$\n","w^{(1)} =\n","\\begin{pmatrix}\n","0.1 & -0.5 \\\\\n","-0.3 & -0.9 \\\\\n","0.8 & 0.02\n","\\end{pmatrix},\n","b^{(1)} = \\begin{pmatrix}\n","0.1 \\\\\n","0.5 \\\\\n","0.8\n","\\end{pmatrix},\n","w^{(2)} =\n","\\begin{pmatrix}\n","-0.4 & 0.2 & -0.5\n","\\end{pmatrix},\n","b^{(2)} = 0.7\n","$$\n","\n","y donde cada capa calcula su salida vía\n","\n","$$\n","y^{(i)} = \\sigma (w^{(i)} \\cdot x^{(i)}+b^{(i)})\n","$$\n","\n","donde $\\sigma (z) = \\frac{1}{1+e^{-z}}$ es la función sigmoidea .\n","\n","\\\\\n","Dada la observación $x=\\begin{pmatrix}\n","1.8 \\\\\n","-3.4\n","\\end{pmatrix}$, $y=5$ y la función de costo $J(\\theta)=\\frac{1}{2}(\\hat{y}_\\theta-y)^2$, calcular las derivadas de J respecto de cada parámetro $w^{(1)}$, $w^{(2)}$, $b^{(1)}$, $b^{(2)}$.\n","\n","*Nota: Con una sigmoidea a la salida jamás va a poder estimar el 5 \"pedido\", pero eso no afecta al mecanismo de backpropagation!*"]},{"cell_type":"markdown","metadata":{"id":"NqMY8fzAgOBk"},"source":["***Resolución***\n","\n","---"]},{"cell_type":"markdown","metadata":{"id":"Gz6o5f0QgacV"},"source":["![Mathematics for machine learning - M. Disenroth - Figure 5.8](resources/Mathematics%20for%20machine%20learning%20-%20M.%20Disenroth%20-%20Figure%205.8.PNG \"Mathematics for machine learning - M. Disenroth - Figure 5.8\")"]},{"cell_type":"markdown","metadata":{"id":"_9uER9bSggCd"},"source":["## ***Marco teórico***\n","\n","Partiendo de las siguientes definiciones:\n","\n","- $f_0 := \\bar{x}$\n","- $f_i := \\sigma_i (A_{i-1} \\cdot f_i + b_i), i=1,...,k$\n","- ${\\frac{\\partial L}{\\partial \\theta_i}} = {\\frac{\\partial L}{\\partial f_k}} \\cdot {\\frac{\\partial f_k}{\\partial f_{k-1}}} \\cdots {\\frac{\\partial f_{i+2}}{\\partial f_{i+1}}} \\cdot {\\frac{\\partial f_{i+1}}{\\partial \\theta_i}}$\n","\n","Para la siguiente función de activación $ \\sigma(z) =  \\frac{\\mathrm{1}}{\\mathrm{1} + e^{-z}} $ y la siguiente función de costo $ L = \\frac{1}{2}(f_k(\\theta, x)-y)^2 $ se obtienen las siguiente gradientes para cada capa:\n","\n","**CAPA K:**\n","\n","${\\frac{\\partial L}{\\partial \\theta_{k-1}}} = {\\frac{\\partial L}{\\partial f_k}} \\cdot {\\frac{\\partial f_k}{\\partial \\theta_{k-1}}}$, donde:\n","- ${\\frac{\\partial L}{\\partial f_k}}$ es como varía el error con respecto a la función $f_k$. Como la función de error está compuesta por la función de activación $\\sigma_z$, entonces aplicando la regla de la cadena tenemos que:\n","\n","$$\n","{\\frac{\\partial L}{\\partial f_k}} = \\underbrace{{\\frac{\\partial L}{\\partial \\sigma_k}}}_{\\substack{(f_k - y)}} \\cdot \\underbrace{{\\frac{\\partial \\sigma_k}{\\partial f_k}}}_{\\substack{f_k \\cdot (1 - f_k)}} \\Rightarrow {\\frac{\\partial L}{\\partial f_k}} = (f_k - y) \\cdot f_k \\cdot (1 - f_k)\n","$$\n","\n","- ${\\frac{\\partial f_k}{\\partial \\theta_{k-1}}}$ es como varía la función $f_k$ con respecto a los parámetros (pesos). En este caso: $\\theta = \\left\\{ \\overset{w^{(1)}}{A_0}, \\overset{b^{(1)}}{b_0}, \\overset{w^{(2)}}{A_1}, \\overset{b^{(2)}}{A_1} \\right\\}$.\n","\n","> ${\\color{Grey}{Nota: \\theta = \\left\\{ A_0, b_0, ..., A_{k-1}, b_{k-1}\\right\\}}}$\n","\n","$\\Rightarrow {\\frac{\\partial L}{\\partial \\theta_{k-1}}} = \\left [ {\\frac{\\partial f_k}{\\partial A_{k-1}}}, {\\frac{\\partial f_k}{\\partial b_{k-1}}} \\right ]$, como ${\\frac{\\partial f_k}{\\partial A_{k-1}}} = f_{k-1}$ y ${\\frac{\\partial f_k}{\\partial b_{k-1}}} = 1$ $\\Rightarrow$ $\\left [ f_{k-1}, 1 \\right ]$\n","\n","Por lo tanto, si llamamos $\\delta^k = (f_{k} - y) \\cdot f_k \\cdot (1 - f_k)$ entonces el gradiente de la *capa k* es:\n","\n","$$\n","{\\frac{\\partial L}{\\partial \\theta_{k-1}}} = \\boxed{\\nabla_k = \\left [ \\delta^k \\cdot f_{k-1}, \\delta^k \\right ]}\n","$$\n","\n","**CAPA K-1:**\n","\n","Para esta capa es necesario calcular únicamente como varía la salida de la capa anterior con respecto a ésta. Partiendo de:\n","\n","$$\n","{\\frac{\\partial L}{\\partial \\theta_{k-2}}} = {\\frac{\\partial L}{\\partial f_k}} \\cdot \\underbrace{{\\frac{\\partial f_k}{\\partial f_{k-1}}}}_{\\substack{\\text{Variación} \\\\ \\text{entre capas}}} \\cdot {\\frac{\\partial f_{k-1}}{\\partial \\theta_{k-2}}}\n","$$\n","\n","Tenemos que calcular únicamente ${\\frac{\\partial f_k}{\\partial f_{k-1}}}$ ya que ${\\frac{\\partial L}{\\partial f_k}} = \\delta^k$ y ${\\frac{\\partial f_{k-1}}{\\partial \\theta_{k-2}}} = \\left [ f_{k-2}, 1 \\right ]$\n","\n","Entonces, aplicando la regla de la cadena (ya que $f$ es una composición de la función de activación) tenemos que:\n","\n","$$\n","{\\frac{\\partial f_k}{\\partial f_{k-1}}} = \\underbrace{{\\frac{\\partial f_k}{\\partial \\sigma_{k-1}}}}_{\\substack{A_{k-1}}} \\cdot \\underbrace{{\\frac{\\partial \\sigma_{k-1}}{\\partial f_{k-1}}}}_{\\substack{(f_{k-1} - y) \\cdot f_k \\cdot (1 - f_k)}}\n","$$\n","\n","Si llamamos $\\delta^{k-1} = \\delta^k \\cdot f_{k-1} \\cdot (1 - f_{k-1}) \\cdot A_{k-1}$, entonces tenemos que el gradiente de la *capa k-1* es:\n","\n","$$\n","{\\frac{\\partial L}{\\partial \\theta_{k-2}}} = \\boxed{\\nabla_{k-1} = \\left [ \\delta^{k-1} \\cdot f_{k-2}, \\delta^{k-1} \\right ]}\n","$$"]},{"cell_type":"markdown","metadata":{"id":"oZaqDqc8gn5e"},"source":["## ***Ejercicio***\n","\n","> Dada la observación $x=\\begin{pmatrix}\n","1.8 \\\\\n","-3.4\n","\\end{pmatrix}$, $y=5$ y la función de costo $L(\\theta)=\\frac{1}{2}(\\hat{y}_\\theta-y)^2$, calcular las derivadas de J respecto de cada parámetro $w^{(1)}$, $w^{(2)}$, $b^{(1)}$, $b^{(2)}$.\n","\n","![Neural Net](resources/Neural%20net.PNG)\n","\n","- $f_0 =$\n","$ \\begin{pmatrix}\n","1.8 \\\\\n","-3.4\n","\\end{pmatrix} $\n","\n","- $f_1 = \\sigma(w^{(1)} \\cdot f_0 + b^{(1)})$\n","\n","$\\Rightarrow$ $f_1=$\n","$ \\begin{pmatrix}\n","0.1 & -0.5 \\\\\n","-0.3 & -0.9 \\\\\n","0.8 & 0.02\n","\\end{pmatrix} $\n","$ \\begin{pmatrix}\n","1.8 \\\\\n","-3.4\n","\\end{pmatrix} $\n","$+$\n","$ \\begin{pmatrix}\n","0.1 \\\\\n","0.5 \\\\\n","0.8\n","\\end{pmatrix} $\n","$\\Rightarrow$ $f_1 = \\sigma$\n","$ \\begin{pmatrix}\n","1.98 \\\\\n","3.02 \\\\\n","2.17\n","\\end{pmatrix}  $\n","\n","$\\therefore \\boxed{f_1=\n","\\begin{pmatrix}\n","0.88 \\\\\n","0.95 \\\\\n","0.90\n","\\end{pmatrix}\n","}$\n","\n","- $f_1 = \\sigma(w^{(2)} \\cdot f_1 + b^{(2)})$\n","\n","$\\Rightarrow$ $f_2 =$\n","$ \\begin{pmatrix}\n","-0.4 \\\\\n","0.2 \\\\\n","-0.5\n","\\end{pmatrix}^T $\n","$\\begin{pmatrix}\n","0.88 \\\\\n","0.95 \\\\\n","0.90\n","\\end{pmatrix}$\n","$+$ $0.7$\n","$\\Rightarrow$ $f_2=\\sigma(0.09)$\n","\n","$\\therefore \\boxed{f_2 = 0.52}$\n","\n","> ${\\color{Gray}{Nota: L = \\frac{1}{2}(0.52-5)^2 = 10.03}}$\n","\n","**CAPA 2:**\n","\n","$\\nabla_2 L = \\left [ \\delta^{(2)} \\cdot f_{1}, \\delta^{(2)} \\right ]$, donde $\\delta^2 = (f_2 - y) \\cdot f_2 \\cdot (1 - f_2)$ $\\Rightarrow$ $\\delta^2 = (0.52 - 5) \\cdot 0.52 \\cdot (1 - 0.52)$ $\\Rightarrow$ $\\delta^{(2)} = -1.11$\n","\n","$$\n","\\therefore \\nabla_2 L = \\left [ (-1.11)\n","\\begin{pmatrix}\n","0.88 \\\\\n","0.95 \\\\\n","0.90\n","\\end{pmatrix}\n",", -1.11\n","\\right ]\n","\\Rightarrow \\boxed{\\nabla_2 L = \\left [\n","\\underbrace{\n","\\begin{pmatrix}\n","-0.98 \\\\\n","-1.05 \\\\\n","1.00\n","\\end{pmatrix}\n","}_{\\substack{w^2}}\n",",\n","\\underbrace{-1.11}_{\\substack{b^2}}\n","\\right ]\n","}\n","$$\n","\n","**CAPA 1:**\n","\n","$\\nabla_2 L = \\left [ \\delta^{(1)} \\cdot f_{0}, \\delta^{(1)} \\right ]$, donde $\\delta^1 = \\delta^2 \\cdot f_1 \\cdot (1 - f_1) \\cdot \\underbrace{A_1}_{\\substack{w^2}}$\n","\n","$\\Rightarrow \\delta^{(1)} = (-1.11)$\n","$\\begin{pmatrix}\n","0.88 \\\\\n","0.95 \\\\\n","0.90\n","\\end{pmatrix}$\n","$\\left [ 1 -\n","\\begin{pmatrix}\n","0.88 \\\\\n","0.95 \\\\\n","0.90\n","\\end{pmatrix}\n","\\right ]$\n","$\\begin{pmatrix}\n","-0.4 \\\\\n","0.2 \\\\\n","-0.5\n","\\end{pmatrix}$\n","$=$\n","$ \\begin{pmatrix}\n","0.11 \\\\\n","-0.05 \\\\\n","0.13\n","\\end{pmatrix} $\n","\n","$$\n","\\therefore \\nabla_1 L = \\left [\n","\\begin{pmatrix}\n","0.11 \\\\\n","-0.05 \\\\\n","0.13\n","\\end{pmatrix}\n","\\begin{pmatrix}\n","1.8 \\\\\n","-3.4\n","\\end{pmatrix}^T\n",",\n","\\begin{pmatrix}\n","0.11 \\\\\n","-0.05 \\\\\n","0.13\n","\\end{pmatrix}\n","\\right ]\n","\\Rightarrow \\boxed{\\nabla_1 L = \\left [\n","\\underbrace{\n","\\begin{pmatrix}\n","0.20 & -0.37 \\\\\n","-0.09 & 0.17 \\\\\n","0.23 & 0.44\n","\\end{pmatrix}\n","}_{\\substack{w^1}}\n",",\n","\\underbrace{\n","\\begin{pmatrix}\n","0.11 \\\\\n","-0.05 \\\\\n","0.13\n","\\end{pmatrix}\n","}_{\\substack{b^1}}\n","\\right ]\n","}\n","$$"]},{"cell_type":"markdown","metadata":{"id":"9uBIJY54kSec"},"source":["# 5 Preguntas en el código"]},{"cell_type":"markdown","metadata":{"id":"z-CNlW06-i-u"},"source":["\n","Previamente las preguntas \"técnicas\" en comentarios en el código eran parte del TP, y buscaban que el alumno logre entrar en el detalle de por qué cada linea de código es como es y en el orden en el que está. Ya no forman parte de la consigna, pero se aconseja al alumno intentar responderlas."]}],"metadata":{"colab":{"provenance":[],"toc_visible":true},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.3"}},"nbformat":4,"nbformat_minor":0}
