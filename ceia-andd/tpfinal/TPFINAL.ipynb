{"cells":[{"cell_type":"markdown","metadata":{"id":"51zGEEc86yfw"},"source":["# <h1><center>Rain in Australia</center></h1>\n"]},{"cell_type":"markdown","metadata":{"id":"hHf3lwp16yfy"},"source":["<center>\n","<img src=\"https://drive.google.com/thumbnail?id=1jeVAK1A6OcNi-bz8ha4_NFbWVXGMJ7Zw&sz=w3000\" width=\"500\" alt=\"Figura 1: Datos meterológicos de Australia del 2008 al 2009, obtenidos de http://www.bom.gov.au/climate/history/enso/\">\n","\n","<small><em>Figura 1: Datos meterológicos de Australia del 2008 al 2009, obtenidos de http://www.bom.gov.au/climate/history/enso/</em></small>\n","</center>"]},{"cell_type":"markdown","metadata":{"id":"Bp8iLkP_6yf0"},"source":["<center>\n","<em>Datos del proyecto:</em>\n","\n","| Subtitulo   | Trabajo final de Análisis de Datos - FIUBA                                                                                                     |\n","| ----------- | ---------------------------------------------------------------------------------------------------------------------------------------------- |\n","| **Descrpción**  | Análisis de datos meteorológicos de Australia con el objetivo de predecir si lloverá al otro día                          |\n","| **Integrantes** | • Juan Cruz Ferreyra (ferreyra.juancruz95@gmail.com)<br>• Simón Rodriguez (simon.andre.r@gmail.com)<br>• Bruno Masoller (brunomaso1@gmail.com) |\n","\n","</center>"]},{"cell_type":"markdown","metadata":{"id":"wJHYo9L46yf3"},"source":["## 1. Metodología"]},{"cell_type":"markdown","metadata":{"id":"FBbRkZu1FMRV"},"source":["En el campo de aprendizaje de máquina, la comunidad todavía está definiendo un proceso sistemático y estructurado para el cliclo de vida de soluciones basadas en aprendizaje automático. El objetivo es enfocar los procedimientos y estándares de calidad de la ingeniería de software clásica a metodologías de este sub-campo de la inteligencia artificial.\n","\n","Es en este punto que surge [CRISP-ML(Q)](https://arxiv.org/pdf/2003.05155.pdf), como una metodología que integra las mejores prácticas de la ingeniería de software sobre todo el ciclo de vida de soluciones enfocadas en resolver problemas con aprendizaje de máquina.\n","\n","CRISP-ML(Q) surge a partir de [CRISP-DM](https://es.wikipedia.org/wiki/Cross_Industry_Standard_Process_for_Data_Mining) como un intento de ampliar dicho \"framework\" al área de \"machine learning\".\n","\n","En palabras de los autores (Stefan Studer et al),  CRISP-ML(Q) propone un modelo de proceso al que llaman modelo de proceso estándar \"CRoss-Industry\" para el desarrollo de aplicaciones de \"Machine Learning\" con metodología de aseguramiento de la Calidad, donde resalta su compatibilidad con CRISP-DM. Está diseñado para el desarrollo de aplicaciones de máquina, es decir, escenarios de aplicaciones donde se implementa y mantiene un modelo de ML\n","como parte de un producto o servicio.\n","\n","Consta de seis fases:\n","<em>\n","\n","1. Business and Data Understanding\n","2. Data Engineering (Data Preparation)\n","3. Machine Learning Model Engineering (Modeling)\n","4. Quality Assurance for Machine Learning Applications\n","5. Deployment\n","6. Monitoring and Maintenance.\n","\n","</em>\n"]},{"cell_type":"markdown","metadata":{"id":"8QsDZEPhAKp9"},"source":["<center>\n","<img src=\"https://drive.google.com/thumbnail?id=1Aoiu62mQCrICj34T6eTHFRJLfmsXxkfS&sz=w2000\" width=\"500\" alt=\"Figura 2: Machine Learning Development Life Cycle Process, obtenido de https://ml-ops.org/content/crisp-ml\">\n","\n","<small><em>Figura 2: Machine Learning Development Life Cycle Process, obtenido de https://ml-ops.org/content/crisp-ml</em></small>\n","</center>"]},{"cell_type":"markdown","metadata":{"id":"XquI0FpRAKp9"},"source":["En donde cada fase requiere el siguiente proceso:"]},{"cell_type":"markdown","metadata":{"id":"tLT-k3pEAKp-"},"source":["<center>\n","<img src=\"https://drive.google.com/thumbnail?id=1BtP076AUQEzeK3gQO0fP8DuOZHQhnS56&sz=w1000\" width=\"500\" alt=\"Figura 3: Proceso dentro de cada fase\">\n","\n","<small><em>Figura 3: Proceso dentro de cada fase</em></small>\n","</center>"]},{"cell_type":"markdown","metadata":{"id":"CqylHIxaAKp-"},"source":["Resumen de tareas de cada fase:\n","*También se agrega infromación adiccional. Toada esta tabla se puede tomar como una especie de \"checklist\".*"]},{"cell_type":"markdown","metadata":{"id":"XRhWSWM96yf6"},"source":["| CRISP-ML(Q) Phase                | Tasks                                                                                                                                                                             |\n","| -------------------------------- | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |\n","| *Business and Data Understanding*  | • Define the Scope of the ML Application<br>• Sucess criteria<br>• Feasibility<br>• Data collection<br>• Data quality verification<br>• Review of output documents                |\n","| *Data Engineering*                 | • Select data<br>• Clean data<br>• Construct data<br>• Standarize data                                                                                                            |\n","| *ML Model Engineering*             | • Modeling<br>• Assure reproducibility                                                                                                                                            |\n","| *ML Model Evaluation*              | • Validate performance<br>• Determine robustness<br>•Increase explainability for ML practitioner and end user<br>• Compare results with defined success criteria                  |\n","| *Model Deployment*                 | • Define inference hardware<br>• Model evaluation under production<br>• Assure user acceptance and usability<br>• Minimize the risks of unforseen errors<br>• Deployment strategy |\n","| *Model Monitoring and Maintenance* | • Monitor<br>• Update                                                                                                                                                             |"]},{"cell_type":"markdown","metadata":{"id":"T8V3Zre56yf7"},"source":["<small>*Tabla 1: Fases y tareas de CRISP-ML(Q)*</small>"]},{"cell_type":"markdown","metadata":{"id":"DEaomgw-6yf8"},"source":["Un ejemplo de algunos de los modelos más utilizados se puede observar en la siguiente imagen:"]},{"cell_type":"markdown","metadata":{"id":"-COu424RAKqA"},"source":["<center>\n","<img src=\"https://drive.google.com/thumbnail?id=1-el9MGC3Ouc0IFCaQD9B9477x4H561IP&sz=w1000\" width=\"500\" alt=\"Figura 4: Machine learning models example\">\n","\n","<small><em>Figura 4: Machine learning models example</em></small>\n","</center>"]},{"cell_type":"markdown","metadata":{"id":"mx2_fn5B6yf8"},"source":["## 2. CRISP-ML(Q)"]},{"cell_type":"markdown","metadata":{"id":"4Flaj9ZT6yf9"},"source":["> *Notas sobre la aplicación del Método*: Dada la acotación planteada para el trabajo, no se tienen en cuenta todas las fases (solamente aquellas que son acotadas a los temas vistos en el curso), ni tampoco el análisis de riesgo que plantea el modelo dentro de cada fase. Se plantean como mejoras posteriores."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"executionInfo":{"elapsed":38237,"status":"ok","timestamp":1718156995862,"user":{"displayName":"Bruno Masoller","userId":"01529176355565509322"},"user_tz":180},"id":"9dal2D5sTEtf","outputId":"26dd56fa-4103-44ef-a8d2-4edf9fe22d8c"},"outputs":[],"source":["# Instalaciones de paquetes del SO\n","!apt-get -qq install -y libspatialindex-dev\n","\n","# Instalacion de paquetes de python\n","!pip install gdown\n","!pip install ydata-profiling\n","!pip install -q -U osmnx"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":10,"status":"ok","timestamp":1718156995863,"user":{"displayName":"Bruno Masoller","userId":"01529176355565509322"},"user_tz":180},"id":"B7EELiJIAKqB"},"outputs":[],"source":["# Descargamos el archivo de utilidades\n","# !gdown 1eNWQJR08ajXPx9tiT1KK1ylEY1uZiiMb"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":6122,"status":"ok","timestamp":1718157001978,"user":{"displayName":"Bruno Masoller","userId":"01529176355565509322"},"user_tz":180},"id":"JWNbxJeJ6yf9"},"outputs":[],"source":["# Importacion de librerias\n","import sys  # Interactuar con el sistema\n","import statsmodels.api as sm  # Regresión lineal\n","import sklearn\n","import seaborn as sns  # Visualización de datos estadísticos\n","import scipy.stats as stats\n","import re  # Expresiones regulares\n","import random\n","import pandas as pd  # Procesamiento de datos\n","import osmnx as ox  # OpenStreetMap\n","import os\n","import numpy as np  # Albegra lineal\n","import matplotlib.pyplot as plt  # Visualización de datos\n","import json\n","import geopandas as gpd  # Georeferenciacion\n","from ydata_profiling import ProfileReport  # Reporte (profiling)\n","from sklearn.preprocessing import OneHotEncoder, StandardScaler\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import (accuracy_score, confusion_matrix, f1_score, precision_score, recall_score)\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn.decomposition import PCA\n","from sklearn.impute import SimpleImputer, KNNImputer\n","from shapely.geometry import Point  # Geometría espacial\n","from scipy.stats import chi2_contingency  # Test de chi-cuadrado\n","from pathlib import Path\n","from itertools import chain, combinations  # Iteradores\n","from IPython import display\n","from geopandas.datasets import get_path  # Ruta de los datos geográficos\n","# from utils import *\n","\n","display.clear_output()\n","%matplotlib inline\n","# Linea mágica para mostrar los graficos dentro del notebook"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":26,"status":"ok","timestamp":1718157001979,"user":{"displayName":"Bruno Masoller","userId":"01529176355565509322"},"user_tz":180},"id":"umYsLp83WfRW"},"outputs":[],"source":["# Configuración del notebook\n","pd.options.mode.chained_assignment = None\n","\n","debug_mode = False # Modo más informativo\n","show_profile = False # Muestra el perfilado de los datos\n","spectral_palette = [ \"#9e0142\", \"#d53e4f\", \"#f46d43\", \"#fdae61\", \"#fee08b\",\n","                    \"#ffffbf\", \"#e6f598\", \"#abdda4\", \"#66c2a5\", \"#3288bd\", \"#5e4fa2\"] # Paleta de colores\n","random_state = 42 # Semilla para reproducibilidad"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":24,"status":"ok","timestamp":1718157001980,"user":{"displayName":"Bruno Masoller","userId":"01529176355565509322"},"user_tz":180},"id":"_vIfBzyYXYCV"},"outputs":[],"source":["#### Funciones auxiliares ####\n","def outliers_iqr(df, columns):\n","    outliers = []\n","    for col in columns:\n","        # Rango itercuartílico\n","        Q1 = df[col].quantile(0.25)\n","        Q3 = df[col].quantile(0.75)\n","        IQR = Q3 - Q1\n","        irq_lower_bound = Q1 - 1.5 * IQR\n","        irq_upper_bound = Q3 + 1.5 * IQR\n","\n","        # Desviación estándar\n","        mean = df[col].mean()\n","        std = df[col].std()\n","        std_lower_bound = mean - 3 * std\n","        std_upper_bound = mean + 3 * std\n","\n","        outliers.append({'Column': col,\n","                         'IRQ-Percentage': (((df[col] < irq_lower_bound) | (df[col] > irq_upper_bound)).sum() / len(df[col]) * 100).round(2),\n","                         '3Std-Percentage': (((df[col] < std_lower_bound) | (df[col] > std_upper_bound)).sum() / len(df[col]) * 100).round(2),\n","                         'IRQ-Count': ((df[col] < irq_lower_bound) | (df[col] > irq_upper_bound)).sum(),\n","                         '3Std-Count': ((df[col] < std_lower_bound) | (df[col] > std_lower_bound)).sum()\n","                         })\n","    return pd.DataFrame(outliers)\n","\n","def show_unique_types_as_df(df, columns):\n","    # Crear una lista para almacenar la información\n","    type_info = []\n","\n","    for column in columns:\n","        if column in df.columns:\n","            types = df[column].apply(lambda x: type(x) if not pd.isna(x) else np.nan)\n","            unique_types = types.unique()\n","            for t in unique_types:\n","                type_info.append({\n","                    'Column': column,\n","                    'Type': 'NaN' if t is np.nan else str(t)\n","                })\n","        else:\n","            type_info.append({\n","                'Column': column,\n","                'Type': 'Not in DataFrame'\n","            })\n","\n","    # Convertir la lista a un DataFrame\n","    type_info_df = pd.DataFrame(type_info)\n","\n","    return type_info_df\n","\n","# Definir una función personalizada para concatenar los valores\n","def concatenate_values(series):\n","    return ', '.join(series.astype(str))\n","\n","# Muestra los tipos únicos de datos en las columnas especificadas de un DataFrame, incluyendo NaN.\n","def show_unique_types(df, columns):\n","    for column in columns:\n","        if column in df.columns:\n","            types = df[column].apply(lambda x: type(x) if not pd.isna(x) else np.nan)\n","            unique_types = types.unique()\n","            print(f\"Columna '{column}' tiene los siguientes tipos únicos:\")\n","            for t in unique_types:\n","                if t is np.nan:\n","                    print(f\"  - NaN\")\n","                else:\n","                    print(f\"  - {t}\")\n","        else:\n","            print(f\"La columna '{column}' no está en el DataFrame\")\n","\n","# Imprimir los valores únicos de cada columna\n","def print_unique_values(unique_values):\n","  for col, values in unique_values.items():\n","      print(f\"Columna: {col}\")\n","      try:\n","        print(values.to_numpy())\n","      except:\n","        print(values)\n","      print()  # Imprimir una línea en blanco para separar las columna\n","\n","# Muestra el porcentaje de valores faltantes.\n","def print_missing_perc(df, column):\n","    missing_perc = round((df[column].isna().sum() / df.shape[0]) * 100, 1)\n","    print(f\"Porcentaje de valores faltantes en la columna {column}: {missing_perc}%\")\n","\n","# Evaluar las predicciones en una matriz de confuncion.\n","def evaluate_predictions(y_true, y_pred, figsize=(4, 4)):\n","    # Generate confusion matrix\n","    cm = confusion_matrix(y_true, y_pred)\n","\n","    # Mapping of labels\n","    labels = [\"No\", \"Yes\"]\n","\n","    # Plot confusion matrix\n","    plt.figure(figsize=figsize)\n","    sns.heatmap(\n","        cm,\n","        annot=True,\n","        fmt=\"d\",\n","        cmap=\"Greens\",\n","        xticklabels=labels,\n","        yticklabels=labels,\n","        cbar=False,\n","    )\n","    plt.xlabel(\"Predicted Values\")\n","    plt.ylabel(\"Real Values\")\n","    plt.title(\"Confusion Matrix\")\n","    plt.show()\n","\n","    # Calculate evaluation metrics\n","    precision = precision_score(y_true, y_pred)\n","    recall = recall_score(y_true, y_pred)\n","    f1 = f1_score(y_true, y_pred)\n","    accuracy = accuracy_score(y_true, y_pred)\n","\n","    # Print evaluation metrics\n","    print()\n","    print(f\"Accuracy: {accuracy:.2f}\")\n","    print(f\"Precision: {precision:.2f}\")\n","    print(f\"Recall: {recall:.2f}\")\n","    print(f\"F1 Score: {f1:.2f}\")\n","\n","def plot_locations_over_time(df, color=\"blue\"):\n","    locations = df[\"Location\"].unique()\n","\n","    _, ax = plt.subplots(figsize=(12, 7))\n","\n","    # Plot data for each location\n","    for location in locations:\n","        filt = df[\"Location\"] == location\n","        df_location = df.loc[filt, :]\n","\n","        # Plot observations present in dataframe\n","        ax.plot(\n","            df_location[\"Date\"],\n","            [location] * len(df_location),\n","            \"o-\",\n","            color=color,\n","            label=location,\n","            markersize=0.5,\n","            linewidth=0.05,\n","        )\n","\n","        # Plot null values in target column\n","        null_indices = df_location.loc[df_location[\"RainTomorrow\"].isnull()].index\n","        for idx in null_indices:\n","            ax.plot(df_location.loc[idx, \"Date\"], location, \"ko\", markersize=0.15)\n","\n","    # Customize the plot\n","    ax.set_yticks(np.arange(len(locations)))\n","    ax.set_yticklabels(\n","        locations, fontsize=\"x-small\"\n","    )  # Increase fontsize for y-axis labels\n","    ax.set_ylim(-0.5, len(locations) - 0.5)\n","\n","    xticks = pd.date_range(start=df[\"Date\"].min(), end=df[\"Date\"].max(), freq=\"6MS\")\n","    ax.set_xticks(xticks)\n","    ax.set_xticklabels(xticks.strftime(\"%Y-%m\"), fontsize=\"x-small\", rotation=90)\n","\n","    ax.grid(True, linestyle=\":\", alpha=0.5)\n","\n","    legend_handles = [\n","        plt.Line2D(\n","            [0],\n","            [0],\n","            marker=\"o\",\n","            color=\"w\",\n","            markerfacecolor=\"lightgreen\",\n","            markersize=5,\n","            label=\"Observación presente en el dataframe\",\n","        ),\n","        plt.Line2D(\n","            [0],\n","            [0],\n","            marker=\"o\",\n","            color=\"w\",\n","            markerfacecolor=\"black\",\n","            markersize=5,\n","            label=\"Observación con valor ausente en la columna 'RainTomorrow'\",\n","        ),\n","    ]\n","    ax.legend(\n","        handles=legend_handles,\n","        loc=\"upper center\",\n","        bbox_to_anchor=(0.5, 1.075),\n","        ncol=2,\n","        fontsize=7,\n","    )\n","\n","    plt.suptitle(\n","        \"Observaciones presentes en la serie temporal por centro meteorológico\",\n","        fontsize=10,\n","    )\n","\n","    plt.show()\n","\n","def phi_coefficient(confusion_matrix):\n","    chi2, p, _, _ = chi2_contingency(confusion_matrix)\n","    n = confusion_matrix.sum().sum()\n","    phi = np.sqrt(chi2 / n)\n","    return phi, p\n","\n","# Función para graficar un pie plot.\n","def plot_pie(df, column, ax):\n","    counts = df[column].value_counts()\n","    labels = counts.index\n","    sizes = counts.values\n","    ax.pie(sizes, labels=labels, autopct='%1.1f%%', startangle=140)\n","    ax.set_title(column)\n","\n","# Función para graficar un gráfico de barras\n","def plot_bar(df, column, ax):\n","    sns.countplot(data=df, x=column, ax=ax)\n","    ax.set_title(column)\n","    # Nota: Elimino las etiquetas en el eje de las x porque quedan muy apretadas\n","    ax.set_xlabel('')  # Eliminar la etiqueta del eje x\n","    ax.set_xticklabels([])  # Eliminar las etiquetas en el eje x\n","\n","# Función para graficar un histograma.\n","def plot_hist(df, column, ax):\n","    sns.histplot(df[column], kde=True, ax=ax)\n","    ax.set_title(column)\n","\n","# Función para graficar qq-plot\n","def plot_qq_plots(df, column, ax):\n","    sm.qqplot(df[column].dropna(), line ='45', fit=True, ax=ax)\n","    ax.set_title(f'QQ-plot for {column}')\n","\n","# Funcion para graficar boxplot\n","def plot_boxplot(df, column, ax):\n","    sns.boxplot(df[column], ax=ax)\n","    ax.set_title(column)\n","\n","def plot_heatmap(correlation_matrix, figsize=(8, 8)):\n","    plt.figure(figsize=figsize)\n","    sns.heatmap(correlation_matrix, annot=True, fmt=\".2f\", cmap=\"coolwarm\", vmin=-1, vmax=1, annot_kws={\"size\": 8}, cbar=False)\n","    plt.title(\"Heatmap de Correlación\")\n","\n","    plt.xticks(fontsize=8)  # Adjust x-axis font size\n","    plt.yticks(fontsize=8)  # Adjust y-axis font size\n","\n","    plt.show()\n","\n","def plot_graph_on_grid(df, columns, graph_type, num_cols=3, figsize=(10, 5)):\n","    num_rows = (len(columns) + num_cols - 1) // num_cols  # cálculo del número de filas\n","\n","    fig, axes = plt.subplots(num_rows, num_cols, figsize=figsize)\n","    axes = axes.flatten()\n","\n","    for i, col in enumerate(columns):\n","        if graph_type == 'pie':\n","            plot_pie(df, col, axes[i])\n","        elif graph_type == 'hist':\n","            plot_hist(df, col, axes[i])\n","        elif graph_type == 'bar':\n","            plot_bar(df, col, axes[i])\n","        elif graph_type == 'qq-plot':\n","            plot_qq_plots(df, col, axes[i])\n","        elif graph_type == 'box-plot':\n","            plot_boxplot(df, col, axes[i])\n","        else:\n","            raise Exception('Tipo de gráfico no soportado.')\n","\n","    # Elimina cualquier gráfico extra en la grilla\n","    for j in range(i + 1, len(axes)):\n","        fig.delaxes(axes[j])\n","\n","    plt.tight_layout()\n","    plt.show()"]},{"cell_type":"markdown","metadata":{"id":"a2wj7VaF6yf-"},"source":["### 2.1. Entendimiento del negocio y los datos (Business and Data Understanding)"]},{"cell_type":"markdown","metadata":{"id":"wbFftKSz6yf_"},"source":["#### 2.1.1. Definir el alcance de la solución"]},{"cell_type":"markdown","metadata":{"id":"R3o-0UF86yf_"},"source":["Este proyecto se plantea en el marco de la materia Análisis de Datos de la especialización en Inteligencia Artifical de la Facultad de Ingeniería de la Universidad de Buenos Aries; por lo tanto, es un proyecto con fines académicos.\n","\n","Como objetivo principal, se plantea lo siguiente:\n","\n","<mark>El objetivo es predecir si lloverá o no al día siguiente (variable RainTomorrow), en función de los datos meterológicos del día actual.</mark>\n","\n","En este caso, el conjunto a analizar corresponde a los datos meterológicos de Australia recolectados durante 10 años de varias ubicaciones.\n","Los datos fueron recolectados en base a observaciones diarias, por la oficina de meteorolgía de Australia (Bureau of Meteorology), los cuales están disponibles al público desde su página: http://www.bom.gov.au/\n","\n","De los tipos de análisis vistos en el curso, este entra dentro del análisis predictivo, donde la pregunta es *¿Qué ocurrirá?*"]},{"cell_type":"markdown","metadata":{"id":"yCOFfFx16yf_"},"source":["#### 2.1.2. Criterios de éxito"]},{"cell_type":"markdown","metadata":{"id":"XvjjtY9a6ygA"},"source":["Como criterio de éxito orientado al negocio, en este caso, dado que es un proyecto académico, se enfoca en interaccionar con todos los tópicos vistos en el curso. Dichos tópicos son:\n","- Análisis estadístico básico de los datos\n","- Caracterización de variables\n","- Imputación de datos faltantes\n","- Preparación de datos y validación de resultados\n","<!-- TODO: Completar con el resto de las herramientas y temas vistos en clase  -->\n","\n","Como criterio de éxito orientado al modelo de aprendizaje, se propone obtener una presición mayor al 90%, donde el enfoque sea en reducir la tasa de falsos negativos, ya que en este caso es más el impacto del usuario para dichos escenarios, en donde es preferible llevar un paraguas y que no llueva, a que no llevar un paraguas y que llueva.\n","\n","Como criterio de éxito orientado a lo económico, dado que es un proyecto académico, se plantea que el tiempo de dedicación se mantenga uniforme entre todos los integrantes del proyecto. Así como también completar el proyecto antes de una fecha límite."]},{"cell_type":"markdown","metadata":{"id":"CUv8u3CS6ygA"},"source":["#### 2.1.3. Factibilidad"]},{"cell_type":"markdown","metadata":{"id":"iZinSrS06ygA"},"source":["Dado que ya existe un conjunto de datos (https://www.kaggle.com/datasets/jsphyg/weather-dataset-rattle-package/data), y que dichos datos provienen de una buena fuente como lo es la oficina de meterología de Australia (también es un conjunto ámpliamente usado y calificado en \"kaggle\"), se considera factible los objetivos planteados."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"executionInfo":{"elapsed":1897,"status":"ok","timestamp":1718157003856,"user":{"displayName":"Bruno Masoller","userId":"01529176355565509322"},"user_tz":180},"id":"9D57vCUuHEA_","outputId":"8a218e56-5d2f-493f-a0d8-1c3b304d927c"},"outputs":[],"source":["# Descargamos el dataset\n","!gdown 1hEBhDvdBoXwNQPt-EpzYen6V2CzMK0cY"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":528,"status":"ok","timestamp":1718157004361,"user":{"displayName":"Bruno Masoller","userId":"01529176355565509322"},"user_tz":180},"id":"K1e89WOK6ygA","outputId":"1a665084-9cb8-49e6-f3bf-45ec24bc896e"},"outputs":[],"source":["# Cargar el dataset\n","\n","dataset_path = 'weatherAUS.csv'\n","\n","try:\n","    df = pd.read_csv(dataset_path)\n","    print(\"Archivo cargado correctamente.\")\n","except FileNotFoundError:\n","    print(f\"Error: El archivo '{dataset_path}' no se encuentra.\")\n","except Exception as e:\n","    print(f\"Ocurrió un error al importar el archivo: {e}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":24,"status":"ok","timestamp":1718157004361,"user":{"displayName":"Bruno Masoller","userId":"01529176355565509322"},"user_tz":180},"id":"HwSX6FS96ygA","outputId":"49497f53-f1d3-4ba4-985d-5ffb96d18d74"},"outputs":[],"source":["print('Tamaño del conjunto:', len(df))"]},{"cell_type":"markdown","metadata":{"id":"ZWEO4S3H6ygB"},"source":["<p><em>\n","El tamaño del conjunto es de <code>145460</code>, por lo que no se identifican problemas de disponibilidad ni de tamaño de los datos.\n","</em><p>"]},{"cell_type":"markdown","metadata":{"id":"9SKYnSee6ygB"},"source":["#### 2.1.4. Recolección de datos"]},{"cell_type":"markdown","metadata":{"id":"BhfSJhyo6ygB"},"source":["En este caso, el conjunto de datos ya fue recolectado y se descargaron de https://www.kaggle.com/datasets/jsphyg/weather-dataset-rattle-package/data, donde se brindan competencias sobre conjuntos determinados.\n","\n","Los datos no fueron actualizados desde que se compartieron y no se pretende que se actualicen, por lo que no es necesario un sistema de control de versiones de los datos."]},{"cell_type":"markdown","metadata":{"id":"aRkFXCOc6ygC"},"source":["#### 2.1.5. Verificación la de calidad de los datos"]},{"cell_type":"markdown","metadata":{"id":"diG_IjQY6ygC"},"source":["##### 2.1.5.1. Exploración de los datos"]},{"cell_type":"markdown","metadata":{"id":"iTUpFSHsTrSo"},"source":["Perfilado de los datos:"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":331,"status":"ok","timestamp":1718157004677,"user":{"displayName":"Bruno Masoller","userId":"01529176355565509322"},"user_tz":180},"id":"a-ho2L8mSyeF"},"outputs":[],"source":["profile = ProfileReport(df, title='Reporte') # Cargamos el reporte\n","if show_profile:\n","  profile.to_notebook_iframe() # Lo mostramos en pantalla (tiempo aproximado 5 minutos)"]},{"cell_type":"markdown","metadata":{"id":"UoErcZNgRvj-"},"source":["###### 2.1.5.1.1. Atributos y significados"]},{"cell_type":"markdown","metadata":{"id":"lMDCF6NQR40R"},"source":["> En este paso, se describen los atributos según el conocimiento del problema."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":412},"executionInfo":{"elapsed":36,"status":"ok","timestamp":1718157004678,"user":{"displayName":"Bruno Masoller","userId":"01529176355565509322"},"user_tz":180},"id":"7jKbciIPSwN0","outputId":"a0b51199-7ab7-4db0-e273-788d7395c936"},"outputs":[],"source":["# Mostramos los datos.\n","df.sample(10, random_state=random_state)"]},{"cell_type":"markdown","metadata":{"id":"3qbbhCNvV2Og"},"source":["<p><em>\n","Podemos observar que el dataset contiene 23 columnas, o sea 23 atributos. También algunos otros datos como que de por sí el conjunto tiene <code>NaN</code> como datos, lo que indica que probablemente sea más fácil cambiar estos datos a los análogos en <code>numpy</code>.\n","</em></p>"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":33,"status":"ok","timestamp":1718157004679,"user":{"displayName":"Bruno Masoller","userId":"01529176355565509322"},"user_tz":180},"id":"TmKeHMapXdZz","outputId":"51600cf1-6024-4f37-f368-b8d31d7d901f"},"outputs":[],"source":["print('Atributos:')\n","df.columns"]},{"cell_type":"markdown","metadata":{"id":"wamqnh6FXntO"},"source":["<p><em>\n","Según el conocimiento del negocio, tenemos la descripción de los siguientes atributos:\n","\n","- `Date` → Tipo: Fecha (formato: `YYYY-MM-DD`) | Fecha de cuando se tomó la observación.\n","- `Location` → Tipo: Cualitativa-Nominal | Ubicación de la estación meterológica\n","- `MinTemp` → Tipo: Cuantitativa-Continua | Temperatura mínima (mínimo histórico de -23°)\n","- `MaxTemp` → Tipo: Cuantitativa-Continua | Temperatura máxima (máximo histórico de 51°)\n","- `Rainfall` → Tipo: Cuantitativa-Continua | Cuanta lluvia calló (máximo histórico en 375mm)\n","- `Evaporation` → Tipo: Cuantitativa-Continua | Cuanto fue la evaporación\n","- `Sunshine` → Tipo: Cuantitativa-Continua | Número de horas solares del día\n","- `WindGustDir` → Tipo: Cualitativa-Nominal | Dirección del viento más fuerte\n","- `WindGustSpeed` → Tipo: Cuantitativa-Continua | Velocidad del viento más fuerte (máximo registrado 408 km/h, en un cliclón, no se toman en cuenta tornados que vuela todo)\n","- `WindDir9am` → Tipo: Cualitativa-Nominal | Datos específicos según hora del día\n","- `WindDir3pm` → Tipo: Cualitativa-Nominal | Datos específicos según hora del día\n","- `WindSpeed9am` → Tipo: Cuantitativa-Continua | Datos específicos según hora del día\n","- `WindSpeed3pm` → Tipo: Cuantitativa-Continua | Datos específicos según hora del día\n","- `Humidity9am` → Tipo: Cuantitativa-Continua | Datos específicos según hora del día\n","- `Humidity3pm` → Tipo: Cuantitativa-Continua | Datos específicos según hora del día\n","- `Pressure9am` → Tipo: Cuantitativa-Continua | Datos específicos según hora del día\n","- `Pressure3pm` → Tipo: Cuantitativa-Continua | Datos específicos según hora del día\n","- `Cloud9am` → Tipo: Cuantitativa-Continua | Datos específicos según hora del día\n","- `Cloud3pm` → Tipo: Cuantitativa-Continua | Datos específicos según hora del día\n","- `Temp9am` → Tipo: Cuantitativa-Continua | Datos específicos según hora del día\n","- `Temp3pm` → Tipo: Cuantitativa-Continua | Datos específicos según hora del día\n","- `RainToday` → Tipo: Booleano | Indica si llovío en el día\n","- `RainTomorrow` → Tipo: Booleano | Indicador de riesgo si lloverá mañana o no. Es la variable objetivo.\n","\n","De este análisis preliminar, podemos deducir las siguientes suposiciones:\n","- Podría haber una correlación entre la cantidad de lluvia y si llovió hoy. O sea, entre `Rainfall` y `RainToday`.\n","</em></p>"]},{"cell_type":"markdown","metadata":{"id":"B5mDchVFb5WF"},"source":["###### 2.1.5.1.2. Datos duplicados"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":24,"status":"ok","timestamp":1718157004680,"user":{"displayName":"Bruno Masoller","userId":"01529176355565509322"},"user_tz":180},"id":"Av6tHqAQcczK","outputId":"316554ee-f448-47fe-dad7-e78079a1dd93"},"outputs":[],"source":["# Información básica de los datos\n","df.info()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":359,"status":"ok","timestamp":1718157005024,"user":{"displayName":"Bruno Masoller","userId":"01529176355565509322"},"user_tz":180},"id":"TND-19L0dBkt","outputId":"69f1598c-5dcb-4b77-cb46-45146aaf0b31"},"outputs":[],"source":["# Chequeamos si hay datos duplicados en las columnas Date y Location\n","df = df.sort_values(by=[\"Location\", \"Date\"]).reset_index(drop=True)\n","\n","is_any_duplicated = df.duplicated(subset=[\"Date\", \"Location\"], keep=False).any()\n","print(\"Observaciones duplicadas para una fecha o localidad: \", is_any_duplicated)"]},{"cell_type":"markdown","metadata":{"id":"5QOAmHFydI-p"},"source":["<p><em>\n","Encontramos que la mayoria de las columnas corresponden a variables cuantitativas. Podemos observar que cuatro de ellas poseen una gran proporción de valores nulos: <code>Evaporation</code>, <code>Sunshine</code>, <code>Cloud9am</code>, <code>Cloud3pm</code>. El resto de las variables numéricas poseen cerca del 10% de valores faltantes o menos.\n","\n","Entre las variables de tipo cualitativas identificamos a <code>Location</code> y <code>Date</code> como columnas identificatorias de una observación, es decir, no hay filas con valores repetidos si tomamos el subset de esas dos columnas. Tampoco se observan valores nulos en niguna de ambas columnas. La importancia de estas variables no reside sólo en su carácter identificatorio, sino además proveen información, espacial y temporal, que vamos a procesar más adelante y que puede resultar útil para nuestro modelo.\n","\n","Observamos además que, entre las variables de tipo cualitativa tenemos la variable target <code>RainTomorrow</code>, binaria por definición del problema a resolver, y la variable <code>RainToday</code> que, intuitivamente, sigue una misma codificación que la variable target mencionada. Es importante notar que ambas variables binarias cuentan con valores nulos en algunas observaciones.\n","\n","Finalmente, encontramos tres variables cualitativas relacionadas con la dirección del viento en diferentes momentos del día. Podemos considerar a estas variables como categóricas ordinales en el sistema de coordenadas polares, no existiendo relación de menor-mayor entre ellas pero si una relación secuencial en dos dimensiones.\n","</em></p>"]},{"cell_type":"markdown","metadata":{"id":"5cFrW5tm1nU_"},"source":["###### 2.1.5.1.3. Tipos de datos"]},{"cell_type":"markdown","metadata":{"id":"cV4W3Rzu16C8"},"source":["> En este punto se investiga los tipos de datos y se asigna el tipo correcto según el caso."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":26,"status":"ok","timestamp":1718157005025,"user":{"displayName":"Bruno Masoller","userId":"01529176355565509322"},"user_tz":180},"id":"NUUoweVkAKqK","outputId":"3e487ed0-87bd-4091-f6db-cd47b78ee54f"},"outputs":[],"source":["# Visualizar los tipos de datos\n","df.dtypes"]},{"cell_type":"markdown","metadata":{"id":"m7bNkDhU11H_"},"source":["<p><em>\n","Según lo analizado anteriormente, tenemos la siguiente clasificación de tipos de datos:\n","</em></p>\n"]},{"cell_type":"markdown","metadata":{"id":"ZLf857ed3Nnz"},"source":["| **Variable**      | **Tipo actual** | **Tipo correcto** |\n","|-------------------|-----------------|-------------------|\n","| **Date**          | object          | datetime64        |\n","| **Location**      | object          | category          |\n","| **MinTemp**       | float64         | ✔                 |\n","| **MaxTemp**       | float64         | ✔                 |\n","| **Rainfall**      | float64         | ✔                 |\n","| **Evaporation**   | float64         | ✔                 |\n","| **Sunshine**      | float64         | ✔                 |\n","| **WindGustDir**   | object          | category          |\n","| **WindGustSpeed** | float64         | ✔                 |\n","| **WindDir9am**    | object          | category          |\n","| **WindDir3pm**    | object          | category          |\n","| **WindSpeed9am**  | float64         | ✔                 |\n","| **WindSpeed3pm**  | float64         | ✔                 |\n","| **Humidity9am**   | float64         | ✔                 |\n","| **Humidity3pm**   | float64         | ✔                 |\n","| **Pressure9am**   | float64         | ✔                 |\n","| **Pressure3pm**   | float64         | ✔                 |\n","| **Cloud9am**      | float64         | ✔                 |\n","| **Cloud3pm**      | float64         | ✔                 |\n","| **Temp9am**       | float64         | ✔                 |\n","| **Temp3pm**       | float64         | ✔                 |\n","| **RainToday**     | object          | bool              |\n","| **RainTomorrow**  | object          | bool              |"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":513,"status":"ok","timestamp":1718157005524,"user":{"displayName":"Bruno Masoller","userId":"01529176355565509322"},"user_tz":180},"id":"UPRXkxjd4559"},"outputs":[],"source":["# Se realizan las asignaciones de datos correspondientes\n","cat_columns = ['Location', 'WindGustDir', 'WindDir9am', 'WindDir3pm']\n","bool_columns = ['RainToday', 'RainTomorrow']\n","date_columns = ['Date']\n","\n","df[cat_columns] = df[cat_columns].astype('category')\n","df[date_columns] = df[date_columns].astype('datetime64[ns]')\n","mapping_dict = {\"Yes\" : 1, \"No\" : 0}\n","df[bool_columns] = df[bool_columns].applymap(lambda x: mapping_dict.get(x, x))"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":27,"status":"ok","timestamp":1718157005524,"user":{"displayName":"Bruno Masoller","userId":"01529176355565509322"},"user_tz":180},"id":"wCgbaCKQAKqL","outputId":"05885770-6ad8-40ca-e8e0-5775c67d1c76"},"outputs":[],"source":["# Comprobamos que los tipos hayan quedado correctamente\n","df.dtypes"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":23,"status":"ok","timestamp":1718157005525,"user":{"displayName":"Bruno Masoller","userId":"01529176355565509322"},"user_tz":180},"id":"EgiqXsNuCpcp"},"outputs":[],"source":["# Finalmente agrego los tipos en estructuras separadas para facilitar el tratamiento\n","cat_columns += bool_columns\n","cont_columns = ['MinTemp','MaxTemp','Rainfall','Evaporation','Sunshine',\n","                   'WindGustSpeed','WindSpeed9am','WindSpeed3pm','Humidity9am',\n","                   'Humidity3pm','Pressure9am','Pressure3pm','Cloud9am',\n","                   'Cloud3pm','Temp9am','Temp3pm']"]},{"cell_type":"markdown","metadata":{"id":"oTwoWTbuWUIT"},"source":["Chequeamos que los valores faltantes tengan el tipo adecuado (también se arreglan valores que tienen incorrecto -basado en intución y conocimiento del negocio)."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":22,"status":"ok","timestamp":1718157005526,"user":{"displayName":"Bruno Masoller","userId":"01529176355565509322"},"user_tz":180},"id":"CEwLVi8FWmez","outputId":"e0f08028-b52d-482d-e086-5b2546ec7765"},"outputs":[],"source":["# Para columnas categóricas, simplemente nos fijamos en los valores que toma\n","# Crear un diccionario para almacenar los valores únicos de cada columna categórica\n","unique_values = {col: df[col].unique() for col in cat_columns}\n","print_unique_values(unique_values)"]},{"cell_type":"markdown","metadata":{"id":"Bqi_vTNpcpC-"},"source":["<p><em>\n","Podemos observar que los valores tienen adecuandamente el tipo <code>np.nan</code>"]},{"cell_type":"markdown","metadata":{"id":"fhtppuPbcyu5"},"source":["Para las columnas continuas, simplemente corroboramos que todas tegan el mismo tipo de datos."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":551},"executionInfo":{"elapsed":2809,"status":"ok","timestamp":1718157008323,"user":{"displayName":"Bruno Masoller","userId":"01529176355565509322"},"user_tz":180},"id":"SYUP1hpI7ZT1","outputId":"45b60917-1415-4d00-d4a0-0a9aee28d121"},"outputs":[],"source":["# Llamar a la función y guardar la información en un DataFrame\n","type_info_df = show_unique_types_as_df(df, cont_columns)\n","type_info_df = type_info_df.groupby('Column').agg({'Type': concatenate_values}).reset_index()\n","type_info_df"]},{"cell_type":"markdown","metadata":{"id":"_q-wT2NWehjY"},"source":["<p><em>\n","Podemos corroborar que únicamente se encuentran tipos <code>float</code>. Nota: Los valores <code>NaN</code> en pandas son del tipo <code>float</code> y se representan como <code>numpy.float64</code>. Para diferenciarlos específicamente como <code>NaN</code>, podemos usar <code>numpy.isnan</code> para identificar estos valores y considerarlos por separado. Esto lo hacemos en la función y por esto se muestra este tipo a parte.\n","</em></p>"]},{"cell_type":"markdown","metadata":{"id":"lgM77Fow_uMC"},"source":["###### 2.1.5.1.4. Momentos"]},{"cell_type":"markdown","metadata":{"id":"3aQr482G__BV"},"source":["> En esta sección se analizan los momentos y datos estadísticos de las variables."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":407},"executionInfo":{"elapsed":520,"status":"ok","timestamp":1718157008818,"user":{"displayName":"Bruno Masoller","userId":"01529176355565509322"},"user_tz":180},"id":"X-oX8DruAE9c","outputId":"8747e586-8145-4f27-f04f-1a6a0770e4d4"},"outputs":[],"source":["# Datos estadísticos del conjunto\n","df.describe()"]},{"cell_type":"markdown","metadata":{"id":"ipdDzcvwETIq"},"source":["<p><em>\n","Dados los datos estadísticos de las variables continuas, podemos ver que los máximos y mínimos de las variables de temperatura son acordes con los datos históricos, así como la velocidad del viento. Otro punto a observar son que los máximos y mínimos de las presiones también están acordes con los datos de la presiona atmosférica promedio, medida en hecto-pascales.\n","\n","Otro punto a destacar es que los máximos y mínimos no se alejan tanto de la media con excepción de algunos atributos como `Rainfall` que el máximo se aleja muchísimo más de la media (mucho más de tres desviaciones), siendo posibles casos de análisis de valores atípicos.\n","</em></p>\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":81},"executionInfo":{"elapsed":40,"status":"ok","timestamp":1718157008820,"user":{"displayName":"Bruno Masoller","userId":"01529176355565509322"},"user_tz":180},"id":"ym4x8uW4BNtD","outputId":"f0c9407d-e29d-43c6-84b1-55fe02d6655e"},"outputs":[],"source":["# Valor más frecuente para variables categóricas\n","df[cat_columns].mode()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":507},"executionInfo":{"elapsed":2821,"status":"ok","timestamp":1718157011610,"user":{"displayName":"Bruno Masoller","userId":"01529176355565509322"},"user_tz":180},"id":"fJm5jjcZPKym","outputId":"545b2b62-31e6-4794-af71-6919946954d4"},"outputs":[],"source":["# Creamos una grilla de gráficos de barras\n","plot_graph_on_grid(df, columns=cat_columns, num_cols=3, graph_type='bar')"]},{"cell_type":"markdown","metadata":{"id":"sUmKwXGdIwzr"},"source":["<p><em>\n","Podemos observar que hay un gran desbalance de clases en la variable <code>RainToday</code> y en la variable objetivo <code>RainTomorrow</code>, en donde el porcentaje es el siguiente:\n","</em></p>"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":420},"executionInfo":{"elapsed":729,"status":"ok","timestamp":1718157012322,"user":{"displayName":"Bruno Masoller","userId":"01529176355565509322"},"user_tz":180},"id":"dAVyYkjXJplT","outputId":"41ede7c4-4710-4a75-d5fd-5c3af171dfe9"},"outputs":[],"source":["# Creamos una grilla de graficos pie\n","plot_graph_on_grid(df, columns=['RainToday', 'RainTomorrow'], num_cols=2, graph_type='pie', figsize=(8, 4))"]},{"cell_type":"markdown","metadata":{"id":"nzu2ChruQT-S"},"source":["<p><em>\n","Este punto lo tenemos que tratar en otra sección: <strong>Clases desbalanceadas</strong>\n","</p></em>"]},{"cell_type":"markdown","metadata":{"id":"SyTJ_c6fRNIc"},"source":["Analizamos la oblicuidad utilizando el estimador por defecto:"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":59,"status":"ok","timestamp":1718157012323,"user":{"displayName":"Bruno Masoller","userId":"01529176355565509322"},"user_tz":180},"id":"7rAZDouoRh6y","outputId":"30a103b4-830d-4c4d-cd6a-203ad28aa4f1"},"outputs":[],"source":["# Calculamos la oblicuidad de cada columna.\n","skewness = df.skew(numeric_only=True)\n","skewness"]},{"cell_type":"markdown","metadata":{"id":"_ak-ch6oRueq"},"source":["<p><em>\n","Para mayor facilidad, analizamos los valores que son están alejados 0.5 del 0:\n","</p></em>"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":39,"status":"ok","timestamp":1718157012324,"user":{"displayName":"Bruno Masoller","userId":"01529176355565509322"},"user_tz":180},"id":"4Ve-TPOmSTR7","outputId":"377d15c6-3c4a-4c74-f113-29cf45dffa08"},"outputs":[],"source":["skewed_colums = skewness[abs(skewness) > 0.5]\n","skewed_colums"]},{"cell_type":"markdown","metadata":{"id":"a12Sy8vZTALd"},"source":["<p><em>\n","Podemos observar que las columnas con mas oblicuidad son <code>Rainfall</code> y <code>Evaporation</code>. Ambos sesgados a la derecha (cola pesada hacia la derecha).\n","</em></p>"]},{"cell_type":"markdown","metadata":{"id":"7_1PuA7LT5Co"},"source":["Analizamos la curtosis para las columnas continuas (utilizando el estimador por defecto):"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":30,"status":"ok","timestamp":1718157012324,"user":{"displayName":"Bruno Masoller","userId":"01529176355565509322"},"user_tz":180},"id":"MC6z8gMfUD1v","outputId":"6e11d5a1-ab6b-4d5a-fa2f-f82336f1cf89"},"outputs":[],"source":["kurtosis = df.kurtosis(numeric_only=True)\n","kurtosis"]},{"cell_type":"markdown","metadata":{"id":"C3fL-zv-VnjS"},"source":["<p><em>\n","Realizamos el mismo proceso que en el paso anterior y analizamos aquellas variables que tienen una crutosis que se aleja más de 5 del 0:\n","</em></p>"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":19,"status":"ok","timestamp":1718157012324,"user":{"displayName":"Bruno Masoller","userId":"01529176355565509322"},"user_tz":180},"id":"O8sPjiqgYxsJ","outputId":"25c1419f-7a9e-49ab-c91a-6ea51aa04123"},"outputs":[],"source":["kurtosis_colums = kurtosis[abs(kurtosis) > 5]\n","kurtosis_colums"]},{"cell_type":"markdown","metadata":{"id":"3a7sRnhMY5ji"},"source":["<p><em>\n","Podemos ver que los atributos <code>Rainfall</code> y <code>Evaporation</code> son los que tienen mayor distancia de 0. Ambas leptocúrticas.\n","\n","Como también están sesgadas, hay una gran posibilidad de que se tenga que hacer un tratamiento diferentes de los datos.\n","</em></p>"]},{"cell_type":"markdown","metadata":{"id":"YQ0NTnGSZbIL"},"source":["Verificamos los histogramas de las variables para validar las métricas anteriores:"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"elapsed":39363,"status":"ok","timestamp":1718157051674,"user":{"displayName":"Bruno Masoller","userId":"01529176355565509322"},"user_tz":180},"id":"uOTf7qNWV1SD","outputId":"6d15c4b1-ec08-496c-9fdf-f86492ad960a"},"outputs":[],"source":["plot_graph_on_grid(df, columns=cont_columns, num_cols=3, graph_type='hist', figsize=(12, 17))"]},{"cell_type":"markdown","metadata":{"id":"O6IFdFiNnVSX"},"source":["<p><em>\n","Luego de analizado los histogrmas para verificar las métricas anteriores, notamos algo importante. Las columnas <code>Clould9am</code> y <code>Cloud3pm</code> tienen una baja cardinalidad. Es más, los únicos valores son:\n","</em></p>"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":24,"status":"ok","timestamp":1718157051676,"user":{"displayName":"Bruno Masoller","userId":"01529176355565509322"},"user_tz":180},"id":"qaz_l6YqpPtc","outputId":"b5ceeeed-1f16-4923-c9d6-dba357cadb78"},"outputs":[],"source":["unique_values = {col: df[col].unique() for col in ['Cloud9am', 'Cloud3pm']}\n","print_unique_values(unique_values)"]},{"cell_type":"markdown","metadata":{"id":"c_7jC2j9qf2v"},"source":["<p><em>\n","Por lo que al pricipio fueron definidas como \"Cuantitativa-Continua\" sería más bien una variable \"Cuantitativa-Discreta\":\n","\n","- `Cloud9am` → Tipo: Cuantitativa-Discreta | Datos específicos según hora del día\n","- `Cloud3pm` → Tipo: Cuantitativa-Discreta | Datos específicos según hora del día\n","\n","</em></p>"]},{"cell_type":"markdown","metadata":{"id":"VVJcwRX8JRej"},"source":["Otro punto para verificar la normalidad de los datos, es hacer gráficos de QQ-plot:"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"elapsed":7247,"status":"ok","timestamp":1718157058911,"user":{"displayName":"Bruno Masoller","userId":"01529176355565509322"},"user_tz":180},"id":"t10iMDroKPaT","outputId":"1981d75e-0d23-421d-dfc6-c28ee365415f"},"outputs":[],"source":["plot_graph_on_grid(df, columns=cont_columns, num_cols=3, graph_type='qq-plot', figsize=(12, 17))"]},{"cell_type":"markdown","metadata":{"id":"VPvcL_HxLnX7"},"source":["<p><em>\n","Del gráfico podemos verificar las medidas anteriores obtenidas (la no normalidad de las columnas antes analizadas).\n","</em></p>"]},{"cell_type":"markdown","metadata":{"id":"DqaSSongeJQq"},"source":["###### 2.1.5.1.5. Valores faltantes"]},{"cell_type":"markdown","metadata":{"id":"L_rdbeiVDeu4"},"source":["Analizamos los valores faltantes de todas las columnas:"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":771},"executionInfo":{"elapsed":49,"status":"ok","timestamp":1718157058913,"user":{"displayName":"Bruno Masoller","userId":"01529176355565509322"},"user_tz":180},"id":"VzUpmT5xD3aL","outputId":"6b58baec-eea7-4b6f-88d6-97d61c051856"},"outputs":[],"source":["missing_values_df = df.isna().sum().reset_index()\n","missing_values_df.columns = ['Columna', 'Valores faltantes']\n","\n","# Calcular el porcentaje de valores faltantes por columna\n","missing_percentage = (df.isna().sum() / len(df))\n","missing_values_df['Proporción de faltantes'] = missing_percentage.values.round(2)\n","\n","# Ordenar de forma descendente según el porcentaje\n","missing_values_df = missing_values_df.sort_values(by='Proporción de faltantes',\n","                                                  ascending=False)\n","\n","missing_values_df"]},{"cell_type":"markdown","metadata":{"id":"Eeh_WeEkIrAj"},"source":["<p><em>\n","Como podemos observar, hay muchos datos faltantes que deben ser analizados en las siguientes secciones.\n","</em></p>"]},{"cell_type":"markdown","metadata":{"id":"AnKfb3_uhQym"},"source":["Analizamos los valores faltantes de las columnas <code>RainToday</code> y <code>RainTomorrow</code>."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":40,"status":"ok","timestamp":1718157058914,"user":{"displayName":"Bruno Masoller","userId":"01529176355565509322"},"user_tz":180},"id":"HALC91SZhgof","outputId":"234cc2ff-0416-47f0-ff52-5d0680996bd7"},"outputs":[],"source":["columns = [\"RainToday\", \"RainTomorrow\"]\n","for column in columns:\n","    print(df[column].value_counts())\n","    print_missing_perc(df, column)\n","    print()"]},{"cell_type":"markdown","metadata":{"id":"UQuTbziPi6UB"},"source":["<p><em>\n","Se observa que la distribución de los valores que asumen las columnas son muy similares, lo cual tiene sentido debido a que el dataset elegido es una serie temporal y el valor de <code>RainTomorrow</code> en una observación es el valor de <code>RainToday</code> para el día siguiente.\n","\n","Encontramos además la misma proporción de valores nulos en ambas columnas.\n","</em></p>"]},{"cell_type":"markdown","metadata":{"id":"ynRlFmDircvT"},"source":["Analizamos las columnas <code>Date</code> y <code>Location</code>."]},{"cell_type":"markdown","metadata":{"id":"i_GSeKSQrp6-"},"source":["Para evaluar la coherencia interna del dataset verificamos que no haya observaciones con valor nulo en <code>RainTomorrow</code>, teniendo la observación del día siguiente con valor existente en la columna <code>RainToday</code>, para una misma estación meteorológica."]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":27,"status":"ok","timestamp":1718157058915,"user":{"displayName":"Bruno Masoller","userId":"01529176355565509322"},"user_tz":180},"id":"CbeHfnj2jUbG"},"outputs":[],"source":["diff_one_day = df[\"Date\"].shift(-1) - df[\"Date\"] == pd.Timedelta(\"1 day\")\n","same_location = df[\"Location\"] == df[\"Location\"].shift(-1)\n","na_today_value_tomorrow = df[\"RainTomorrow\"].isna() & ~(df[\"RainToday\"].shift(-1).isna())\n","\n","filt = diff_one_day & same_location & na_today_value_tomorrow\n","assert df.loc[filt, :].shape[0] == 0"]},{"cell_type":"markdown","metadata":{"id":"7q3FAglIjUme"},"source":["Graficamos las observaciones diarias en función de la estación meteorológica para ver con que registros contamos."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":695},"executionInfo":{"elapsed":7391,"status":"ok","timestamp":1718157066281,"user":{"displayName":"Bruno Masoller","userId":"01529176355565509322"},"user_tz":180},"id":"3IJVRIkdsL21","outputId":"18e41e2c-b4bf-4bdf-bfa7-e03efca1a510"},"outputs":[],"source":["plot_locations_over_time(df, color=spectral_palette[7])"]},{"cell_type":"markdown","metadata":{"id":"iA5XJhvztZ6A"},"source":["<p><em>\n","Se observa que tenemos registros faltantes correspondientes a meses enteros para todas las estaciones meteorológicas. Además podemos observar que el inicio de la serie temporal para cada una de ellas difiere, siendo la observación más antigua de noviembre 2007.\n","\n","Encontramos que en algunas de las localidades contamos con mayor cantidad de valores nulos en la columna target <code>RainTomorrow</code>. A priori esperábamos encontrar que los valores nulos para esa variable se encontraran dónde la serie temporal se corta, sin embargo podemos ver que aparecen aleatoriamente a lo largo de la serie temporal, y que en algunas locaciones incluso tenemos datos válidos en la variable <code>RainTomorrow</code> a pesar de no contar con una observación para el día siguiente al registrado.\n","\n","Las estaciones con mayor cantidad de datos faltantes en la variable target son Melbourne y Wiliamtown. En lo que a Melbourne concierne, podemos observar que hay registros también en el aeropuerto de esa ciudad, por lo que podemos inferir que los patrones cimáticos de una de esas estaciones puede brindar información de utilidad para predecir si llueve al día siguiente en la otra. Extendiendo a todas las estaciones meteorológicas entendemos que incorporar la posición geográfica puede resultar de gran utilidad para el desarrollo de nuestro modelo predictivo.\n","</em></p>"]},{"cell_type":"markdown","metadata":{"id":"z31jeZ_-ePav"},"source":["###### 2.1.5.1.6. Valores atípicos"]},{"cell_type":"markdown","metadata":{"id":"Pjvijx7k5XMt"},"source":["Para la detección de valores atípicos, primeramente realizamos diagramas de cajas para visualizar los datos. Estos diagramas de cajas también nos permiten las características anteriormente medidas, como la oblicuidad, media, mediana, etc; y así también validar estas métricas antes obtenidas."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"elapsed":7827,"status":"ok","timestamp":1718157074081,"user":{"displayName":"Bruno Masoller","userId":"01529176355565509322"},"user_tz":180},"id":"BkWV5inyCHt1","outputId":"8f83f02f-a7b2-4cc9-bb24-4c9d1cfec051"},"outputs":[],"source":["plot_graph_on_grid(df, columns=cont_columns, num_cols=3, graph_type='box-plot', figsize=(12, 17))"]},{"cell_type":"markdown","metadata":{"id":"aWXSb7MHeRsi"},"source":["<p><em>\n","Podemos observar que las siguentes columnas tienen outlíers:\n","</em></p>"]},{"cell_type":"markdown","metadata":{"id":"wz0syVeOmzgB"},"source":["- <code>MinTemp</code>\n","- <code>MaxTemp</code>\n","- <code>RainFall</code>\n","- <code>Evaporation</code>\n","- <code>WindGustSpeed</code>\n","- <code>WindSpeed9am</code>\n","- <code>WindSpeed3pm</code>\n","- <code>Humidity9am</code>\n","- <code>Pressure9am</code>\n","- <code>Pressure3pm</code>\n","- <code>Temp9am</code>\n","- <code>Temp3pm</code>"]},{"cell_type":"markdown","metadata":{"id":"1_qy1VVkm6Br"},"source":["Porcentaje de outliers según IRQ y desviacón estándar:"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":551},"executionInfo":{"elapsed":693,"status":"ok","timestamp":1718157074744,"user":{"displayName":"Bruno Masoller","userId":"01529176355565509322"},"user_tz":180},"id":"3sBgTQsnt__e","outputId":"263287b3-2fd0-4f9a-f63f-fc87f7dd8ab2"},"outputs":[],"source":["outliers = outliers_iqr(df, cont_columns).sort_values(by='IRQ-Percentage', ascending=False)\n","outliers"]},{"cell_type":"markdown","metadata":{"id":"cACRoTEW5dsb"},"source":["<p><em>\n","Con la tabla podemos observar que aquellas columnas que se alejan de la distribución normal, como por ejemplo <code>Rainfall</code>, lo correcto sería tratar muchos más outliers mediante el rango inter cuartílico (que utiliza la mediana) que mediante la desviación estandar (que utiliza la media).\n","</em></p>"]},{"cell_type":"markdown","metadata":{"id":"CL2po6yoeUME"},"source":["###### 2.1.5.1.7. Correlación entre datos"]},{"cell_type":"markdown","metadata":{"id":"eDe_STj1tGZb"},"source":["Análisis de correlación linear de los datos:"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":634},"executionInfo":{"elapsed":42,"status":"ok","timestamp":1718157074745,"user":{"displayName":"Bruno Masoller","userId":"01529176355565509322"},"user_tz":180},"id":"_VWeWu5eAKqY","outputId":"c2e9d18d-fcfa-48ec-bc7c-00254271ddd6"},"outputs":[],"source":["# Correlación de los datos\n","correlation_matrix = df.corr(numeric_only=True, method='pearson')\n","correlation_matrix"]},{"cell_type":"markdown","metadata":{"id":"h8D4t4jEAKqY"},"source":["Para mejor visualización realizamos un mapa de calor:"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":774},"executionInfo":{"elapsed":1724,"status":"ok","timestamp":1718157076437,"user":{"displayName":"Bruno Masoller","userId":"01529176355565509322"},"user_tz":180},"id":"jleUONxEuQ94","outputId":"c050beb6-f1c6-409d-9368-b82523334a26"},"outputs":[],"source":["plot_heatmap(correlation_matrix)"]},{"cell_type":"markdown","metadata":{"id":"y5VyjUfmusHL"},"source":["<p><em>\n","Podemos apreciar que hay datos que están bastante corelacionados. Para mejor observación, graficamos las columnas cuya correlación es mayor a 0.75 (establecidos como correlación de intensidad fuerte):\n","</em></p>"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":527},"executionInfo":{"elapsed":773,"status":"ok","timestamp":1718157077189,"user":{"displayName":"Bruno Masoller","userId":"01529176355565509322"},"user_tz":180},"id":"qxz4w-9cviQI","outputId":"e8d8d1de-07a8-4cee-b11d-c5b3dab05b7c"},"outputs":[],"source":["# Eliminar la diagonal principal (auto-correlaciones)\n","mask = np.eye(len(correlation_matrix), dtype=bool)\n","correlation_matrix_no_diag = correlation_matrix.where(~mask)\n","\n","# Encontrar columnas altamente correlacionadas\n","high_corr_columns = correlation_matrix_no_diag.columns[correlation_matrix_no_diag.abs().max() > 0.75]\n","high_corr_matrix = correlation_matrix.loc[high_corr_columns, high_corr_columns]\n","\n","plot_heatmap(high_corr_matrix, figsize=(5, 5))"]},{"cell_type":"markdown","metadata":{"id":"tUUUfkOA0XLS"},"source":["<p><em>\n","En este punto podemos ver las siguientes correlaciones fuertes:\n","\n","<code>MinTemp</code> ⟷ <code>Temp9am</code>\n","\n","<code>MaxTemp</code> ⟷ <code>Temp3am</code> | <code>Temp9am</code>\n","\n","<code>Pressure9am</code> ⟷ <code>Pressure3pm</code>\n","\n","<code>Temp3am</code> ⟷ <code>Temp9am</code>\n","\n","La relación en todos los casos es directa.\n","\n","Estas correlaciones hay que tenerlas en cuenta a la hora de entrenamor modelos que son suceptibles a este tipo de correlación (ej: modelos lineales). También hay que tener en cuenta estas correlaciones en el caso de necesitar imputación de datos.\n","</em></p>"]},{"cell_type":"markdown","metadata":{"id":"n3TD3F2C4Vv_"},"source":["Otra forma de ver la correlación y afirmar aún más nuestras hipotesis, es graficando par a par (pairplot):"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fSqyj6qh4gGp"},"outputs":[],"source":["# sns.pairplot(df)\n","columnas = list(set(cont_columns) - set(['Cloud9am', 'Cloud3pm']) | set([\"RainTomorrow\"]))\n","sns.pairplot(\n","    df[columnas].sample(10000, random_state=random_state),\n","    hue=\"RainTomorrow\",\n","    palette=[spectral_palette[9], spectral_palette[1]],\n","    diag_kind=\"kde\",\n","    plot_kws={'alpha': 0.2}\n","    )"]},{"cell_type":"markdown","metadata":{"id":"IRASf4IXo19p"},"source":["_En este ploteo podemos observar la correlación lineal entre ciertas variables analizadas anteriormente. Además, encontramos que las variables Humidity3pm y Sunshine resultan sumamente promimsorias para la separación de clases en la variable objetivo. Sin embargo, recordamos que la variable Sunshine posee datos faltantes en casi la mitad de los registros. Realizar un buen trabajo de imputación de datos faltantes resulta entonces indispensable para obtener una buena performance en nuestro tarea predictiva._"]},{"cell_type":"markdown","metadata":{"id":"-g2W-4j9ub5L"},"source":["Análisis de la correlación según <code>Date</code> y <code>Location</code>."]},{"cell_type":"markdown","metadata":{"id":"oBUPmIz1uu_P"},"source":["Geolocalizamos las estaciones meteorológicas utilizando el servicio Open Street Map para obtener la posición geográfica de cada una de ellas."]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"id":"zHgI9_pBuxg7"},"outputs":[],"source":["country = \"Australia\"\n","\n","world = gpd.read_file(get_path('naturalearth_lowres'))\n","gdf_australia = world[world.name == country]\n","\n","# Solve manually some mistaken names\n","mapping_dict = {\"Dartmoor\": \"DartmoorVillage\", \"Richmond\": \"RichmondSydney\"}\n","df[\"Location\"] = df[\"Location\"].map(mapping_dict).fillna(df[\"Location\"])\n","\n","locations = df[\"Location\"].unique()\n","locations = [re.sub(r'([a-z])([A-Z])', r'\\1 \\2', l) for l in locations]\n","\n","locs = []\n","lats = []\n","lons = []\n","for location in locations:\n","  try:\n","    lat, lon = ox.geocode(location + f\", {country}\")\n","\n","    locs.append(location.replace(\" \", \"\"))\n","    lats.append(lat)\n","    lons.append(lon)\n","  except Exception as e:\n","    print(f\"Error retrieving coordinates for {location}: {e}\")\n","\n","df_locations = pd.DataFrame({\n","    'Location': locs,\n","    'Lat': lats,\n","    'Lon': lons\n","})\n","geometry = [Point(lon, lat) for lon, lat in zip(df_locations['Lon'], df_locations['Lat'])]\n","gdf_locations = gpd.GeoDataFrame(df_locations, geometry=geometry, crs=\"EPSG:4326\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zb-NlmuFvvLQ"},"outputs":[],"source":["fig, ax = plt.subplots(figsize=(10, 6))\n","gdf_australia.plot(ax=ax, edgecolor='k', facecolor=spectral_palette[4])\n","\n","# Plot locations\n","gdf_locations.plot(ax=ax, marker='o', color='black', markersize=10, label='Locations')\n","\n","for idx, row in gdf_locations.iterrows():\n","    ax.text(\n","        row['geometry'].x,\n","        row['geometry'].y + .2,\n","        row['Location'],\n","        fontsize=5,\n","        ha='center',\n","        va='bottom'\n","        )\n","\n","ax.set_xticks([])\n","ax.set_yticks([])\n","\n","ax.set_aspect('equal', adjustable='box')\n","\n","plt.title(\n","        \"Ubicación de las estaciones meteorológicas observadas\",\n","        fontsize=10,\n","    )\n","\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"8FKeflvkv3V8"},"source":["<p><em>\n","Podemos ver una gran cantidad de estaciones meteorológicas concentradas en la costa sureste de Australia, y en menor medida en la costa suroeste. Además notamos que algunas de las estaciones se encuentran muy cercanas entre si, lo cual puede traer aparejado una correlación entre los patrones climáticos.\n","</em></p>"]},{"cell_type":"markdown","metadata":{"id":"RbhsUb2pwajh"},"source":["Analizamos el coeficiente Phi para la variable <code>RainToday</code> comparando entre pares de estaciones meteorológicas y cruzando con la información de la distancia euclideana entre ellas."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EeJbV6cjwwac"},"outputs":[],"source":["locations = df[\"Location\"].unique()\n","location_pairs = list(combinations(locations, 2))\n","\n","df_location_pairs = pd.DataFrame(location_pairs, columns=['LocationA', 'LocationB'])\n","df_location_pairs['phi'] = np.nan\n","df_location_pairs['pvalue'] = np.nan\n","\n","for index, row in df_location_pairs.iterrows():\n","    loc1, loc2 = row['LocationA'], row['LocationB']\n","    df_pair = df[df['Location'].isin([loc1, loc2])]\n","    df_pivot = df_pair.pivot(index='Date', columns='Location', values='RainToday').dropna()\n","\n","    if not df_pivot.empty:\n","        confusion_matrix = pd.crosstab(df_pivot[loc1], df_pivot[loc2])\n","        phi, pvalue = phi_coefficient(confusion_matrix.values)\n","        df_location_pairs.at[index, 'phi'] = phi\n","        df_location_pairs.at[index, 'pvalue'] = pvalue"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hJZT0b9ixRC8"},"outputs":[],"source":["df_location_a = df_location_pairs[[\"LocationA\"]].merge(\n","    gdf_locations[[\"Location\", \"geometry\"]],\n","    how=\"left\",\n","    left_on=\"LocationA\",\n","    right_on=\"Location\",\n",")\n","gdf_location_a = gpd.GeoDataFrame(df_location_a, geometry=\"geometry\")\n","gdf_location_a_gda94 = gdf_location_a.to_crs(epsg=3112)\n","\n","df_location_b = df_location_pairs[[\"LocationB\"]].merge(\n","    gdf_locations[[\"Location\", \"geometry\"]],\n","    how=\"left\",\n","    left_on=\"LocationB\",\n","    right_on=\"Location\",\n",")\n","gdf_location_b = gpd.GeoDataFrame(df_location_b, geometry=\"geometry\")\n","gdf_location_b_gda94 = gdf_location_b.to_crs(epsg=3112)\n","\n","distance_ab = gdf_location_a_gda94[\"geometry\"].distance(gdf_location_b_gda94[\"geometry\"])\n","\n","df_location_pairs[\"distance\"] = distance_ab / 1000 # distance in kilometers"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"g2qb6uA-xUd3"},"outputs":[],"source":["df_location_pairs.sort_values(by=\"phi\", ascending=False).head(10)"]},{"cell_type":"markdown","metadata":{"id":"gVJpVhLYxeKF"},"source":["<p><em>\n","Observamos que, tal como intuíamos, las estaciones con mayor coeficiente Phi tienen entre si para la variable <code>RainToday</code>, son aquellos que se encuentran cerca geográficamente.\n","</em></p>"]},{"cell_type":"markdown","metadata":{"id":"pmsOvR7RxvWv"},"source":["Vamos a gráficar la relación entre la distancia geográfica y la correlación calculada mediante el coeficiente Phi para todos los pares de estaciones meteorológicas en el dataset."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FXV3b4Dgxydk"},"outputs":[],"source":["plt.figure(figsize=(8, 8))\n","\n","df_location_pairs[\"significant\"] = df_location_pairs['pvalue'] <= .05\n","colors = [spectral_palette[10] if sig else spectral_palette[2] for sig in df_location_pairs['significant']]\n","\n","sc = plt.scatter(df_location_pairs['phi'], np.log(df_location_pairs['distance']),\n","                 s=12, c=colors, alpha=.8, edgecolors='w', linewidth=0.5)\n","\n","plt.legend(handles=[\n","    plt.Line2D([0], [0], marker='o', color='w', markerfacecolor=spectral_palette[10], markersize=5, label='p-value <= 0.05'),\n","    plt.Line2D([0], [0], marker='o', color='w', markerfacecolor=spectral_palette[2], markersize=5, label='p-value > 0.05')\n","])\n","\n","plt.xlabel('Phi Coefficient')\n","plt.ylabel('Log de la distancia en km')\n","plt.title('Phi coefficient entre dos localidades vs distancia entre ellas\\nutilizando la variable binaria RainToday para cada día')\n","\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"uIY345BryArr"},"source":["<p><em>\n","Efectivamente, encontramos una importante correlación visual entre ambas variables graficadas: la distancia y la correlación para la variable <code>RainToday</code> para cada par de estaciones.\n","\n","Esta situación resulta interesante en cuanto refuerza nuestra hipótesis de que la codificación espacial de las estaciones meteorológicas resulta informativa para un modelo encargado de predecir lluvia para el día siguiente.\n","\n","Además, entendemos que debemos de estar atentos al momento de dividir nuestro dataset para entrenamiento y testeo del mismo. Si entrenamos nuestro modelo con datos de un día específico en estaciones meteorológicas cercanas entre si, y luego lo testeamos con una observación del mismo día pero de otra estación también cercana, corremos el riesgo de que el modelo aprenda que en esa zona geográfica llovió al día siguiente en lugar de realmente realizar una predicción desconociendo el futuro. Por esto consideramos que nuestro set de testeo y validación debe comprender un periodo de tiempo definido e incluir a todas las estaciones para ese período.\n","</em></p>"]},{"cell_type":"markdown","metadata":{"id":"kaxI-1NBvc7_"},"source":["Finalmente vamos a analizar la relación existente entre la variables categóricas relacionadas a la dirección que puede tomar el viento. Como vimos anteriormente son 16 las categorías posibles."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8lrG39d4vlUz"},"outputs":[],"source":["wind_dir_columns = [\"WindGustDir\", \"WindDir9am\", \"WindDir3pm\"]\n","\n","uniques_dirs = set(chain.from_iterable(df[column].unique() for column in wind_dir_columns))\n","print(\"Valores tipo string en las columnas WindDir: \", ' - '.join(d for d in uniques_dirs if isinstance(d, str)))\n","print(\"Otros valores en las columnas WindDir: \",' - '.join(str(d) for d in uniques_dirs if not isinstance(d, str)))"]},{"cell_type":"markdown","metadata":{"id":"rfJOFhJbwRAn"},"source":["Se observan 16 categorías, cada una de ellas representando una dirección posible del viento con una resolución de 22.5°. Vamos a transformar la dirección del viento a grados considerando la dirección 'E' como nuestro 0° (y por lo tanto también nuestro 360°). Este método nos permitirá ordenar los valores para poder analizar graficamente la relación entre las variables WindDir9am y WindDir3pm."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"j_p3rhM4wTGP"},"outputs":[],"source":["df_wind_dir = df[wind_dir_columns]\n","\n","dirs = [\"E\", \"ENE\", \"NE\", \"NNE\", \"N\", \"NNW\", \"NW\", \"WNW\", \"W\", \"WSW\", \"SW\", \"SSW\", \"S\", \"SSE\", \"SE\", \"ESE\"]\n","angles = list(np.arange(0, 360, 22.5))\n","mapping_dict = {d: a for (d, a) in zip(dirs, angles)}\n","\n","df_wind_dir[wind_dir_columns] = df_wind_dir[wind_dir_columns].applymap(lambda x: mapping_dict.get(x, x))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"C6iORz9hxAHO"},"outputs":[],"source":["confusion_matrix = pd.crosstab(df_wind_dir[\"WindDir9am\"], df_wind_dir[\"WindDir3pm\"])\n","\n","# Plot confusion matrix as a heatmap\n","plt.figure(figsize=(8, 8))\n","sns.heatmap(confusion_matrix, annot=True, fmt='d', cmap='coolwarm', xticklabels=dirs, yticklabels=dirs, cbar=False)\n","plt.xlabel('WindDir9am')\n","plt.ylabel('WindDir3pm')\n","plt.title('Confusion Matrix Heatmap')\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"zs9K6fwuxXLn"},"source":["_Se observa una estrecha relación visual entre la dirección del viento registrada a las 9am y la dirección del viento registrada a las 3pm. Debido a la condición circular de las posibles direcciones que el viento puede tomar, aparecen valores altos en las esquinas inferior izquierda y superior derecha del gráfico._"]},{"cell_type":"markdown","metadata":{"id":"Q_L5zlewRoQc"},"source":["##### 2.1.5.2. Requerimientos de datos"]},{"cell_type":"markdown","metadata":{"id":"0eYzv2e2AKqd"},"source":["Como resumen del análisis de datos, éstos presentan la siguiente información:\n","\n","<font color='blue'>ℹ</font> El conjunto contiene un total de <code>145460</code> observaciones.\n","\n","<font color='blue'>ℹ</font> Las columnas cualitativas-nominales son: <code>WindGustDir</code> | <code>WindDir9am</code> | <code>WindDir3pm</code> | <code>Location</code>\n","\n","<font color='blue'>ℹ</font> Las columnas cuantitativas-continuas son: <code>MinTemp</code> | <code>MaxTemp</code> | <code>Rainfall</code> | <code>Evaporation</code> | <code>Sunshine</code> | <code>WindGustSpeed</code> | <code>WindSpeed9am</code> | <code>WindSpeed3pm</code> | <code>Humidity9am</code> | <code>Humidity3pm</code> | <code>Pressure9am</code> | <code>Pressure3pm</code> | <code>Cloud9am</code> | <code>Cloud3pm</code> | <code>Temp9am</code> | <code>Temp3pm</code>\n","\n","<font color='blue'>ℹ</font> Hay una columna booleana: <code>RainToday</code>\n","\n","<font color='blue'>ℹ</font> Hay una columna del tipo fecha: <code>Date</code>\n","\n","<font color='green'>✔</font> La variable objetivo es del tipo booleanna: <code>RainTomorrow</code>\n","\n","<font color='green'>✔</font> No presentan datos duplicados.\n","\n","<font color='red'>❌</font> Gran propoción de valores nulos en: <code>Evaporation</code> | <code>Sunshine</code> | <code>Cloud9am</code> | <code>Cloud3pm</code>\n","\n","<font color='green'>✔</font> No hay valores nulos en las columnas: <code>Date</code> | <code>Location</code>\n","\n","<font color='green'>✔</font> Máximos y mínimos acordes.\n","\n","<font color='yellow'>⚠</font> Hay un desbalance de clases en la variable target.\n","\n","<font color='green'>✔</font> Hay muchas columnas que se pueden estandarizar y siguen una distribución cuasi-normal como: <code>MinTemp</code> | <code>MaxTemp</code> | <code>Humidity3pm</code> | <code>Pressure9am</code> | <code>Pressure3pm</code> | <code>Temp9am</code>\n","\n","<font color='yellow'>⚠</font> Hay columnas que son no-normales como: <code>Rainfall</code> | <code>Evaporation</code>\n","\n","<font color='yellow'>⚠</font> Hay columnas que tienen colas pesadas y livianas como: <code>Sunshine</code> | <code>WindGustSpeed</code> | <code>WindSpeed9am</code> | <code>WindSpeed3pm</code> | <code>Humidity9am</code>\n","\n","<font color='yellow'>⚠</font> Las columnas <code>Colud9am</code> y <code>Cloud3pm</code> podrían llevar un tratamiento de cuantitativas-discretas.\n","\n","<font color='yellow'>⚠</font> Hay valores faltantes en el resto de las columnas, pero la mayoría (del 40 para arriba son las columnas): <code>Evaporation</code> | <code>Sunshine</code> | <code>Cloud9am</code> | <code>Cloud3pm</code>\n","\n","<font color='blue'>ℹ</font> Hay relación entre los datos faltantes (<code>RainTomorrow</code>), fechas y localidades.\n","\n","<font color='red'>❌</font> La columna <code>RainFall</code> contiene muchos outliers.\n","\n","<font color='yellow'>⚠</font> Hay variables áltamente co-relacionadas como:\n","\n","<pre><code>MinTemp</code> ⟷ <code>Temp9am</code>\n","\n","<code>MaxTemp</code> ⟷ <code>Temp3am</code> | <code>Temp9am</code>\n","\n","<code>Pressure9am</code> ⟷ <code>Pressure3pm</code>\n","\n","<code>Temp3am</code> ⟷ <code>Temp9am</code></pre>"]},{"cell_type":"markdown","metadata":{"id":"pxZTr6b-6ygC"},"source":["<!-- TODO -->"]},{"cell_type":"markdown","metadata":{"id":"C6ZbqjAO6ygC"},"source":["#### 2.1.6. Revisión de los documentos de salida\n","\n","<!-- TODO: Borrar? -->"]},{"cell_type":"markdown","metadata":{"id":"0u8rYBIMDXFJ"},"source":["El documento de salida es un único nootebook autocontenido."]},{"cell_type":"markdown","metadata":{"id":"ir2QRoQ26ygD"},"source":["### 2.2. Ingeniería de datos (Data Engineering)"]},{"cell_type":"markdown","metadata":{"id":"aZIeMKWR6ygD"},"source":["#### 2.2.1. Seleccionar datos"]},{"cell_type":"markdown","metadata":{"id":"ZRHRzN7mEG2K"},"source":["🔮 Futuras versiones 🔮\n","\n","<!-- TODO: Seleccionar características importantes según el conocimiento del negocio. También ver si la correlación no impacta en los atributos de entrada. Es una buen práctica descartar características que no son importantes ya que no aportan al modelo pero ofrecen posibles puntos de errores -->"]},{"cell_type":"markdown","metadata":{"id":"rnq-srUM6ygE"},"source":["#### 2.2.2. Limpiar datos"]},{"cell_type":"markdown","metadata":{"id":"55mbnx1K-Y51"},"source":["##### 2.2.2.1 Imputaciones\n"]},{"cell_type":"markdown","metadata":{"id":"WBBtI3sGEQPR"},"source":["Como se observó en la sección anterior, se tienen variables tanto numéricas como categóricas a las que les faltan algunos valores."]},{"cell_type":"markdown","metadata":{"id":"mIyG6Nii3nk0"},"source":["###### Considerando las variables categóricas:\n"]},{"cell_type":"markdown","metadata":{"id":"8Iju--NvEWK3"},"source":["Se toma la moda para completar los valores faltantes.\n","\n","Sin embargo, para el caso de \"RainToday\" y \"RainTomorrow\", el porcentaje de valores faltantes es bajo (~2%) y parece ser aleatorio; por lo que podríamos excluir las observaciones donde estas columnas no tengan estos valores."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qakPplEy_zWU"},"outputs":[],"source":["columnas_cat = ['Location', 'WindGustDir', 'WindDir9am', 'WindDir3pm']"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"o-oOm1dq3qXq"},"outputs":[],"source":["df_cat_imputed = df.copy()\n","\n","# Imputación con la Moda para Variables Categóricas\n","categorical_imputer = SimpleImputer(strategy='most_frequent')\n","df_cat_imputed[columnas_cat] = categorical_imputer.fit_transform(df_cat_imputed[columnas_cat])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FFDCprwBDCq6"},"outputs":[],"source":["missing_values = df_cat_imputed.isna().sum()\n","observations = len(df_cat_imputed)\n","\n","missing_values_df = pd.DataFrame({\n","    'Variable': missing_values.index,\n","    'Valores faltantes': missing_values.values,\n","    'Cantidad de observaciones': observations,\n","    'Porcentaje valores faltantes': (missing_values.values / observations) * 100\n","})\n","\n","missing_values_df.sort_values(by=\"Porcentaje valores faltantes\", ascending=0)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pkagGqO5BsCa"},"outputs":[],"source":["# Retiramos las observaciones sin valores de \"RainToday\" y \"RainTomorrow\"\n","invalid_rows = df[df['RainToday'].isna() | df['RainTomorrow'].isna()].index\n","\n","df_cat_imputed.drop(invalid_rows, inplace=True)\n","df_cat_imputed.shape"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fD0xC08tI8lD"},"outputs":[],"source":["# Chequeamos los valores de nuevo\n","missing_values = df_cat_imputed.isna().sum()\n","observations = len(df_cat_imputed)\n","\n","missing_values_df = pd.DataFrame({\n","    'Variable': missing_values.index,\n","    'Valores faltantes': missing_values.values,\n","    'Cantidad de observaciones': observations,\n","    'Porcentaje valores faltantes': (missing_values.values / observations) * 100\n","})\n","\n","missing_values_df.sort_values(by=\"Porcentaje valores faltantes\", ascending=0)"]},{"cell_type":"markdown","metadata":{"id":"eHiwF-Cx3d88"},"source":["###### Considerando las variables numéricas:\n"]},{"cell_type":"markdown","metadata":{"id":"QQXVzToiG1Ox"},"source":["\n","Considerando que las faltas son por razones aleatorias, y, dado que la mayoría de las variables presentan oblicuidad, se considera la mediana como un buen candidato para reemplazar a los valores faltantes. Para este caso, se puede utilizar el SimpleImputer considerando una imputación de una variable.\n","\n","Alternativamente, se puede utilizar un método multivariado como KNN (vecinos cercanos) y comparar con la imputación simple."]},{"cell_type":"markdown","metadata":{"id":"AaWb5tT05DYY"},"source":["De esta forma se tienen dos alternativas que pueden compararse para determinar cuál es mejor."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gsli2M3p3nJ9"},"outputs":[],"source":["from sklearn.impute import SimpleImputer, KNNImputer\n","\n","# SimpleImputer (mediana)\n","def simple_imputer_mean(df, numerical_vars):\n","    imputer = SimpleImputer(strategy='mean')\n","    df[numerical_vars] = imputer.fit_transform(df[numerical_vars])\n","    return df\n","\n","# KNN Imputer\n","def knn_imputer(df, numerical_vars, n_neighbors=5):\n","    imputer = KNNImputer(n_neighbors=n_neighbors)\n","    df[numerical_vars] = imputer.fit_transform(df[numerical_vars])\n","    return df"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kR7DDlGa5buX"},"outputs":[],"source":["columnas_num = ['MinTemp', 'MaxTemp', 'Rainfall', 'Evaporation', 'Sunshine',\n","                'WindGustSpeed','WindSpeed9am', 'WindSpeed3pm', 'Humidity9am', 'Humidity3pm',\n","                'Pressure9am', 'Pressure3pm', 'Cloud9am', 'Cloud3pm', 'Temp9am','Temp3pm']"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"30Bb5O_V4pVV"},"outputs":[],"source":["# Se realiza la imputación simple de una sola variable utilizando la mediana:\n","df_mean_imputed = simple_imputer_mean(df_cat_imputed.copy(), columnas_num)\n","df_mean_imputed.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GAPdiG7U4zSD"},"outputs":[],"source":["# Se realiza la imputación multivariada utilizando los vecinos cercanos (KNN):\n","df_knn_imputed = knn_imputer(df_cat_imputed.copy(), columnas_num, n_neighbors=3)\n","df_knn_imputed.head()"]},{"cell_type":"markdown","metadata":{"id":"6gGEmglOJvbq"},"source":["De esta forma se tienen dos opciones para comparar:\n","`df_mean_imputed` y `df_knn_imputed`"]},{"cell_type":"markdown","metadata":{"id":"msaVPOAaPKdm"},"source":["##### Análisis postimputación\n"]},{"cell_type":"markdown","metadata":{"id":"ReJ6e1MDG4wR"},"source":["Para entender cómo ha cambiado el dataset luego de las dos alternativas de imputación para los datos numéricos, se puede realizar una análisis de las estadísticas descriptivas del dataset."]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"id":"KerFNbLrPu-k"},"outputs":[],"source":["# Estadísticas descriptivas\n","original = df.describe()\n","mean_imputed_stats = df_mean_imputed.describe()\n","knn_imputed_stats = df_knn_imputed.describe()\n","\n","# Se muestran las estadísticas para comparar\n","print(\"Estadísticas: dataset original:\\n\", original)\n","print(\"\\nEstadísticas: imputación (mediana):\\n\", mean_imputed_stats)\n","print(\"\\nEstadísticas: imputación (KNN):\\n\", knn_imputed_stats)"]},{"cell_type":"markdown","metadata":{"id":"gStF8EwCRdhK"},"source":["Analisis por variable. Se consideran solo las que tenían un gran porcentaje de valores faltantes (>10%)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"R1LElLDkRStb"},"outputs":[],"source":["def plot_distributions(original, mean_imputed, knn_imputed, variable, figsize=(15, 3)):\n","    plt.figure(figsize=figsize)\n","\n","    # Original data\n","    plt.subplot(1, 3, 1)\n","    sns.histplot(original[variable].dropna(), kde=True)\n","    plt.title(f'Original: \"{variable}\"')\n","\n","    # Mean imputed data\n","    plt.subplot(1, 3, 2)\n","    sns.histplot(mean_imputed[variable], kde=True)\n","    plt.title(f'Imputación (mediana): \"{variable}\"')\n","\n","    # KNN imputed data\n","    plt.subplot(1, 3, 3)\n","    sns.histplot(knn_imputed[variable], kde=True)\n","    plt.title(f'Imputación (KNN): \"{variable}\"')\n","\n","    plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3BsXUU-zRz0z"},"outputs":[],"source":["# Sunshine (48%), Evaporation (43%), Cloud3pm (40%), Cloud9am (38%), Pressure9am (10%), Pressure3pm (10%)\n","top_six = ['Sunshine', 'Evaporation', 'Cloud3pm', 'Cloud9am', 'Pressure9am', 'Pressure3pm']\n","\n","for variable in top_six:\n","  plot_distributions(df, df_mean_imputed, df_knn_imputed, variable, figsize=(15, 4))"]},{"cell_type":"markdown","metadata":{"id":"war43mFPUTrV"},"source":["Como se puede observar en los gráficos, las distribuciones se han visto afectadas (en especial \"Sunshine\") por las imputaciones.\n","\n","El caso de Sunshine es particular y debería ser analizado a mayor profundidad para entender que opciones pueden existir para eliminar el sesgo excesivo que se observa.\n","\n","En general, el método de imputación utilizando KNN es mejor que el método que se basa solamente en la mediana."]},{"cell_type":"markdown","metadata":{"id":"xdKyormU6ygE"},"source":["#### 2.2.3. Codificar de variables categóricas"]},{"cell_type":"markdown","metadata":{"id":"dSRT2Zr3xu2X"},"source":["A partir del análisis realizado hasta el momento encontramos las siguientes variables categóricas a codificar:\n","\n","- \"Date\"\n","- \"Location\"\n","- \"WindGustDir\", \"WindDir9am\", \"WindDir3pm\""]},{"cell_type":"markdown","metadata":{"id":"wKtPNakAx-Ee"},"source":["##### Variable \"Date\"\n"]},{"cell_type":"markdown","metadata":{"id":"4YIc3_5mG_kZ"},"source":["Cada observación se registra con el día, mes y año. Sin embargo, tratar estos componentes como características independientes presenta varios inconvenientes:\n","\n","Días del mes:\n","- La representación numérica entera de los días no refleja la condición circular de anterioridad de los días altos de un mes respecto a los bajos del siguiente. Además, los meses tienen diferentes números de días por lo que aun si el modelo logra capturar esa circularidad, puede ser dificultoso entender que la distancia entre un determinado número de día y otro no es siempre la misma (ej. entre el 28 de un mes y el 1 del siguiente puede haber 1 o 4 días).\n","- El valor numérico del día en un mes no informa directamente sobre la probabilidad de lluvia para el día siguiente.\n","\n","Meses:\n","- Tienen una moderadamente alta cardinalidad para ser representados mediante one-hot encoding. pAdemás, esta técnica otorga la misma distancia euclideana a cada par de vectores que representan cada uno de los 12 meses.\n","- Si se usan valores enteros del 1 al 12, se pierde la circularidad, complicando la interpretación de la distancia temporal entre los útlimos meses y los primeros.\n","\n","Años:\n","- Son informativos para variaciones climáticas anuales, pero pueden introducir problemas al generalizar a datos de años no vistos durante el entrenamiento.\n","\n","Por estas razones, codificamos la fecha como el número de día del año, utilizando coordenadas polares para reflejar su estructura circular. De este modo se traduce la información brindada por la fecha a su ubicación dentro de un año calendario, codificandola en dos nuevas variables."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SA9LFADoyDvd"},"outputs":[],"source":["def plot_day_of_year_in_unit_circle():\n","    # Create a DataFrame to hold the values\n","    days = np.arange(1, 366, 2)\n","    days_in_year = 366\n","\n","    angles = 2 * np.pi * (days - 1) / days_in_year\n","    cos_vals = np.cos(angles)\n","    sin_vals = np.sin(angles)\n","\n","    df_days = pd.DataFrame({\n","        'Day': days,\n","        'Angle': angles,\n","        'DayCos': cos_vals,\n","        'DaySin': sin_vals\n","    })\n","\n","    # Randomly select a day\n","    random_day = 37\n","    random_day_row = df_days[df_days['Day'] == random_day]\n","\n","    # Plot the circle with 365 dots\n","    plt.figure(figsize=(5, 5))\n","    plt.plot(df_days['DayCos'], df_days['DaySin'], 'bo', markersize=1)  # Circle with 365 dots\n","\n","    # Highlight the random day\n","    plt.plot(random_day_row['DayCos'], random_day_row['DaySin'], 'ro', markersize=2)\n","    plt.text(random_day_row['DayCos'].values[0] + 0.02, random_day_row['DaySin'].values[0],\n","            f\"Day {random_day}\\n({random_day_row['DayCos'].values[0]:.2f},{random_day_row['DaySin'].values[0]:.2f})\",\n","            fontsize=6, ha='left', va='bottom')\n","\n","    # Draw x and y axes\n","    plt.axhline(0, color='grey', linestyle='--', linewidth=0.5)\n","    plt.axvline(0, color='grey', linestyle='--', linewidth=0.5)\n","\n","    # Draw the angle\n","    plt.plot([0, 1], [0, 0], 'k-', linewidth=1)\n","    plt.plot([0, random_day_row['DayCos'].values[0]], [0, random_day_row['DaySin'].values[0]], 'k-', linewidth=1)\n","\n","    angle_text = f\"{np.degrees(random_day_row['Angle'].values[0]):.2f}°\"\n","    label_angle = random_day_row['Angle'].values[0] / 4\n","    plt.text(0.3 * np.cos(label_angle) + 0.05, 0.3 * np.sin(label_angle) + 0.05, angle_text, color='k', fontsize=8, ha='center', va='center')\n","\n","    # Mark and label the cosine value on the axes\n","    plt.plot([random_day_row['DayCos'].values[0], random_day_row['DayCos'].values[0]], [0, random_day_row['DaySin'].values[0]], 'k--', linewidth=0.4)\n","    plt.text(random_day_row['DayCos'].values[0], -0.05,\n","            f\"{random_day_row['DayCos'].values[0]:.2f}\",\n","            fontsize=7,\n","            ha='center',\n","            va='top')\n","\n","    plt.plot([0, random_day_row['DayCos'].values[0]], [random_day_row['DaySin'].values[0], random_day_row['DaySin'].values[0]], 'k--', linewidth=0.4)\n","    plt.text(-0.05, random_day_row['DaySin'].values[0],\n","            f\"{random_day_row['DaySin'].values[0]:.2f}\",\n","            fontsize=7,\n","            ha='right',\n","            va='center')\n","\n","    # Set equal aspect ratio\n","    plt.gca().set_aspect('equal', adjustable='box')\n","\n","    # Labels and title\n","    plt.xlabel('DayCos')\n","    plt.ylabel('DaySin')\n","    plt.title('Representación del día del año en coordenadas polares', fontsize=10)\n","\n","    plt.tick_params(axis='both', labelsize=6)\n","\n","    plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rl_dWYPAyFue"},"outputs":[],"source":["plot_day_of_year_in_unit_circle()"]},{"cell_type":"markdown","metadata":{"id":"eOpONoigyKdE"},"source":["<p><em>\n","Codificar la fecha de esta manera cuenta con la ventaja de que indirectamente estamos incorporando la información de las estaciones del año, ya que para valores positivos de CosDay y SinDay nos encontramos con días de verano mientras que para valores negativos de ambos lo hacemos con días de invierno. En la diagonal opuesta sucede algo similar con días de primavera y otoño.\n","</em></p>"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kmW1KYUDyNZ2"},"outputs":[],"source":["df['DayOfYear'] = df['Date'].dt.dayofyear\n","\n","# Determine the number of days in the year for each date (taking leap years into account)\n","df['DaysInYear'] = df['Date'].dt.is_leap_year.apply(lambda leap: 366 if leap else 365)\n","\n","# Convert day of the year to angle in radians, dividing by DaysInYear + 1\n","df['Angle'] = 2 * np.pi * (df['DayOfYear'] - 1) / (df['DaysInYear'])\n","\n","df['DayCos'] = np.cos(df['Angle'])\n","df['DaySin'] = np.sin(df['Angle'])\n","\n","df = df.drop(columns=[\"DayOfYear\", \"DaysInYear\", \"Angle\"])"]},{"cell_type":"markdown","metadata":{"id":"vaF065J-yQ24"},"source":["Vamos a entrenar un modelo de Regresión Logística utilizando únicamente las columnas creadas como features para predecir la variable objetivo <code>RainTomorrow</code>. Vamos a comparar los resultados obtenidos mediante esa codificación con la codificación ordinal del día y el mes para examinar el desempeño de la técnica utilizada."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7nku3ml6yQR2"},"outputs":[],"source":["# Initialize lists to store results\n","random_states = [10, 20, 30, 40, 50, 60, 70, 80, 90, 100]\n","polar_results = []\n","integer_results = []\n","\n","# Methodology 1: DayCos, DaySin\n","df_model_cos_sin = df[[\"DayCos\", \"DaySin\", \"RainTomorrow\"]]\n","df_model_cos_sin = df_model_cos_sin.dropna(how='any').reset_index(drop=True)\n","\n","# Methodology 2: Day, Month\n","df_model_day_month = df[[\"Date\", \"RainTomorrow\"]]\n","df_model_day_month[\"Day\"] = df_model_day_month[\"Date\"].dt.day\n","df_model_day_month[\"Month\"] = df_model_day_month[\"Date\"].dt.month\n","df_model_day_month = pd.get_dummies(df_model_day_month, columns=[\"Month\"], drop_first=True, dtype=int)\n","df_model_day_month = df_model_day_month.drop(columns=\"Date\")\n","df_model_day_month = df_model_day_month.dropna(how='any').reset_index(drop=True)\n","\n","for random_state in random_states:\n","    # Methodology 1: DayCos, DaySin\n","    X1 = df_model_cos_sin.drop(columns=\"RainTomorrow\")\n","    y1 = df_model_cos_sin[\"RainTomorrow\"]\n","    X1_train, X1_test, y1_train, y1_test = train_test_split(X1, y1, test_size=0.2, random_state=random_state)\n","\n","    scaler = StandardScaler()\n","    X1_train_scaled = scaler.fit_transform(X1_train)\n","    X1_test_scaled = scaler.transform(X1_test)\n","\n","    model1 = LogisticRegression(class_weight='balanced')\n","    model1.fit(X1_train_scaled, y1_train)\n","    y1_pred = model1.predict(X1_test_scaled)\n","\n","    acc1 = accuracy_score(y1_test, y1_pred)\n","    f1_1 = f1_score(y1_test, y1_pred)\n","\n","    polar_results.append({\n","        'RandomState': random_state,\n","        'PolarAccuracy': round(acc1, 3),\n","        'PolarF1': round(f1_1, 3)\n","    })\n","\n","    # Methodology 2: Day, Month\n","    X2 = df_model_day_month.drop(columns=\"RainTomorrow\")\n","    y2 = df_model_day_month[\"RainTomorrow\"]\n","    X2_train, X2_test, y2_train, y2_test = train_test_split(X2, y2, test_size=0.2, random_state=random_state)\n","\n","    scaler = StandardScaler()\n","    X2_train_scaled = scaler.fit_transform(X2_train)\n","    X2_test_scaled = scaler.transform(X2_test)\n","\n","    model2 = LogisticRegression(class_weight='balanced')\n","    model2.fit(X2_train_scaled, y2_train)\n","    y2_pred = model2.predict(X2_test_scaled)\n","\n","    acc2 = accuracy_score(y2_test, y2_pred)\n","    f1_2 = f1_score(y2_test, y2_pred)\n","\n","    integer_results.append({\n","        'RandomState': random_state,\n","        'IntegerAccuracy': round(acc2, 3),\n","        'IntegerF1': round(f1_2, 3)\n","    })\n","\n","# Create DataFrames from results\n","polar_df = pd.DataFrame(polar_results)\n","integer_df = pd.DataFrame(integer_results)\n","\n","# Merge the DataFrames on 'RandomState'\n","comparison_df = pd.merge(polar_df, integer_df, on='RandomState')\n","\n","# Display the comparison DataFrame\n","comparison_df"]},{"cell_type":"markdown","metadata":{"id":"3rOa1rnQyb0e"},"source":["_Observamos que los resultados obtenidos con la codificación de las fechas en coordenadas polares tuvo una performance superior para el F1 score para todos los valores de random state seleccionados. Si bien el accuracy de los modelos codificados con el número para el día y one-hot pára los meses es superior, recordamos que debido al desbalanceo entre clases, el accuracy se encuentra inflado para modelos que predicen mayormente que no llueve. Por otro lado, el F1 score refleja un balance entre la precisión y la sensibilidad del modelo._"]},{"cell_type":"markdown","metadata":{"id":"LU-TcTRIyg-2"},"source":["##### Variable \"Location\""]},{"cell_type":"markdown","metadata":{"id":"EU1vRzkPHOAw"},"source":["Como evaluamos en el apartado anterior, la distancia entre estaciones meteorológicas es inversamente proporcional al coeficiente Phi para la variable RainToday. En decir, si en una estación se registra lluvia, es probable que en una estación cercana también se registre la misma condición.\n","\n","Vamos a codificar la locación a partir de las coordenadas de Latitud y Longitud obtenidas mediante geolocalización con Open Street Map. De este modo informamos al modelo con la relación espacial entre las estaciones. Además, evitamos la representación dispersa que implicara utilizar one hot encoding con una variable de alta cardinalidad."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"V9Gfynrqylx-"},"outputs":[],"source":["df = pd.merge(df, gdf_locations.drop(columns=\"geometry\"), on=\"Location\")"]},{"cell_type":"markdown","metadata":{"id":"B0-vILzyypVe"},"source":["##### Variables \"WindDir\""]},{"cell_type":"markdown","metadata":{"id":"xJCGv4STHRSi"},"source":["Al igual que sucede con la variable de fecha, las variables relacionadas con la dirección del viento también poseen un orden circular. En el caso de estas últimas variables, incluso, necesitamos un menor grado de abstracción ya que la dirección del viento puede representarse intuitivamente como la dirección de un vector en dos dimensiones (x, y). El eje x representa la dirección Este-Oeste y el eje y representa la dirección Norte-Sur."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-FAM0TQ6yzMX"},"outputs":[],"source":["dirs = [\"E\", \"ENE\", \"NE\", \"NNE\", \"N\", \"NNW\", \"NW\", \"WNW\", \"W\", \"WSW\", \"SW\", \"SSW\", \"S\", \"SSE\", \"SE\", \"ESE\"]\n","angles = np.radians(np.arange(0, 360, 22.5))\n","mapping_dict = {d: a for (d, a) in zip(dirs, angles)}\n","\n","wind_dir_columns = [\"WindGustDir\", \"WindDir9am\", \"WindDir3pm\"]\n","for column in wind_dir_columns:\n","    df[f\"{column}Angle\"] = df[column].map(mapping_dict)\n","\n","    df[f\"{column}Cos\"] = np.cos(df[f\"{column}Angle\"].astype(float))\n","    df[f\"{column}Sin\"] = np.sin(df[f\"{column}Angle\"].astype(float))\n","\n","    df = df.drop(columns=f\"{column}Angle\")"]},{"cell_type":"markdown","metadata":{"id":"uf9PCj0DzN2v"},"source":["Vamos a entrenar un modelo de Regresión Logística utilizando las columnas creadas como features para predecir la variable objetivo <code>RainTomorrow</code>, incluyendo las variables de velocidad del viento, ubicación de la estación meteorológica y la presencia o ausencia de lluvia para el día registrado. La incorporación de estás últimas variables se debe a que no se espera que los datos de la dirección del viento por si solo resulte informativa para predecir la lluvia del día siguiente. Vamos a comparar los resultados obtenidos mediante esa codificación con la codificación mediante one-hot encoding de la dirección del viento para examinar el desempeño de la técnica utilizada."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DlhlbpHhy1MG"},"outputs":[],"source":["# Initialize lists to store results\n","random_states = [10, 20, 30, 40, 50, 60, 70, 80, 90, 100]\n","polar_results = []\n","integer_results = []\n","\n","# Methodology 1: DayCos, DaySin\n","df_model_cos_sin = df[\n","    [\n","        \"Lat\",\n","        \"Lon\",\n","        \"WindGustSpeed\",\n","        \"WindSpeed9am\",\n","        \"WindSpeed3pm\",\n","        \"WindGustDirCos\",\n","        \"WindGustDirSin\",\n","        \"WindDir9amCos\",\n","        \"WindDir9amSin\",\n","        \"WindDir3pmCos\",\n","        \"WindDir3pmSin\",\n","        \"RainToday\",\n","        \"RainTomorrow\",\n","    ]\n","]\n","df_model_cos_sin = df_model_cos_sin.dropna(how=\"any\").reset_index(drop=True)\n","\n","# Methodology 2: Day, Month\n","df_model_one_hot = df[\n","    [\n","        \"Lat\",\n","        \"Lon\",\n","        \"WindGustSpeed\",\n","        \"WindSpeed9am\",\n","        \"WindSpeed3pm\",\n","        \"WindGustDir\",\n","        \"WindDir9am\",\n","        \"WindDir3pm\",\n","        \"RainToday\",\n","        \"RainTomorrow\",\n","    ]\n","]\n","df_model_one_hot = df_model_one_hot.dropna(how=\"any\").reset_index(drop=True)\n","\n","df_model_one_hot = pd.get_dummies(\n","    df_model_one_hot,\n","    columns=[\"WindGustDir\", \"WindDir9am\", \"WindDir3pm\"],\n","    drop_first=True,\n","    dtype=int,\n",")\n","\n","for random_state in random_states:\n","    # Methodology 1: DayCos, DaySin\n","    X1 = df_model_cos_sin.drop(columns=\"RainTomorrow\")\n","    y1 = df_model_cos_sin[\"RainTomorrow\"]\n","    X1_train, X1_test, y1_train, y1_test = train_test_split(\n","        X1, y1, test_size=0.2, random_state=random_state\n","    )\n","\n","    scaler = StandardScaler()\n","    X1_train_scaled = scaler.fit_transform(X1_train)\n","    X1_test_scaled = scaler.transform(X1_test)\n","\n","    model1 = LogisticRegression(class_weight=\"balanced\")\n","    model1.fit(X1_train_scaled, y1_train)\n","    y1_pred = model1.predict(X1_test_scaled)\n","\n","    acc1 = accuracy_score(y1_test, y1_pred)\n","    f1_1 = f1_score(y1_test, y1_pred)\n","\n","    polar_results.append(\n","        {\n","            \"RandomState\": random_state,\n","            \"PolarAccuracy\": round(acc1, 3),\n","            \"PolarF1\": round(f1_1, 3),\n","        }\n","    )\n","\n","    # Methodology 2: Day, Month\n","    X2 = df_model_one_hot.drop(columns=\"RainTomorrow\")\n","    y2 = df_model_one_hot[\"RainTomorrow\"]\n","    X2_train, X2_test, y2_train, y2_test = train_test_split(\n","        X2, y2, test_size=0.2, random_state=random_state\n","    )\n","\n","    scaler = StandardScaler()\n","    X2_train_scaled = scaler.fit_transform(X2_train)\n","    X2_test_scaled = scaler.transform(X2_test)\n","\n","    model2 = LogisticRegression(class_weight=\"balanced\")\n","    model2.fit(X2_train_scaled, y2_train)\n","    y2_pred = model2.predict(X2_test_scaled)\n","\n","    acc2 = accuracy_score(y2_test, y2_pred)\n","    f1_2 = f1_score(y2_test, y2_pred)\n","\n","    integer_results.append(\n","        {\n","            \"RandomState\": random_state,\n","            \"IntegerAccuracy\": round(acc2, 3),\n","            \"IntegerF1\": round(f1_2, 3),\n","        }\n","    )\n","\n","# Create DataFrames from results\n","polar_df = pd.DataFrame(polar_results)\n","integer_df = pd.DataFrame(integer_results)\n","\n","# Merge the DataFrames on 'RandomState'\n","comparison_df = pd.merge(polar_df, integer_df, on=\"RandomState\")\n","\n","# Display the comparison DataFrame\n","comparison_df"]},{"cell_type":"markdown","metadata":{"id":"GUKoMnBkzWzQ"},"source":["<p><em>\n","La comparación realizada no arroja evidencia contundente de la superioridad de alguno de los métodos sobre el otro para la tarea que estamos realizando.\n","\n","Tanto la codificación de coordenadas polares como la codificación one-hot ofrecen distintas ventajas y potenciales riesgos como representación de la dirección del viento.\n","\n","La codificación de coordenadas polares, con su representación compacta de solo dos características (CosAngle y SinAngle), es computacionalmente eficiente y captura la naturaleza circular de la dirección del viento, lo que la hace adecuada para modelos que manejan datos continuos de manera efectiva. Además puede constribuir a la interpretabilidad si se entiende la relación entre los valores que asumen estas variables y los puntos cardinales. Se intuye que este tipo de codificación puede resultar beneficiosa para modelos como SVM que son computacionalmente muy costosos en altas dimensiones, y modelos como KNN que se ven favorecidos por una codificación que respeta la distancia espacial entre categorías. Por otro lado, los árboles de decisión trabajan sobre separaciones ortogonales, esto puede complejizar la tarea de aislar los observaciones correspondientes a una sóla de las categorías posibles. Sin embargo, no se descarta que resulte beneficioso la posibilidad de separar direcciones del viento similares entre si de forma eficiente.\n","\n","Por otro lado, la codificación one-hot garantiza la independencia de las variables creadas, lo que es especialmente beneficioso para modelos lineales como la regresión logística. Aumentando el espacio de características a 15 características binarias por cada una de las variables originales, preserva la granularidad de los datos categóricos sin asumir ningún orden inherente. Las nuevas variables creadas generan un dataset de entrenamiento altamente disperso lo que puede resultar problemático para modelos de redes neuronales. La representación categórica de los datos, incluso sin la necesidad de realizar one hot encoding, puede resultar beneficioso para modelos generados a partir de ensambles de árboles.\n","\n","Dadas las ligeras diferencias en las puntuaciones F1 observadas en nuestros experimentos y las diferentes fortalezas de cada método, no encontramos evidencia definitiva para elegir uno sobre el otro. Por lo tanto, continuaremos probando nuestros modelos utilizando ambas representaciones, seleccionando el método más apropiado en función de casos de uso específicos.\n","</em></p>"]},{"cell_type":"markdown","metadata":{"id":"wvOwr5y96ygF"},"source":["#### 2.2.4. Evaluar importancia de las variables"]},{"cell_type":"markdown","metadata":{"id":"L16WFBHpzohP"},"source":["Vamos a realizar un análisis de componentes principales para realizar una inspección visual e intuit la potencialidad de las variables elegidas para separar entre clases de la variable objetivo."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zXp3HoMgzpLg"},"outputs":[],"source":["features_list = [\n","    \"DayCos\",\n","    \"DaySin\",\n","    \"Lat\",\n","    \"Lon\",\n","    \"MinTemp\",\n","    \"MaxTemp\",\n","    \"Rainfall\",\n","    \"Evaporation\",\n","    \"Sunshine\",\n","    \"WindGustDirCos\",\n","    \"WindGustDirSin\",\n","    \"WindGustSpeed\",\n","    \"WindDir9amCos\",\n","    \"WindDir9amSin\",\n","    \"WindSpeed9am\",\n","    \"WindDir3pmCos\",\n","    \"WindDir3pmSin\",\n","    \"WindSpeed3pm\",\n","    \"Humidity9am\",\n","    \"Humidity3pm\",\n","    \"Pressure9am\",\n","    \"Pressure3pm\",\n","    \"Cloud9am\",\n","    \"Cloud3pm\",\n","    \"Temp9am\",\n","    \"Temp3pm\",\n","    \"RainToday\",\n","]\n","\n","n_max_feat = len(max(features_list, key=len))\n","\n","df_model = df[features_list + [\"RainTomorrow\"]]\n","df_model = df_model.dropna(how='any').reset_index(drop=True)\n","\n","X = df_model[features_list]\n","y = df_model[\"RainTomorrow\"]\n","\n","# Plot training data in two dimensions to visualize if captured features relate to labels.\n","scaler_manual = StandardScaler()\n","X_scaled = scaler_manual.fit_transform(X)\n","\n","# Apply PCA for dimensionality reduction\n","pca = PCA(random_state=42)\n","X_pca = pca.fit_transform(X_scaled)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"X1Zi68cszr0h"},"outputs":[],"source":["# Plot subsample\n","random.seed(42)\n","subsample_size = 5000\n","subsample_indices = random.sample(range(len(y)), subsample_size)\n","\n","X_subsample = X_pca[subsample_indices]\n","y_subsample = y[subsample_indices]\n","\n","plt.figure(figsize=(5, 5))\n","\n","plt.scatter(\n","    X_subsample[y_subsample == 0, 0],\n","    X_subsample[y_subsample == 0, 1],\n","    c=spectral_palette[9],\n","    label=\"Negativo para RainTomorrow\",\n","    s=5,\n","    alpha=0.4,\n",")\n","\n","plt.scatter(\n","    X_subsample[y_subsample == 1, 0],\n","    X_subsample[y_subsample == 1, 1],\n","    c=spectral_palette[1],\n","    label=\"Positivo para RainTomorrow\",\n","    s=5,\n","    alpha=0.4\n",")\n","\n","x_min, x_max = -10, 10\n","y_min, y_max = -10, 10\n","\n","plt.xticks(fontsize=8)\n","plt.yticks(fontsize=8)\n","\n","plt.xlim(x_min, x_max)\n","plt.ylim(y_min, y_max)\n","\n","plt.xlabel('Componente principal 1')\n","plt.ylabel('Componente principal 2')\n","plt.title('PCA de las features para nuestros modelos', fontsize=10)\n","\n","# Set alpha=1 for the legend only\n","legend = plt.legend(prop={'size': 8})\n","for lh in legend.legend_handles:\n","    lh.set_alpha(1)\n","\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"_YZ3RSR-0HXh"},"source":["<p><em>\n","Podemos observar que las dos componentes principales no son suficientes para esbozar una separación entre las clases de la variable objetivo. Sin embargo, podemos ver que efectivamente ambas clases se agrupan en respectivos clusters aunque estos se encuentren solapados. El reconocimiento de estos clusters sugiere que las variables elegidas poseen cierta habilidad para distinguir entre clases.\n","</em></p>"]},{"cell_type":"markdown","metadata":{"id":"kZuXxL7k0KBA"},"source":["Vamos a analizar ahora la composición del componente principal 1 obtenido."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pTLCIiSS0LxF"},"outputs":[],"source":["# Get the loadings for the first principal component\n","loadings = pca.components_[0]\n","pc1_loadings = pd.DataFrame(loadings, index=X.columns, columns=['PC1 Loading'])\n","\n","# Get the explained variance ratio to understand how much variance each component explains\n","explained_variance = pca.explained_variance_ratio_\n","\n","# Print loadings for the first principal component\n","print(\"Explained Variance by PC1:\", explained_variance[0], \"\\n\")\n","print(pc1_loadings.sort_values(by='PC1 Loading', ascending=False).head(10))"]},{"cell_type":"markdown","metadata":{"id":"kNeZtvud0P_F"},"source":["<p><em>\n","Analizar la composición de los componentes principales puede resultar útil para entender el aporte de cada una de las variables de entrada a la varianza del dataset. Sin embargo, en este caso, el componente principal 1 explica sólo el 24% de la varianza total por lo que el análisis resulta insuficiente para entender el aporte de las features a la estructura del dataset.\n","</em></p>"]},{"cell_type":"markdown","metadata":{"id":"gq1zPv900SkX"},"source":["Para realizar una evaluación más precisa de la importancia de las variables del dataset en la tarea de predecir lluvia para el día siguiente, entrenamos un modelo de random forest y extraemos del mismo la contribución de cada una de las variables a la reducción de la impureza Gini. Para el entrenamiento del modelo no se dividirán los datos en entrenamiento y testeo, ya que no nos interesa utilizarlo para realizar inferencias."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gG6o9fEr0hTH"},"outputs":[],"source":["# Check importance of each feature.\n","rf_importance = RandomForestClassifier(random_state=42)\n","rf_importance.fit(X, y)\n","\n","feature_importances = rf_importance.feature_importances_\n","rf_feature_importances = pd.DataFrame(feature_importances, index=X.columns, columns=['Feature Importance'])\n","\n","print(rf_feature_importances.sort_values(by='Feature Importance', ascending=False))"]},{"cell_type":"markdown","metadata":{"id":"pBMmWyJb6ygF"},"source":["<p><em>\n","Todas las variables oscilan en un rango de importancia comprendida entre 0.16 y 0.015, teniendo la mayoría de ellas valores cercanos al umbral mínimo mencionado.\n","\n","El análisis nos permite distinguir dos cuestiones de importancia:\n","\n","En primer lugar que no encontramos variables con una importancia ínfima que sugiera su descarte, es decir, en mayor o menor medida todas las features resultan informativas para la tarea que llevamos a cabo.\n","\n","En segundo lugar, la medida de Humidity3pm y de Sunshine se destacan por su importancia relevada respecto al resto de las variables. Esta situación verifica nuestra hipótesis de la importancia de ambas variables cuando analizamos el pairplot entre todas las variables numéricas.\n","</em></p>"]},{"cell_type":"markdown","metadata":{"id":"I4Qw9XwT6ygG"},"source":["<!-- Normalization -->"]},{"cell_type":"markdown","metadata":{"id":"p2kkhkB16ygG"},"source":["### 2.3. Ingeniería de modelos de aprendizaje automático (ML Model Engineering)"]},{"cell_type":"markdown","metadata":{"id":"1MPvwIOikg1g"},"source":["#### 2.3.1. Modelado"]},{"cell_type":"markdown","metadata":{"id":"EMOYy0aMkn3R"},"source":["##### 2.3.1.1 Estudio de la literatura y modelo base"]},{"cell_type":"markdown","metadata":{"id":"faM52OHSk12f"},"source":["A modo de baseline inicial para nuestro trabajo realizamos un modelo simple que predice lluvia para el día siguiente si el día de la observación también llovió, y evaluamos los resultados obtenidos."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Cv0LBGxTk4NK"},"outputs":[],"source":["filt = (df[\"RainToday\"].isna()) | (df[\"RainTomorrow\"].isna())\n","\n","evaluate_predictions(df.loc[~filt, \"RainTomorrow\"], df.loc[~filt, \"RainToday\"])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fCH0G9eRnBFd"},"outputs":[],"source":["confusion_matrix = pd.crosstab(df[\"RainToday\"], df[\"RainTomorrow\"])\n","phi, p = phi_coefficient(confusion_matrix)\n","\n","print(f\"Phi Coefficient: {phi}\")\n","print(f\"p-value: {p}\")"]},{"cell_type":"markdown","metadata":{"id":"eRytMx5kn7Zd"},"source":["<p><em>\n","Observamos que el valor de accuracy se encuentra inflado por el desbalanceo en el dataset. De hecho, si predijeramos siempre ausencia de lluvia obtendríamos un valor de 0.78 en esa métrica. Claro que en ese caso obtendríamos un valor de 0 para precision y recall.\n","\n","Con este modelo inicial planteado obtenemos un F1 score de 0.47, con valores para precision y recall muy similares.\n","\n","Calculando el coeficiente Phi entre ambas variables binarias encontramos un valor de 0.3 (asociación entre variables moderada y positiva) con una significancia que nos permite rechazar la hipóteses de que la asociación encontrada puede deberse simplemente a la variabilidad de el muestreo aleatorio.</em></p>"]},{"cell_type":"markdown","metadata":{"id":"yea-kJDv6ygH"},"source":["### 2.4 Evaluación de modelos de aprendizaje automático (ML Model Evaluation)"]},{"cell_type":"markdown","metadata":{"id":"UiKYebxUAKqp"},"source":["🔮 Futuras versiones 🔮"]},{"cell_type":"markdown","metadata":{"id":"aA8GlTHC6ygI"},"source":["### 2.5 Despliegue del modelo (Model Deployment)"]},{"cell_type":"markdown","metadata":{"id":"AZ3j_IftAKqp"},"source":["🔮 Futuras versiones 🔮"]},{"cell_type":"markdown","metadata":{"id":"mb63TL_06ygI"},"source":["### 2.6 Monitoreo y mantenimiento del modelo (Model Monitoring and Maintenance)"]},{"cell_type":"markdown","metadata":{"id":"Fsw3YH8nAKqp"},"source":["🔮 Futuras versiones 🔮"]},{"cell_type":"markdown","metadata":{"id":"SBHtocWZ6ygK"},"source":["## 3. Mejora continua"]},{"cell_type":"markdown","metadata":{"id":"fC19d38ZAKqp"},"source":["> Mejoras planteadas en un futuro:\n","> - Fases que faltaron implementar o mejorar\n","> - Mejorar los procesos\n","> - Mejorar la documentación de los procesos\n","> - Mejorar las referencias\n","> - Mejorar el archivo de código auxiliar (principalmente las improtaciones)\n","> - Refactorizar el código actual en código auxiliar"]},{"cell_type":"markdown","metadata":{"id":"s9tJtWPx6ygL"},"source":["## 4. Referencias"]},{"cell_type":"markdown","metadata":{"id":"pD9wt3NJAKqp"},"source":["- https://en.wikipedia.org/wiki/List_of_extreme_temperatures_in_Australia\n","- https://ml-ops.org/content/crisp-ml\n","- https://www.researchgate.net/publication/369194767_A_Different_Traditional_Approach_for_Automatic_Comparative_Machine_Learning_in_Multimodality_Covid-19_Severity_Recognition\n","- https://arxiv.org/pdf/2003.05155.pdf"]},{"cell_type":"markdown","metadata":{"id":"oJaF_tor6ygL"},"source":["## 5. Apendices"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"a-j8_w-wAKqp"},"outputs":[],"source":["#with open(\"utils.py\") as f:\n","#  print(f.read())"]}],"metadata":{"colab":{"gpuType":"T4","provenance":[{"file_id":"1MRvf7L5Vyu8qoIqrYZLfngQfRi_Qt4Jm","timestamp":1718154740692}]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.3"}},"nbformat":4,"nbformat_minor":0}
