{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# <div align=\"center\"><b> Desafio 4 - Procesamiento del lenguaje Natural - CEIA </b></div>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<div align=\"right\">üìù <em><small><font color='Gray'>Nota:</font></small></em></div>\n",
        "\n",
        "<div align=\"right\"> <em><small><font color='Gray'> La funcionalidad de visualizaci√≥n de jupyter notebooks en <a href=\"https://github.com/\" target=\"_blank\">github</a> es solamente un preview.</font></small></em> </div>\n",
        "\n",
        "<div align=\"right\"> <em><small><font color='Gray'> Para mejor visualizaci√≥n se sugiere utilizar el visualizador recomndado por la comunidad: <a href=\"https://nbviewer.org/\" target=\"_blank\">nbviewer</a></font></small></em> </div>\n",
        "\n",
        "<div align=\"right\"> <em><small><font color='Gray'> Puedes a acceder al sigiente enlace para ver este notebook en dicha p√°gina: <a href=\"https://nbviewer.org/github/brunomaso1/uba-ceia/blob/ceia-nlp/ceia-nlp/Desafio%204.ipynb\">Desafio 4</a></font></small></em> </div>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Descargamos la carpeta con archivos auxiliares (Colab)\n",
        "# %pip install gdown\n",
        "# !gdown https://drive.google.com/drive/folders/1hNPqr6g3opu9u-UwnngVcyxUUzfmKZdT?usp=sharing --folder"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<style>\n",
        "/* Limitar la altura de las celdas de salida en html */\n",
        ".jp-OutputArea.jp-Cell-outputArea {\n",
        "    max-height: 500px;\n",
        "}\n",
        "</style>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<div align=\"center\"><img src=\"./resources/Desafio_4_portada.jpeg\" width=\"600\" alt=\"Figura 1: A data scientist is sitting in front of a computer screen, which is the focal point of the image. The lighting is dark and moody, with a blue hue. The computer screen displays a visualization of a QA bot. The bot and the scientist are chatting with each other, with the bot's responses appearing on the screen. - Generada con Microsoft Image generator\"></div>\n",
        "\n",
        "<div align=\"center\"><small><em>Figura 1: A data scientist is sitting in front of a computer screen, which is the focal point of the image. The lighting is dark and moody, with a blue hue. The computer screen displays a visualization of a QA bot. The bot and the scientist are chatting with each other, with the bot's responses appearing on the screen. - Generada con Microsoft Image generator</em></small></div>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<div align=\"center\">‚ú®Datos del proyecto:‚ú®</div>\n",
        "\n",
        "<p></p>\n",
        "\n",
        "<div align=\"center\">\n",
        "\n",
        "| Subtitulo       | Desaf√≠o 4 - NLP - FIUBA               |\n",
        "| --------------- | ------------------------------------- |\n",
        "| **Descrpci√≥n**  | BOT QA                                |\n",
        "| **Integrantes** | Bruno Masoller (brunomaso1@gmail.com) |\n",
        "\n",
        "</div>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "‚úã <em><font color='DodgerBlue'>Importaciones:</font></em> ‚úã\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "id": "l7cXR6CI30ry"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import urllib.request\n",
        "import json\n",
        "import re\n",
        "from IPython.display import display, HTML\n",
        "import pandas as pd\n",
        "import random\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "üîß <em><font color='tomato'>Configuraciones:</font></em> üîß\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {},
      "outputs": [],
      "source": [
        "RANDOM_SEED = 42\n",
        "DATA_URL = \"http://convai.io/data/data_volunteers.json\"\n",
        "LOCAL_FILENAME = \"./resources/data_volunteers.json\"\n",
        "LABEL_SOS = \"<sos> \"\n",
        "LABEL_EOS = \" <eos>\"\n",
        "FILTERS = '!\"#$%&()*+,-./:;=¬ø?@[\\\\]^_`{|}~\\t\\n'\n",
        "\n",
        "random.seed(RANDOM_SEED)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "McArD4rSDR2K"
      },
      "source": [
        "## Consigna del desaf√≠o\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "El objetivo es utilizar datos disponibles del challenge [ConvAI2](http://convai.io/data/) (Conversational Intelligence Challenge 2) de conversaciones en ingl√©s, para convertirlo en un BOT QA para responder preguntas del usuario."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Parte 1\n",
        "\n",
        "- Descargar el conjunto y pre-procesarlo."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Parte 2 \n",
        "\n",
        "- Realizar el preprocesamiento necesario para obtener:\n",
        "    - `word2idx_inputs`, `max_input_len`\n",
        "    - `word2idx_outputs`, `max_out_len`, `num_words_output`\n",
        "    - `encoder_input_sequences`, `decoder_output_sequences`, `decoder_targets`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Parte 3\n",
        "\n",
        "- Utilizar los embeddings de *Glove* o *FastText* para transformar los tokens de entrada en vectores."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Parte 4\n",
        "\n",
        "- Entrenar un modelo basado en el esquema *encoder-decoder* utilizando los datos generados en los puntos anteriores."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Parte 5\n",
        "\n",
        "- Experimentar el modelo en inferencia."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Resoluci√≥n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Parte 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "> - Descargar el conjunto y pre-procesarlo."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "*Cargamos el conjunto:*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "El archivo ya existe localmente.\n",
            "Archivo cargado correctamente.\n"
          ]
        }
      ],
      "source": [
        "try:\n",
        "    # Verificar si el archivo ya est√° descargado\n",
        "    if not os.path.isfile(LOCAL_FILENAME):\n",
        "        print(f\"Archivo no encontrado localmente. Descargando desde {DATA_URL}...\")\n",
        "        urllib.request.urlretrieve(DATA_URL, LOCAL_FILENAME)\n",
        "        print(\"Descarga completada.\")\n",
        "    else:\n",
        "        print(\"El archivo ya existe localmente.\")\n",
        "\n",
        "    # Cargar el archivo JSON en la variable 'data'\n",
        "    with open(LOCAL_FILENAME, \"r\", encoding=\"utf-8\") as f:\n",
        "        data = json.load(f)\n",
        "    print(\"Archivo cargado correctamente.\")\n",
        "except urllib.error.URLError as e:\n",
        "    print(f\"Error al descargar el archivo: {e}\")\n",
        "except FileNotFoundError as e:\n",
        "    print(f\"Error al abrir el archivo: {e}\")\n",
        "except json.JSONDecodeError as e:\n",
        "    print(f\"Error al decodificar el archivo JSON: {e}\")\n",
        "except Exception as e:\n",
        "    print(f\"Se produjo un error inesperado: {e}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "*Mostramos los datos descargados:*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "üîÆ <em><font color='violet'>Funci√≥n auxiliar:</font></em>\n",
        "<em><font color='violet'><p>Funci√≥n que limpia los textos..</p></font></em>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {},
      "outputs": [],
      "source": [
        "def clean_text(txt, contractions):\n",
        "    # Convertimos todo el texto a min√∫sculas\n",
        "    txt = txt.lower()\n",
        "    \n",
        "    # Reemplazamos contracciones comunes    \n",
        "    for contraction, full_form in contractions.items():\n",
        "        txt = txt.replace(contraction, full_form)\n",
        "    \n",
        "    # Eliminamos cualquier car√°cter no alfanum√©rico (y sustituimos por un espacio)\n",
        "    txt = re.sub(r'\\W+', ' ', txt)\n",
        "    \n",
        "    return txt.strip()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "*Definimos el contexto m√°ximo:*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {},
      "outputs": [],
      "source": [
        "MAX_LEN = 10"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "*Definimos las abrebiaciones identificadas:*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {},
      "outputs": [],
      "source": [
        "CONTRACTIONS = {\"'d\": \" had\", \"'s\": \" is\", \"'m\": \" am\", \"don't\": \"do not\"}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "*Pre-procesamos el texto en sentencias:*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cantidad de rows utilizadas: 9092\n"
          ]
        }
      ],
      "source": [
        "input_sentences = []\n",
        "output_sentences = []\n",
        "output_sentences_inputs = []\n",
        "\n",
        "for line in data:\n",
        "    dialog = line[\"dialog\"]\n",
        "    # Recorremos los di√°logos pero limitamos a los pares de (input, respuesta)\n",
        "    for i in range(len(dialog) - 1):\n",
        "        # Limpiamos el texto de la \"pregunta\" y la \"respuesta\"\n",
        "        chat_in = clean_text(dialog[i][\"text\"], CONTRACTIONS)\n",
        "        chat_out = clean_text(dialog[i + 1][\"text\"], CONTRACTIONS)\n",
        "\n",
        "        # Filtramos si alguna oraci√≥n supera la longitud m√°xima permitida\n",
        "        if len(chat_in.split()) >= MAX_LEN or len(chat_out.split()) >= MAX_LEN:\n",
        "            continue\n",
        "\n",
        "        # Agregamos las etiquetas <eos> y <sos> a las frases de salida\n",
        "        output_sentence = chat_out + LABEL_EOS\n",
        "        output_sentence_input = LABEL_SOS + chat_out\n",
        "\n",
        "        # A√±adimos las oraciones limpias a sus respectivas listas\n",
        "        input_sentences.append(chat_in)\n",
        "        output_sentences.append(output_sentence)\n",
        "        output_sentences_inputs.append(output_sentence_input)\n",
        "\n",
        "# Mostramos la cantidad de oraciones procesadas\n",
        "print(\"Cantidad de rows utilizadas:\", len(input_sentences))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "*Chequeamos algunas sentencias:*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Indice</th>\n",
              "      <th>Input Sentence</th>\n",
              "      <th>Output Sentence</th>\n",
              "      <th>Output Sentence Input</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1824</td>\n",
              "      <td>jonny depp</td>\n",
              "      <td>i know right i know right &lt;eos&gt;</td>\n",
              "      <td>&lt;sos&gt; i know right i know right</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>409</td>\n",
              "      <td>i am a pa how about you</td>\n",
              "      <td>i am a doctor and looking for new opportunities &lt;eos&gt;</td>\n",
              "      <td>&lt;sos&gt; i am a doctor and looking for new opportunities</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>4506</td>\n",
              "      <td>yes i have a lot of friends</td>\n",
              "      <td>how do they taste &lt;eos&gt;</td>\n",
              "      <td>&lt;sos&gt; how do they taste</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4012</td>\n",
              "      <td>hyy</td>\n",
              "      <td>begin &lt;eos&gt;</td>\n",
              "      <td>&lt;sos&gt; begin</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>3657</td>\n",
              "      <td>what do you do for a living</td>\n",
              "      <td>job &lt;eos&gt;</td>\n",
              "      <td>&lt;sos&gt; job</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>2286</td>\n",
              "      <td>so you are divorced</td>\n",
              "      <td>yes i am i am a little overweight &lt;eos&gt;</td>\n",
              "      <td>&lt;sos&gt; yes i am i am a little overweight</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>1679</td>\n",
              "      <td>i like to read</td>\n",
              "      <td>why &lt;eos&gt;</td>\n",
              "      <td>&lt;sos&gt; why</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>8935</td>\n",
              "      <td>i am a student what do you do</td>\n",
              "      <td>i am aslo a student &lt;eos&gt;</td>\n",
              "      <td>&lt;sos&gt; i am aslo a student</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>1424</td>\n",
              "      <td>why</td>\n",
              "      <td>i love to read &lt;eos&gt;</td>\n",
              "      <td>&lt;sos&gt; i love to read</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>6912</td>\n",
              "      <td>yes</td>\n",
              "      <td>what do you do for a living &lt;eos&gt;</td>\n",
              "      <td>&lt;sos&gt; what do you do for a living</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Seleccionamos √≠ndices aleatorios\n",
        "random_indices = random.sample(range(len(input_sentences)), 10)\n",
        "\n",
        "# Creamos una lista para almacenar los datos\n",
        "df_data = []\n",
        "\n",
        "# Recorremos los √≠ndices aleatorios y creamos un diccionario para cada uno\n",
        "for idx in random_indices:\n",
        "    df_row = {\n",
        "        \"Indice\": idx,\n",
        "        \"Input Sentence\": input_sentences[idx],\n",
        "        \"Output Sentence\": output_sentences[idx],\n",
        "        \"Output Sentence Input\": output_sentences_inputs[idx]\n",
        "    }\n",
        "    df_data.append(df_row)\n",
        "\n",
        "# Creamos el DataFrame con los datos\n",
        "df = pd.DataFrame(df_data)\n",
        "\n",
        "# Mostramos el DataFrame\n",
        "display(HTML(df.to_html()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Parte 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "> - Realizar el preprocesamiento necesario para obtener:  \n",
        "    - `word2idx_inputs`, `max_input_len`  \n",
        "    - `word2idx_outputs`, `max_out_len`, `num_words_output`  \n",
        "    - `encoder_input_sequences`, `decoder_output_sequences`, `decoder_targets`  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "*Primeramente, definimos el vocabulario m√°ximo:*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {},
      "outputs": [],
      "source": [
        "MAX_VOCAB_SIZE = 8000"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "*Luego, tokenizamos las palabras de entrada:*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Palabras en el vocabulario: 2724\n",
            "Sentencia de entrada m√°s larga: 9\n"
          ]
        }
      ],
      "source": [
        "input_tokenizer = Tokenizer(num_words=MAX_VOCAB_SIZE)\n",
        "input_tokenizer.fit_on_texts(input_sentences)\n",
        "input_integer_seq = input_tokenizer.texts_to_sequences(input_sentences)\n",
        "\n",
        "word2idx_inputs = input_tokenizer.word_index\n",
        "print(\"Palabras en el vocabulario:\", len(word2idx_inputs))\n",
        "\n",
        "max_input_len = max(len(sen) for sen in input_integer_seq)\n",
        "print(\"Sentencia de entrada m√°s larga:\", max_input_len)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "*Y tokenizamos las palabras de salidas (target):*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Palabras en el vocabulario: 2737\n",
            "Sentencia de salida m√°s larga: 10\n"
          ]
        }
      ],
      "source": [
        "output_tokenizer = Tokenizer(num_words=MAX_VOCAB_SIZE, filters=FILTERS)\n",
        "output_tokenizer.fit_on_texts([str(LABEL_SOS.strip()), str(LABEL_EOS.strip)] + output_sentences)\n",
        "output_integer_seq = output_tokenizer.texts_to_sequences(output_sentences)\n",
        "output_input_integer_seq = output_tokenizer.texts_to_sequences(output_sentences_inputs)\n",
        "\n",
        "word2idx_outputs = output_tokenizer.word_index\n",
        "print(\"Palabras en el vocabulario:\", len(word2idx_outputs))\n",
        "\n",
        "# Se suma 1 para incluir el token de palabra desconocida\n",
        "num_words_output = min(len(word2idx_outputs) + 1, MAX_VOCAB_SIZE) \n",
        "\n",
        "max_out_len = max(len(sen) for sen in output_integer_seq)\n",
        "print(\"Sentencia de salida m√°s larga:\", max_out_len)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
