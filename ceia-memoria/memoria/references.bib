@article{ARTICLE:1,
  author  = {John Doe},
  title   = {Title},
  journal = {Journal},
  year    = {2017}
}

@book{BOOK:1,
  author    = {John Doe},
  title     = {The Book without Title},
  publisher = {Dummy Publisher},
  year      = {2100}
}

@inbook{BOOK:2,
  author    = {John Doe},
  title     = {The Book without Title},
  publisher = {Dummy Publisher},
  year      = {2100},
  pages     = {100-200}
}

@misc{WEBSITE:1,
  howpublished = {\url{http://example.com}},
  author       = {Intel},
  title        = {Example Website},
  month        = {12},
  year         = {1988},
  urldate      = {2012-11-26}
}

@manual{dallas:ds18b20,
  organization = {Dallas Semiconductor},
  title        = {DS18B20 Programmable Resolution 1-Wire Digital Thermometer},
  number       = {050400},
  year         = 2015,
  note         = {Rev. 3}
}

@book{teton2003,
  title     = {Gu{\'\i}a t{\'e}cnica de la acuariofilia},
  author    = {Teton, J.},
  isbn      = {9788489840249},
  series    = {Gu{\'\i}as t{\'e}cnicas},
  url       = {https://books.google.com.ar/books?id=6vrjovWkQicC},
  year      = {2003},
  publisher = {Tursen S.A. - H. Blume}
}

@book{IEEE:citation,
  author    = {IEEE},
  title     = {IEEE Citation Reference},
  edition   = {1},
  url       = {http://www.ieee.org/documents/ieeecitationref.pdf},
  urldate   = {2016-9-26},
  publisher = {IEEE Publications},
  year      = {2016}
}

@misc{CIAA,
  author = {{Proyecto CIAA}},
  title  = {Computadora Industrial Abierta Argentina},
  url    = {http://proyecto-ciaa.com.ar/devwiki/doku.php?id=start},
  note   = {Visitado el 2016-06-25},
  year   = {2014}
}

@misc{lpcopen,
  author = {{NXP Semiconductors}},
  title  = {LPCOPEN v2.16 Drivers, Middleware and Examples},
  url    = {http://www.nxp.com/products/microcontrollers-and-processors/arm-processors/lpc-cortex-m-mcus/software-tools/lpcopen-libraries-and-examples:LPC-OPEN-LIBRARIES},
  note   = {Disponible: 2016-06-25}
}
 
@misc{contrib,
  author = {{Savannah}},
  title  = {lwIP - A Lightweight TCP/IP stack},
  url    = {http://git.savannah.gnu.org/cgit/lwip/lwip-contrib.git},
  note   = {Disponible: 2016-06-25}
}

@misc{webserver,
  author = {{Savannah}},
  title  = {lwIP - Wiki - Sample Web Server},
  url    = { http://lwip.wikia.com/wiki/Sample_Web_Server},
  note   = {Disponible: 2016-06-25}
} 
 


@misc{sensor_pH,
  author = {dfRobot},
  title  = {PH meter(SKU: SEN0161)},
  url    = {http://www.dfrobot.com/wiki/index.php?title=PH_meter(SKU:_SEN0161)},
  note   = {Disponible: 2016-06-25}
}

@misc{rfc2616,
  series       = {Request for Comments},
  number       = 2616,
  howpublished = {RFC 2616},
  publisher    = {RFC Editor},
  doi          = {10.17487/rfc2616},
  url          = {https://rfc-editor.org/rfc/rfc2616.txt},
  author       = {Jeffrey Mogul and Larry M Masinter and Roy T. Fielding and Jim Gettys and Paul J. Leach and Tim Berners-Lee},
  title        = {{Hypertext Transfer Protocol -- HTTP/1.1}},
  pagetotal    = 176,
  year         = 2013,
  month        = 3,
  day          = 2,
  abstract     = {HTTP has been in use by the World-Wide Web global information initiative since 1990. This specification defines the protocol referred to as &quot;HTTP/1.1&quot;, and is an update to RFC 2068. [STANDARDS-TRACK]}
}

@misc{rfc3875,
  series       = {Request for Comments},
  number       = 3875,
  howpublished = {RFC 3875},
  publisher    = {RFC Editor},
  doi          = {10.17487/rfc3875},
  url          = {https://rfc-editor.org/rfc/rfc3875.txt},
  author       = {David Robinson},
  title        = {{The Common Gateway Interface (CGI) Version 1.1}},
  pagetotal    = 36,
  year         = 2013,
  month        = mar,
  day          = 20,
  abstract     = {The Common Gateway Interface (CGI) is a simple interface for running external programs, software or gateways under an information server in a platform-independent manner. Currently, the supported information servers are HTTP servers.}
}

@misc{rfc7540,
  series       = {Request for Comments},
  number       = 7540,
  howpublished = {RFC 7540},
  publisher    = {RFC Editor},
  doi          = {10.17487/rfc7540},
  url          = {https://rfc-editor.org/rfc/rfc7540.txt},
  author       = {Mike Belshe and Roberto Peon and Martin Thomson},
  title        = {{Hypertext Transfer Protocol Version 2 (HTTP/2)}},
  pagetotal    = 96,
  year         = 2015,
  month        = nov,
  day          = 18,
  abstract     = {This specification describes an optimized expression of the semantics of the Hypertext Transfer Protocol (HTTP), referred to as HTTP version 2 (HTTP/2). HTTP/2 enables a more efficient use of network resources and a reduced perception of latency by introducing header field compression and allowing multiple concurrent exchanges on the same connection. It also introduces unsolicited push of representations from servers to clients.}
}

% --------------------------------------------------------------------------------------------------------------------------------------------------------
% --------------------------------------------------------------------------------------------------------------------------------------------------------

% Refenencias




@misc{bmva_what_2017,
  title  = {What is computer vision?},
  url    = {https://web.archive.org/web/20170216180225/http://www.bmva.org/visionoverview},
  author = {{BMVA}},
  year   = {2017},
  annote = {Visitado el 2025-03-21}
}

@book{torralba_foundations_2024,
  series    = {Adaptive {Computation} and {Machine} {Learning} series},
  title     = {Foundations of {Computer} {Vision}},
  isbn      = {978-0-262-37866-6},
  url       = {https://mitpress.mit.edu/9780262048972/foundations-of-computer-vision/},
  publisher = {MIT Press},
  author    = {Torralba, A. and Isola, P. and Freeman, W.T.},
  year      = {2024},
  lccn      = {2023024589},
  annote    = {Chapter 1.2 - Vision
               }
}

@misc{dong_applications_2024,
  title      = {Applications of {Computer} {Vision} in {Autonomous} {Vehicles}: {Methods}, {Challenges} and {Future} {Directions}},
  shorttitle = {Applications of {Computer} {Vision} in {Autonomous} {Vehicles}},
  url        = {http://arxiv.org/abs/2311.09093},
  doi        = {10.48550/arXiv.2311.09093},
  abstract   = {Autonomous vehicle refers to a vehicle capable of perceiving its surrounding environment and driving with little or no human driver input. The perception system is a fundamental component which enables the autonomous vehicle to collect data and extract relevant information from the environment to drive safely. Benefit from the recent advances in computer vision, the perception task can be achieved by using sensors, such as camera, LiDAR, radar, and ultrasonic sensor. This paper reviews publications on computer vision and autonomous driving that are published during the last ten years. In particular, we first investigate the development of autonomous driving systems and summarize these systems that are developed by the major automotive manufacturers from different countries. Second, we investigate the sensors and benchmark data sets that are commonly utilized for autonomous driving. Then, a comprehensive overview of computer vision applications for autonomous driving such as depth estimation, object detection, lane detection, and traffic sign recognition are discussed. Additionally, we review public opinions and concerns on autonomous vehicles. Based on the discussion, we analyze the current technological challenges that autonomous vehicles meet with. Finally, we present our insights and point out some promising directions for future research. This paper will help the reader to understand autonomous vehicles from the perspectives of academia and industry.},
  urldate    = {2025-03-21},
  publisher  = {arXiv},
  author     = {Dong, Xingshuai and Cappuccio, Massimiliano L.},
  month      = jun,
  year       = {2024},
  note       = {arXiv:2311.09093 [cs]},
  keywords   = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Robotics},
  file       = {Dong y Cappuccio - 2024 - Applications of Computer Vision in Autonomous Vehicles Methods, Challenges and Future Directions.pdf:G\:\\Mi unidad\\Libros\\Papers\\Dong y Cappuccio - 2024 - Applications of Computer Vision in Autonomous Vehicles Methods, Challenges and Future Directions.pdf:application/pdf}
}

@article{sutar_convolutional_2025,
  title      = {Convolutional {Neural} {Networks} ({CNNs}): {Advancements} and {Future} {Trends}},
  shorttitle = {Convolutional {Neural} {Networks} ({CNNs})},
  url        = {https://www.academia.edu/128256115/Convolutional_Neural_Networks_CNNs_Advancements_and_Future_Trends},
  abstract   = {Convolutional Neural Networks (CNNs) have revolutionized computer vision and pattern recognition by efficiently analyzing visual data. With increasing computational power and improved architectures, CNNs are evolving rapidly. This paper explores the},
  urldate    = {2025-03-21},
  journal    = {Convolutional Neural Networks (CNNs): Advancements and Future Trends},
  author     = {Sutar, Manav},
  month      = jan,
  year       = {2025}
}

@article{krizhevsky_imagenet_2017,
  title    = {{ImageNet} classification with deep convolutional neural networks},
  volume   = {60},
  issn     = {0001-0782, 1557-7317},
  url      = {https://dl.acm.org/doi/10.1145/3065386},
  doi      = {10.1145/3065386},
  abstract = {We trained a large, deep convolutional neural network to classify the 1.2 million high-resolution images in the ImageNet LSVRC-2010 contest into the 1000 different classes. On the test data, we achieved top-1 and top-5 error rates of 37.5\% and 17.0\% which is considerably better than the previous state-of-the-art. The neural network, which has 60 million parameters and 650,000 neurons, consists of ﬁve convolutional layers, some of which are followed by max-pooling layers, and three fully-connected layers with a ﬁnal 1000-way softmax. To make training faster, we used non-saturating neurons and a very efﬁcient GPU implementation of the convolution operation. To reduce overﬁtting in the fully-connected layers we employed a recently-developed regularization method called “dropout” that proved to be very effective. We also entered a variant of this model in the ILSVRC-2012 competition and achieved a winning top-5 test error rate of 15.3\%, compared to 26.2\% achieved by the second-best entry.},
  language = {en},
  number   = {6},
  urldate  = {2025-03-21},
  journal  = {Communications of the ACM},
  author   = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E.},
  month    = may,
  year     = {2017},
  pages    = {84--90},
  file     = {Krizhevsky et al. - 2017 - ImageNet classification with deep convolutional neural networks.pdf:G\:\\Mi unidad\\Libros\\Papers\\Krizhevsky et al. - 2017 - ImageNet classification with deep convolutional neural networks.pdf:application/pdf}
}

@misc{redmon_you_2016,
  title      = {You {Only} {Look} {Once}: {Unified}, {Real}-{Time} {Object} {Detection}},
  shorttitle = {You {Only} {Look} {Once}},
  url        = {http://arxiv.org/abs/1506.02640},
  doi        = {10.48550/arXiv.1506.02640},
  abstract   = {We present YOLO, a new approach to object detection. Prior work on object detection repurposes classifiers to perform detection. Instead, we frame object detection as a regression problem to spatially separated bounding boxes and associated class probabilities. A single neural network predicts bounding boxes and class probabilities directly from full images in one evaluation. Since the whole detection pipeline is a single network, it can be optimized end-to-end directly on detection performance. Our unified architecture is extremely fast. Our base YOLO model processes images in real-time at 45 frames per second. A smaller version of the network, Fast YOLO, processes an astounding 155 frames per second while still achieving double the mAP of other real-time detectors. Compared to state-of-the-art detection systems, YOLO makes more localization errors but is far less likely to predict false detections where nothing exists. Finally, YOLO learns very general representations of objects. It outperforms all other detection methods, including DPM and R-CNN, by a wide margin when generalizing from natural images to artwork on both the Picasso Dataset and the People-Art Dataset.},
  urldate    = {2025-03-21},
  publisher  = {arXiv},
  author     = {Redmon, Joseph and Divvala, Santosh and Girshick, Ross and Farhadi, Ali},
  month      = may,
  year       = {2016},
  note       = {arXiv:1506.02640 [cs]},
  keywords   = {Computer Science - Computer Vision and Pattern Recognition},
  file       = {Redmon et al. - 2016 - You Only Look Once Unified, Real-Time Object Detection.pdf:G\:\\Mi unidad\\Libros\\Papers\\Redmon et al. - 2016 - You Only Look Once Unified, Real-Time Object Detection.pdf:application/pdf}
}

@article{gao_recent_2024,
  title      = {Recent {Advances} in {Computer} {Vision}: {Technologies} and {Applications}},
  volume     = {13},
  copyright  = {http://creativecommons.org/licenses/by/3.0/},
  issn       = {2079-9292},
  shorttitle = {Recent {Advances} in {Computer} {Vision}},
  url        = {https://www.mdpi.com/2079-9292/13/14/2734},
  doi        = {10.3390/electronics13142734},
  abstract   = {Computer vision plays a pivotal role in modern society, which transforms fields such as healthcare, transportation, entertainment, and manufacturing by enabling machines to interpret and understand visual information, revolutionizing industries, and enhancing daily life [...]},
  language   = {en},
  number     = {14},
  urldate    = {2025-03-21},
  journal    = {Electronics},
  author     = {Gao, Mingliang and Zou, Guofeng and Li, Yun and Guo, Xiangyu},
  month      = jan,
  year       = {2024},
  note       = {Number: 14
                Publisher: Multidisciplinary Digital Publishing Institute},
  keywords   = {n/a},
  pages      = {2734},
  file       = {Gao et al. - 2024 - Recent Advances in Computer Vision Technologies and Applications.pdf:G\:\\Mi unidad\\Libros\\Papers\\Gao et al. - 2024 - Recent Advances in Computer Vision Technologies and Applications.pdf:application/pdf}
}

@article{kagan_automatic_2021,
  title    = {Automatic large scale detection of red palm weevil infestation using street view images},
  volume   = {182},
  issn     = {09242716},
  url      = {https://linkinghub.elsevier.com/retrieve/pii/S0924271621002665},
  doi      = {10.1016/j.isprsjprs.2021.10.004},
  language = {en},
  urldate  = {2024-11-21},
  journal  = {ISPRS Journal of Photogrammetry and Remote Sensing},
  author   = {Kagan, Dima and Fuhrmann Alpert, Galit and Fire, Michael},
  month    = dec,
  year     = {2021},
  keywords = {ai-computer-vision, proyecto-picudo-rojo, readed},
  pages    = {122--133},
  file     = {Kagan et al. - 2021 - Automatic large scale detection of red palm weevil infestation using street view images.pdf:G\:\\Mi unidad\\Libros\\Papers\\Kagan et al. - 2021 - Automatic large scale detection of red palm weevil infestation using street view images.pdf:application/pdf}
}

@misc{ren_faster_2016,
  title      = {Faster {R}-{CNN}: {Towards} {Real}-{Time} {Object} {Detection} with {Region} {Proposal} {Networks}},
  shorttitle = {Faster {R}-{CNN}},
  url        = {http://arxiv.org/abs/1506.01497},
  doi        = {10.48550/arXiv.1506.01497},
  abstract   = {State-of-the-art object detection networks depend on region proposal algorithms to hypothesize object locations. Advances like SPPnet and Fast R-CNN have reduced the running time of these detection networks, exposing region proposal computation as a bottleneck. In this work, we introduce a Region Proposal Network (RPN) that shares full-image convolutional features with the detection network, thus enabling nearly cost-free region proposals. An RPN is a fully convolutional network that simultaneously predicts object bounds and objectness scores at each position. The RPN is trained end-to-end to generate high-quality region proposals, which are used by Fast R-CNN for detection. We further merge RPN and Fast R-CNN into a single network by sharing their convolutional features---using the recently popular terminology of neural networks with 'attention' mechanisms, the RPN component tells the unified network where to look. For the very deep VGG-16 model, our detection system has a frame rate of 5fps (including all steps) on a GPU, while achieving state-of-the-art object detection accuracy on PASCAL VOC 2007, 2012, and MS COCO datasets with only 300 proposals per image. In ILSVRC and COCO 2015 competitions, Faster R-CNN and RPN are the foundations of the 1st-place winning entries in several tracks. Code has been made publicly available.},
  urldate    = {2025-03-21},
  publisher  = {arXiv},
  author     = {Ren, Shaoqing and He, Kaiming and Girshick, Ross and Sun, Jian},
  month      = jan,
  year       = {2016},
  note       = {arXiv:1506.01497 [cs]},
  keywords   = {Computer Science - Computer Vision and Pattern Recognition},
  annote     = {Comment: Extended tech report},
  file       = {Ren et al. - 2016 - Faster R-CNN Towards Real-Time Object Detection with Region Proposal Networks.pdf:G\:\\Mi unidad\\Libros\\Papers\\Ren et al. - 2016 - Faster R-CNN Towards Real-Time Object Detection with Region Proposal Networks.pdf:application/pdf}
}

@article{delalieux_red_2023,
  title     = {Red {Palm} {Weevil} {Detection} in {Date} {Palm} {Using} {Temporal} {UAV} {Imagery}},
  volume    = {15},
  copyright = {http://creativecommons.org/licenses/by/3.0/},
  issn      = {2072-4292},
  url       = {https://www.mdpi.com/2072-4292/15/5/1380},
  doi       = {10.3390/rs15051380},
  abstract  = {Red palm weevil (RPW) is widely considered a key pest of palms, creating extensive damages to the date palm trunk that inevitably leads to palm death if no pest eradication is done. This study evaluates the potential of a remote sensing approach for the timely and reliable detection of RPW infestation on the palm canopy. For two consecutive years, an experimental field with infested and control palms was regularly monitored by an Unmanned Aerial Vehicle (UAV) carrying RGB, multispectral, and thermal sensors. Simultaneously, detailed visual observations of the RPW effects on the palms were made to assess the evolution of infestation from the initial stage until palm death. A UAV-based image processing chain for nondestructive RPW detection was built based on segmentation and vegetation index analysis techniques. These algorithms reveal the potential of thermal data to detect RPW infestation. Maximum temperature values and standard deviations within the palm crown revealed a significant (α = 0.05) difference between infested and non-infested palms at a severe infestation stage but before any visual canopy symptoms were noticed. Furthermore, this proof-of-concept study showed that the temporal monitoring of spectral vegetation index values could contribute to the detection of infested palms before canopy symptoms are visible. The seasonal significant (α = 0.05) increase of greenness index values, as observed in non-infested trees, could not be observed in infested palms. These findings are of added value for steering management practices and future related studies, but further validation of the results is needed. The workflow and resulting maps are accessible through the Mapeo® visualization platform.},
  language  = {en},
  number    = {5},
  urldate   = {2025-03-21},
  journal   = {Remote Sensing},
  author    = {Delalieux, Stephanie and Hardy, Tom and Ferry, Michel and Gomez, Susi and Kooistra, Lammert and Culman, Maria and Tits, Laurent},
  month     = jan,
  year      = {2023},
  note      = {Number: 5
               Publisher: Multidisciplinary Digital Publishing Institute},
  keywords  = {date palm, decision tree, photogrammetry, red palm weevil, remote sensing, segmentation, UAV, vegetation index},
  pages     = {1380},
  file      = {Delalieux et al. - 2023 - Red Palm Weevil Detection in Date Palm Using Temporal UAV Imagery 1.pdf:G\:\\Mi unidad\\Libros\\Papers\\Delalieux et al. - 2023 - Red Palm Weevil Detection in Date Palm Using Temporal UAV Imagery 1.pdf:application/pdf}
}

@misc{poplin_palm_2014,
  title  = {Palm {Weevils}},
  author = {Poplin, A. and Roda, A. and Bhotika, S. and Sobel, L.},
  month  = may,
  year   = {2014},
  annote = {Obtenido de: https://entnemdept.ufl.edu/hodges/Collaborative/Documents/Palm\_weevils.pdf}
}

@article{mgap_informacion_nodate,
  title   = {Información actualizada sobre el picudo rojo de las palmeras a setiembre 2024},
  url     = {https://www.gub.uy/ministerio-ganaderia-agricultura-pesca/comunicacion/noticias/informacion-actualizada-sobre-picudo-rojo-palmeras-setiembre-2024},
  urldate = {2025-03-21},
  author  = {{MGAP}}
}


@misc{arcos_picudo_2024,
  address = {Montevideo},
  title   = {{PICUDO} {ROJO} ({Rynchophorus} ferrugineus) {EN} {MONTEVIDEO} {MONITOREO} {Y} {CONTROL} ({Avances})},
  urldate = {2025-03-21},
  author  = {Arcos, Alfonso},
  month   = aug,
  year    = {2024}
}

@misc{anticimex_picudo_nodate,
  title   = {Picudo {Rojo} {\textbar} {Daños} y tratamientos - {Anticimex}},
  url     = {https://www.anticimex.es/picudo-rojo/},
  urldate = {2025-03-21},
  journal = {Anticimex},
  author  = {{Anticimex}}
}


@article{zhorif_implementation_2024,
  title    = {Implementation of {Slicing} {Aided} {Hyper} {Inference} ({SAHI}) in {YOLOv8} to {Counting} {Oil} {Palm} {Trees} {Using} {High}-{Resolution} {Aerial} {Imagery} {Data}},
  volume   = {15},
  issn     = {21565570, 2158107X},
  url      = {http://thesai.org/Publications/ViewPaper?Volume=15&Issue=7&Code=ijacsa&SerialNo=86},
  doi      = {10.14569/IJACSA.2024.0150786},
  language = {en},
  number   = {7},
  urldate  = {2025-03-22},
  journal  = {International Journal of Advanced Computer Science and Applications},
  author   = {Zhorif, Naufal Najiv and Anandyto, Rahmat Kenzie and Rusyadi, Albrizy Ullaya and Irwansyah, Edy},
  year     = {2024}
}


@inproceedings{muna_development_2023,
  title     = {Development of {Automatic} {Counting} {System} for {Palm} {Oil} {Tree} {Based} on {Remote} {Sensing} {Imagery}},
  doi       = {10.2991/978-94-6463-086-2_68},
  booktitle = {Proceedings of the {International} {Conference} on {Sustainable} {Environment}, {Agriculture} and {Tourism} ({ICOSEAT} 2022)},
  publisher = {Atlantis Press},
  author    = {Muna, M. S. and Nugroho, A. P. and Syarovy, M. and Wiratmoko, A. and {Suwardi} and Sutiarso, L.},
  month     = jan,
  year      = {2023}
}

@article{wibowo_large-scale_2022,
  title   = {Large-{Scale} {Oil} {Palm} {Trees} {Detection} from {High}-{Resolution} {Remote} {Sensing} {Images} {Using} {Deep} {Learning}},
  volume  = {6},
  doi     = {10.3390/bdcc6030089},
  number  = {3},
  journal = {Big Data and Cognitive Computing},
  author  = {Wibowo, H. and Sitanggang, I. S. and Mushthofa, M. and Adrianto, H. A.},
  month   = sep,
  year    = {2022}
}

@inproceedings{wardana_detection_2023,
  title     = {Detection of {Oil} {Palm} {Trees} {Using} {Deep} {Learning} {Method} with {High}-{Resolution} {Aerial} {Image} {Data}},
  doi       = {10.1145/3626641.3626667},
  booktitle = {{ACM} {International} {Conference} {Proceeding} {Series}},
  publisher = {Association for Computing Machinery},
  author    = {Wardana, D. P. T. and Sianturi, R. S. and Fatwa, R.},
  year      = {2023},
  pages     = {90--98}
}

@misc{prasvita_automatic_nodate,
  title  = {Automatic {Detection} of {Oil} {Palm} {Growth} {Rate} {Status} with {YOLOv5}},
  author = {Prasvita, D. Sandya and Chahyati, D. and Arymurthy, A. M.},
  annote = {[Online]. Available: www.ijacsa.thesai.org}
}

@inproceedings{nuwara_modern_2022,
  title     = {Modern {Computer} {Vision} for {Oil} {Palm} {Tree} {Health} {Surveillance} using {YOLOv5}},
  doi       = {10.1109/GECOST55694.2022.10010668},
  booktitle = {2022 {International} {Conference} on {Green} {Energy}, {Computing} and {Sustainable} {Technology}, {GECOST} 2022},
  publisher = {Institute of Electrical and Electronics Engineers Inc.},
  author    = {Nuwara, Y. and Wong, W. K. and Juwono, F. H.},
  year      = {2022},
  pages     = {404--409}
}

@misc{junos_notitle_nodate,
  author = {Junos, M. H. and Salwa, A. and Khairuddin, M. and Kairi, M. I. and Siran, Y. M.},
  annote = {Organized by the Faculty of Engineering, doi: 10.21467/proceedings.141}
}


@article{ammar_deep-learning-based_2021,
  title     = {Deep-{Learning}-{Based} {Automated} {Palm} {Tree} {Counting} and {Geolocation} in {Large} {Farms} from {Aerial} {Geotagged} {Images}},
  volume    = {11},
  copyright = {https://creativecommons.org/licenses/by/4.0/},
  issn      = {2073-4395},
  url       = {https://www.mdpi.com/2073-4395/11/8/1458},
  doi       = {10.3390/agronomy11081458},
  abstract  = {In this paper, we propose an original deep learning framework for the automated counting and geolocation of palm trees from aerial images using convolutional neural networks. For this purpose, we collected aerial images from two different regions in Saudi Arabia, using two DJI drones, and we built a dataset of around 11,000 instances of palm trees. Then, we applied several recent convolutional neural network models (Faster R-CNN, YOLOv3, YOLOv4, and EfficientDet) to detect palms and other trees, and we conducted a complete comparative evaluation in terms of average precision and inference speed. YOLOv4 and EfficientDet-D5 yielded the best trade-off between accuracy and speed (up to 99\% mean average precision and 7.4 FPS). Furthermore, using the geotagged metadata of aerial images, we used photogrammetry concepts and distance corrections to automatically detect the geographical location of detected palm trees. This geolocation technique was tested on two different types of drones (DJI Mavic Pro and Phantom 4 pro) and was assessed to provide an average geolocation accuracy that attains 1.6 m. This GPS tagging allows us to uniquely identify palm trees and count their number from a series of drone images, while correctly dealing with the issue of image overlapping. Moreover, this innovative combination between deep learning object detection and geolocalization can be generalized to any other objects in UAV images.},
  language  = {en},
  number    = {8},
  urldate   = {2025-03-22},
  journal   = {Agronomy},
  author    = {Ammar, Adel and Koubaa, Anis and Benjdira, Bilel},
  month     = jul,
  year      = {2021},
  pages     = {1458}
}
