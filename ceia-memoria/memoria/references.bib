@article{ARTICLE:1,
  author  = {John Doe},
  title   = {Title},
  journal = {Journal},
  year    = {2017}
}

@book{BOOK:1,
  author    = {John Doe},
  title     = {The Book without Title},
  publisher = {Dummy Publisher},
  year      = {2100}
}

@inbook{BOOK:2,
  author    = {John Doe},
  title     = {The Book without Title},
  publisher = {Dummy Publisher},
  year      = {2100},
  pages     = {100-200}
}

@misc{WEBSITE:1,
  howpublished = {\url{http://example.com}},
  author       = {Intel},
  title        = {Example Website},
  month        = {12},
  year         = {1988},
  urldate      = {2012-11-26}
}

@manual{dallas:ds18b20,
  organization = {Dallas Semiconductor},
  title        = {DS18B20 Programmable Resolution 1-Wire Digital Thermometer},
  number       = {050400},
  year         = 2015,
  note         = {Rev. 3}
}

@book{teton2003,
  title     = {Gu{\'\i}a t{\'e}cnica de la acuariofilia},
  author    = {Teton, J.},
  isbn      = {9788489840249},
  series    = {Gu{\'\i}as t{\'e}cnicas},
  url       = {https://books.google.com.ar/books?id=6vrjovWkQicC},
  year      = {2003},
  publisher = {Tursen S.A. - H. Blume}
}

@book{IEEE:citation,
  author    = {IEEE},
  title     = {IEEE Citation Reference},
  edition   = {1},
  url       = {http://www.ieee.org/documents/ieeecitationref.pdf},
  urldate   = {2016-9-26},
  publisher = {IEEE Publications},
  year      = {2016}
}

@misc{CIAA,
  author = {{Proyecto CIAA}},
  title  = {Computadora Industrial Abierta Argentina},
  url    = {http://proyecto-ciaa.com.ar/devwiki/doku.php?id=start},
  note   = {Visitado el 2016-06-25},
  year   = {2014}
}

@misc{lpcopen,
  author = {{NXP Semiconductors}},
  title  = {LPCOPEN v2.16 Drivers, Middleware and Examples},
  url    = {http://www.nxp.com/products/microcontrollers-and-processors/arm-processors/lpc-cortex-m-mcus/software-tools/lpcopen-libraries-and-examples:LPC-OPEN-LIBRARIES},
  note   = {Disponible: 2016-06-25}
}
 
@misc{contrib,
  author = {{Savannah}},
  title  = {lwIP - A Lightweight TCP/IP stack},
  url    = {http://git.savannah.gnu.org/cgit/lwip/lwip-contrib.git},
  note   = {Disponible: 2016-06-25}
}

@misc{webserver,
  author = {{Savannah}},
  title  = {lwIP - Wiki - Sample Web Server},
  url    = { http://lwip.wikia.com/wiki/Sample_Web_Server},
  note   = {Disponible: 2016-06-25}
} 
 


@misc{sensor_pH,
  author = {dfRobot},
  title  = {PH meter(SKU: SEN0161)},
  url    = {http://www.dfrobot.com/wiki/index.php?title=PH_meter(SKU:_SEN0161)},
  note   = {Disponible: 2016-06-25}
}

@misc{rfc2616,
  series       = {Request for Comments},
  number       = 2616,
  howpublished = {RFC 2616},
  publisher    = {RFC Editor},
  doi          = {10.17487/rfc2616},
  url          = {https://rfc-editor.org/rfc/rfc2616.txt},
  author       = {Jeffrey Mogul and Larry M Masinter and Roy T. Fielding and Jim Gettys and Paul J. Leach and Tim Berners-Lee},
  title        = {{Hypertext Transfer Protocol -- HTTP/1.1}},
  pagetotal    = 176,
  year         = 2013,
  month        = 3,
  day          = 2,
  abstract     = {HTTP has been in use by the World-Wide Web global information initiative since 1990. This specification defines the protocol referred to as &quot;HTTP/1.1&quot;, and is an update to RFC 2068. [STANDARDS-TRACK]}
}

@misc{rfc3875,
  series       = {Request for Comments},
  number       = 3875,
  howpublished = {RFC 3875},
  publisher    = {RFC Editor},
  doi          = {10.17487/rfc3875},
  url          = {https://rfc-editor.org/rfc/rfc3875.txt},
  author       = {David Robinson},
  title        = {{The Common Gateway Interface (CGI) Version 1.1}},
  pagetotal    = 36,
  year         = 2013,
  month        = mar,
  day          = 20,
  abstract     = {The Common Gateway Interface (CGI) is a simple interface for running external programs, software or gateways under an information server in a platform-independent manner. Currently, the supported information servers are HTTP servers.}
}

@misc{rfc7540,
  series       = {Request for Comments},
  number       = 7540,
  howpublished = {RFC 7540},
  publisher    = {RFC Editor},
  doi          = {10.17487/rfc7540},
  url          = {https://rfc-editor.org/rfc/rfc7540.txt},
  author       = {Mike Belshe and Roberto Peon and Martin Thomson},
  title        = {{Hypertext Transfer Protocol Version 2 (HTTP/2)}},
  pagetotal    = 96,
  year         = 2015,
  month        = nov,
  day          = 18,
  abstract     = {This specification describes an optimized expression of the semantics of the Hypertext Transfer Protocol (HTTP), referred to as HTTP version 2 (HTTP/2). HTTP/2 enables a more efficient use of network resources and a reduced perception of latency by introducing header field compression and allowing multiple concurrent exchanges on the same connection. It also introduces unsolicited push of representations from servers to clients.}
}

% --------------------------------------------------------------------------------------------------------------------------------------------------------
% --------------------------------------------------------------------------------------------------------------------------------------------------------

% Refenencias




@misc{bmva_what_2017,
  title  = {What is computer vision?},
  url    = {https://web.archive.org/web/20170216180225/http://www.bmva.org/visionoverview},
  author = {{BMVA}},
  year   = {2017},
  annote = {Visitado el 2025-03-21}
}

@book{torralba_foundations_2024,
  series    = {Adaptive {Computation} and {Machine} {Learning} series},
  title     = {Foundations of {Computer} {Vision}},
  isbn      = {978-0-262-37866-6},
  url       = {https://mitpress.mit.edu/9780262048972/foundations-of-computer-vision/},
  publisher = {MIT Press},
  author    = {Torralba, A. and Isola, P. and Freeman, W.T.},
  year      = {2024},
  lccn      = {2023024589},
  annote    = {Chapter 1.2 - Vision
               }
}

@misc{dong_applications_2024,
  title      = {Applications of {Computer} {Vision} in {Autonomous} {Vehicles}: {Methods}, {Challenges} and {Future} {Directions}},
  shorttitle = {Applications of {Computer} {Vision} in {Autonomous} {Vehicles}},
  url        = {http://arxiv.org/abs/2311.09093},
  doi        = {10.48550/arXiv.2311.09093},
  abstract   = {Autonomous vehicle refers to a vehicle capable of perceiving its surrounding environment and driving with little or no human driver input. The perception system is a fundamental component which enables the autonomous vehicle to collect data and extract relevant information from the environment to drive safely. Benefit from the recent advances in computer vision, the perception task can be achieved by using sensors, such as camera, LiDAR, radar, and ultrasonic sensor. This paper reviews publications on computer vision and autonomous driving that are published during the last ten years. In particular, we first investigate the development of autonomous driving systems and summarize these systems that are developed by the major automotive manufacturers from different countries. Second, we investigate the sensors and benchmark data sets that are commonly utilized for autonomous driving. Then, a comprehensive overview of computer vision applications for autonomous driving such as depth estimation, object detection, lane detection, and traffic sign recognition are discussed. Additionally, we review public opinions and concerns on autonomous vehicles. Based on the discussion, we analyze the current technological challenges that autonomous vehicles meet with. Finally, we present our insights and point out some promising directions for future research. This paper will help the reader to understand autonomous vehicles from the perspectives of academia and industry.},
  urldate    = {2025-03-21},
  publisher  = {arXiv},
  author     = {Dong, Xingshuai and Cappuccio, Massimiliano L.},
  month      = jun,
  year       = {2024},
  note       = {arXiv:2311.09093 [cs]},
  keywords   = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Robotics},
  file       = {Dong y Cappuccio - 2024 - Applications of Computer Vision in Autonomous Vehicles Methods, Challenges and Future Directions.pdf:G\:\\Mi unidad\\Libros\\Papers\\Dong y Cappuccio - 2024 - Applications of Computer Vision in Autonomous Vehicles Methods, Challenges and Future Directions.pdf:application/pdf}
}

@article{sutar_convolutional_2025,
  title      = {Convolutional {Neural} {Networks} ({CNNs}): {Advancements} and {Future} {Trends}},
  shorttitle = {Convolutional {Neural} {Networks} ({CNNs})},
  url        = {https://www.academia.edu/128256115/Convolutional_Neural_Networks_CNNs_Advancements_and_Future_Trends},
  abstract   = {Convolutional Neural Networks (CNNs) have revolutionized computer vision and pattern recognition by efficiently analyzing visual data. With increasing computational power and improved architectures, CNNs are evolving rapidly. This paper explores the},
  urldate    = {2025-03-21},
  journal    = {Convolutional Neural Networks (CNNs): Advancements and Future Trends},
  author     = {Sutar, Manav},
  month      = jan,
  year       = {2025}
}

@article{krizhevsky_imagenet_2017,
  title    = {{ImageNet} classification with deep convolutional neural networks},
  volume   = {60},
  issn     = {0001-0782, 1557-7317},
  url      = {https://dl.acm.org/doi/10.1145/3065386},
  doi      = {10.1145/3065386},
  abstract = {We trained a large, deep convolutional neural network to classify the 1.2 million high-resolution images in the ImageNet LSVRC-2010 contest into the 1000 different classes. On the test data, we achieved top-1 and top-5 error rates of 37.5\% and 17.0\% which is considerably better than the previous state-of-the-art. The neural network, which has 60 million parameters and 650,000 neurons, consists of ﬁve convolutional layers, some of which are followed by max-pooling layers, and three fully-connected layers with a ﬁnal 1000-way softmax. To make training faster, we used non-saturating neurons and a very efﬁcient GPU implementation of the convolution operation. To reduce overﬁtting in the fully-connected layers we employed a recently-developed regularization method called “dropout” that proved to be very effective. We also entered a variant of this model in the ILSVRC-2012 competition and achieved a winning top-5 test error rate of 15.3\%, compared to 26.2\% achieved by the second-best entry.},
  language = {en},
  number   = {6},
  urldate  = {2025-03-21},
  journal  = {Communications of the ACM},
  author   = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E.},
  month    = may,
  year     = {2017},
  pages    = {84--90},
  file     = {Krizhevsky et al. - 2017 - ImageNet classification with deep convolutional neural networks.pdf:G\:\\Mi unidad\\Libros\\Papers\\Krizhevsky et al. - 2017 - ImageNet classification with deep convolutional neural networks.pdf:application/pdf}
}

@misc{redmon_you_2016,
  title      = {You {Only} {Look} {Once}: {Unified}, {Real}-{Time} {Object} {Detection}},
  shorttitle = {You {Only} {Look} {Once}},
  url        = {http://arxiv.org/abs/1506.02640},
  doi        = {10.48550/arXiv.1506.02640},
  abstract   = {We present YOLO, a new approach to object detection. Prior work on object detection repurposes classifiers to perform detection. Instead, we frame object detection as a regression problem to spatially separated bounding boxes and associated class probabilities. A single neural network predicts bounding boxes and class probabilities directly from full images in one evaluation. Since the whole detection pipeline is a single network, it can be optimized end-to-end directly on detection performance. Our unified architecture is extremely fast. Our base YOLO model processes images in real-time at 45 frames per second. A smaller version of the network, Fast YOLO, processes an astounding 155 frames per second while still achieving double the mAP of other real-time detectors. Compared to state-of-the-art detection systems, YOLO makes more localization errors but is far less likely to predict false detections where nothing exists. Finally, YOLO learns very general representations of objects. It outperforms all other detection methods, including DPM and R-CNN, by a wide margin when generalizing from natural images to artwork on both the Picasso Dataset and the People-Art Dataset.},
  urldate    = {2025-03-21},
  publisher  = {arXiv},
  author     = {Redmon, Joseph and Divvala, Santosh and Girshick, Ross and Farhadi, Ali},
  month      = may,
  year       = {2016},
  note       = {arXiv:1506.02640 [cs]},
  keywords   = {Computer Science - Computer Vision and Pattern Recognition},
  file       = {Redmon et al. - 2016 - You Only Look Once Unified, Real-Time Object Detection.pdf:G\:\\Mi unidad\\Libros\\Papers\\Redmon et al. - 2016 - You Only Look Once Unified, Real-Time Object Detection.pdf:application/pdf}
}

@article{gao_recent_2024,
  title      = {Recent {Advances} in {Computer} {Vision}: {Technologies} and {Applications}},
  volume     = {13},
  copyright  = {http://creativecommons.org/licenses/by/3.0/},
  issn       = {2079-9292},
  shorttitle = {Recent {Advances} in {Computer} {Vision}},
  url        = {https://www.mdpi.com/2079-9292/13/14/2734},
  doi        = {10.3390/electronics13142734},
  abstract   = {Computer vision plays a pivotal role in modern society, which transforms fields such as healthcare, transportation, entertainment, and manufacturing by enabling machines to interpret and understand visual information, revolutionizing industries, and enhancing daily life [...]},
  language   = {en},
  number     = {14},
  urldate    = {2025-03-21},
  journal    = {Electronics},
  author     = {Gao, Mingliang and Zou, Guofeng and Li, Yun and Guo, Xiangyu},
  month      = jan,
  year       = {2024},
  note       = {Number: 14
                Publisher: Multidisciplinary Digital Publishing Institute},
  keywords   = {n/a},
  pages      = {2734},
  file       = {Gao et al. - 2024 - Recent Advances in Computer Vision Technologies and Applications.pdf:G\:\\Mi unidad\\Libros\\Papers\\Gao et al. - 2024 - Recent Advances in Computer Vision Technologies and Applications.pdf:application/pdf}
}

@article{kagan_automatic_2021,
  title    = {Automatic large scale detection of red palm weevil infestation using street view images},
  volume   = {182},
  issn     = {09242716},
  url      = {https://linkinghub.elsevier.com/retrieve/pii/S0924271621002665},
  doi      = {10.1016/j.isprsjprs.2021.10.004},
  language = {en},
  urldate  = {2024-11-21},
  journal  = {ISPRS Journal of Photogrammetry and Remote Sensing},
  author   = {Kagan, Dima and Fuhrmann Alpert, Galit and Fire, Michael},
  month    = dec,
  year     = {2021},
  keywords = {ai-computer-vision, proyecto-picudo-rojo, readed},
  pages    = {122--133},
  file     = {Kagan et al. - 2021 - Automatic large scale detection of red palm weevil infestation using street view images.pdf:G\:\\Mi unidad\\Libros\\Papers\\Kagan et al. - 2021 - Automatic large scale detection of red palm weevil infestation using street view images.pdf:application/pdf}
}

@misc{ren_faster_2016,
  title      = {Faster {R}-{CNN}: {Towards} {Real}-{Time} {Object} {Detection} with {Region} {Proposal} {Networks}},
  shorttitle = {Faster {R}-{CNN}},
  url        = {http://arxiv.org/abs/1506.01497},
  doi        = {10.48550/arXiv.1506.01497},
  abstract   = {State-of-the-art object detection networks depend on region proposal algorithms to hypothesize object locations. Advances like SPPnet and Fast R-CNN have reduced the running time of these detection networks, exposing region proposal computation as a bottleneck. In this work, we introduce a Region Proposal Network (RPN) that shares full-image convolutional features with the detection network, thus enabling nearly cost-free region proposals. An RPN is a fully convolutional network that simultaneously predicts object bounds and objectness scores at each position. The RPN is trained end-to-end to generate high-quality region proposals, which are used by Fast R-CNN for detection. We further merge RPN and Fast R-CNN into a single network by sharing their convolutional features---using the recently popular terminology of neural networks with 'attention' mechanisms, the RPN component tells the unified network where to look. For the very deep VGG-16 model, our detection system has a frame rate of 5fps (including all steps) on a GPU, while achieving state-of-the-art object detection accuracy on PASCAL VOC 2007, 2012, and MS COCO datasets with only 300 proposals per image. In ILSVRC and COCO 2015 competitions, Faster R-CNN and RPN are the foundations of the 1st-place winning entries in several tracks. Code has been made publicly available.},
  urldate    = {2025-03-21},
  publisher  = {arXiv},
  author     = {Ren, Shaoqing and He, Kaiming and Girshick, Ross and Sun, Jian},
  month      = jan,
  year       = {2016},
  note       = {arXiv:1506.01497 [cs]},
  keywords   = {Computer Science - Computer Vision and Pattern Recognition},
  annote     = {Comment: Extended tech report},
  file       = {Ren et al. - 2016 - Faster R-CNN Towards Real-Time Object Detection with Region Proposal Networks.pdf:G\:\\Mi unidad\\Libros\\Papers\\Ren et al. - 2016 - Faster R-CNN Towards Real-Time Object Detection with Region Proposal Networks.pdf:application/pdf}
}

@article{delalieux_red_2023,
  title     = {Red {Palm} {Weevil} {Detection} in {Date} {Palm} {Using} {Temporal} {UAV} {Imagery}},
  volume    = {15},
  copyright = {http://creativecommons.org/licenses/by/3.0/},
  issn      = {2072-4292},
  url       = {https://www.mdpi.com/2072-4292/15/5/1380},
  doi       = {10.3390/rs15051380},
  abstract  = {Red palm weevil (RPW) is widely considered a key pest of palms, creating extensive damages to the date palm trunk that inevitably leads to palm death if no pest eradication is done. This study evaluates the potential of a remote sensing approach for the timely and reliable detection of RPW infestation on the palm canopy. For two consecutive years, an experimental field with infested and control palms was regularly monitored by an Unmanned Aerial Vehicle (UAV) carrying RGB, multispectral, and thermal sensors. Simultaneously, detailed visual observations of the RPW effects on the palms were made to assess the evolution of infestation from the initial stage until palm death. A UAV-based image processing chain for nondestructive RPW detection was built based on segmentation and vegetation index analysis techniques. These algorithms reveal the potential of thermal data to detect RPW infestation. Maximum temperature values and standard deviations within the palm crown revealed a significant (α = 0.05) difference between infested and non-infested palms at a severe infestation stage but before any visual canopy symptoms were noticed. Furthermore, this proof-of-concept study showed that the temporal monitoring of spectral vegetation index values could contribute to the detection of infested palms before canopy symptoms are visible. The seasonal significant (α = 0.05) increase of greenness index values, as observed in non-infested trees, could not be observed in infested palms. These findings are of added value for steering management practices and future related studies, but further validation of the results is needed. The workflow and resulting maps are accessible through the Mapeo® visualization platform.},
  language  = {en},
  number    = {5},
  urldate   = {2025-03-21},
  journal   = {Remote Sensing},
  author    = {Delalieux, Stephanie and Hardy, Tom and Ferry, Michel and Gomez, Susi and Kooistra, Lammert and Culman, Maria and Tits, Laurent},
  month     = jan,
  year      = {2023},
  note      = {Number: 5
               Publisher: Multidisciplinary Digital Publishing Institute},
  keywords  = {date palm, decision tree, photogrammetry, red palm weevil, remote sensing, segmentation, UAV, vegetation index},
  pages     = {1380},
  file      = {Delalieux et al. - 2023 - Red Palm Weevil Detection in Date Palm Using Temporal UAV Imagery 1.pdf:G\:\\Mi unidad\\Libros\\Papers\\Delalieux et al. - 2023 - Red Palm Weevil Detection in Date Palm Using Temporal UAV Imagery 1.pdf:application/pdf}
}

@misc{poplin_palm_2014,
  title  = {Palm {Weevils}},
  author = {Poplin, A. and Roda, A. and Bhotika, S. and Sobel, L.},
  month  = may,
  year   = {2014},
  annote = {Obtenido de: https://entnemdept.ufl.edu/hodges/Collaborative/Documents/Palm\_weevils.pdf}
}

@article{mgap_informacion_nodate,
  title   = {Información actualizada sobre el picudo rojo de las palmeras a setiembre 2024},
  url     = {https://www.gub.uy/ministerio-ganaderia-agricultura-pesca/comunicacion/noticias/informacion-actualizada-sobre-picudo-rojo-palmeras-setiembre-2024},
  urldate = {2025-03-21},
  author  = {{MGAP}}
}


@misc{arcos_picudo_2024,
  address = {Montevideo},
  title   = {{PICUDO} {ROJO} ({Rynchophorus} ferrugineus) {EN} {MONTEVIDEO} {MONITOREO} {Y} {CONTROL} ({Avances})},
  urldate = {2025-03-21},
  author  = {Arcos, Alfonso},
  month   = aug,
  year    = {2024}
}

@misc{anticimex_picudo_nodate,
  title   = {Picudo {Rojo} {\textbar} {Daños} y tratamientos - {Anticimex}},
  url     = {https://www.anticimex.es/picudo-rojo/},
  urldate = {2025-03-21},
  journal = {Anticimex},
  author  = {{Anticimex}}
}


@article{zhorif_implementation_2024,
  title    = {Implementation of {Slicing} {Aided} {Hyper} {Inference} ({SAHI}) in {YOLOv8} to {Counting} {Oil} {Palm} {Trees} {Using} {High}-{Resolution} {Aerial} {Imagery} {Data}},
  volume   = {15},
  issn     = {21565570, 2158107X},
  url      = {http://thesai.org/Publications/ViewPaper?Volume=15&Issue=7&Code=ijacsa&SerialNo=86},
  doi      = {10.14569/IJACSA.2024.0150786},
  language = {en},
  number   = {7},
  urldate  = {2025-03-22},
  journal  = {International Journal of Advanced Computer Science and Applications},
  author   = {Zhorif, Naufal Najiv and Anandyto, Rahmat Kenzie and Rusyadi, Albrizy Ullaya and Irwansyah, Edy},
  year     = {2024}
}


@inproceedings{muna_development_2023,
  title     = {Development of {Automatic} {Counting} {System} for {Palm} {Oil} {Tree} {Based} on {Remote} {Sensing} {Imagery}},
  doi       = {10.2991/978-94-6463-086-2_68},
  booktitle = {Proceedings of the {International} {Conference} on {Sustainable} {Environment}, {Agriculture} and {Tourism} ({ICOSEAT} 2022)},
  publisher = {Atlantis Press},
  author    = {Muna, M. S. and Nugroho, A. P. and Syarovy, M. and Wiratmoko, A. and {Suwardi} and Sutiarso, L.},
  month     = jan,
  year      = {2023}
}

@article{wibowo_large-scale_2022,
  title   = {Large-{Scale} {Oil} {Palm} {Trees} {Detection} from {High}-{Resolution} {Remote} {Sensing} {Images} {Using} {Deep} {Learning}},
  volume  = {6},
  doi     = {10.3390/bdcc6030089},
  number  = {3},
  journal = {Big Data and Cognitive Computing},
  author  = {Wibowo, H. and Sitanggang, I. S. and Mushthofa, M. and Adrianto, H. A.},
  month   = sep,
  year    = {2022}
}

@inproceedings{wardana_detection_2023,
  title     = {Detection of {Oil} {Palm} {Trees} {Using} {Deep} {Learning} {Method} with {High}-{Resolution} {Aerial} {Image} {Data}},
  doi       = {10.1145/3626641.3626667},
  booktitle = {{ACM} {International} {Conference} {Proceeding} {Series}},
  publisher = {Association for Computing Machinery},
  author    = {Wardana, D. P. T. and Sianturi, R. S. and Fatwa, R.},
  year      = {2023},
  pages     = {90--98}
}

@misc{prasvita_automatic_nodate,
  title  = {Automatic {Detection} of {Oil} {Palm} {Growth} {Rate} {Status} with {YOLOv5}},
  author = {Prasvita, D. Sandya and Chahyati, D. and Arymurthy, A. M.},
  annote = {[Online]. Available: www.ijacsa.thesai.org}
}

@inproceedings{nuwara_modern_2022,
  title     = {Modern {Computer} {Vision} for {Oil} {Palm} {Tree} {Health} {Surveillance} using {YOLOv5}},
  doi       = {10.1109/GECOST55694.2022.10010668},
  booktitle = {2022 {International} {Conference} on {Green} {Energy}, {Computing} and {Sustainable} {Technology}, {GECOST} 2022},
  publisher = {Institute of Electrical and Electronics Engineers Inc.},
  author    = {Nuwara, Y. and Wong, W. K. and Juwono, F. H.},
  year      = {2022},
  pages     = {404--409}
}

@misc{junos_notitle_nodate,
  author = {Junos, M. H. and Salwa, A. and Khairuddin, M. and Kairi, M. I. and Siran, Y. M.},
  annote = {Organized by the Faculty of Engineering, doi: 10.21467/proceedings.141}
}


@article{ammar_deep-learning-based_2021,
  title     = {Deep-{Learning}-{Based} {Automated} {Palm} {Tree} {Counting} and {Geolocation} in {Large} {Farms} from {Aerial} {Geotagged} {Images}},
  volume    = {11},
  copyright = {https://creativecommons.org/licenses/by/4.0/},
  issn      = {2073-4395},
  url       = {https://www.mdpi.com/2073-4395/11/8/1458},
  doi       = {10.3390/agronomy11081458},
  abstract  = {In this paper, we propose an original deep learning framework for the automated counting and geolocation of palm trees from aerial images using convolutional neural networks. For this purpose, we collected aerial images from two different regions in Saudi Arabia, using two DJI drones, and we built a dataset of around 11,000 instances of palm trees. Then, we applied several recent convolutional neural network models (Faster R-CNN, YOLOv3, YOLOv4, and EfficientDet) to detect palms and other trees, and we conducted a complete comparative evaluation in terms of average precision and inference speed. YOLOv4 and EfficientDet-D5 yielded the best trade-off between accuracy and speed (up to 99\% mean average precision and 7.4 FPS). Furthermore, using the geotagged metadata of aerial images, we used photogrammetry concepts and distance corrections to automatically detect the geographical location of detected palm trees. This geolocation technique was tested on two different types of drones (DJI Mavic Pro and Phantom 4 pro) and was assessed to provide an average geolocation accuracy that attains 1.6 m. This GPS tagging allows us to uniquely identify palm trees and count their number from a series of drone images, while correctly dealing with the issue of image overlapping. Moreover, this innovative combination between deep learning object detection and geolocalization can be generalized to any other objects in UAV images.},
  language  = {en},
  number    = {8},
  urldate   = {2025-03-22},
  journal   = {Agronomy},
  author    = {Ammar, Adel and Koubaa, Anis and Benjdira, Bilel},
  month     = jul,
  year      = {2021},
  pages     = {1458}
}


@misc{intendencia_de_montevideo_acciones_nodate,
  title   = {Acciones de la {Intendencia} de {Montevideo} contra el {Picudo} {Rojo} {\textbar} {Portal} institucional},
  url     = {https://montevideo.gub.uy/noticias/urbanismo-y-obras/acciones-de-la-intendencia-de-montevideo-contra-el-picudo-rojo},
  urldate = {2025-03-26},
  author  = {{Intendencia de Montevideo}}
}


@article{sanchez_cirugiespecializada_nodate,
  title    = {Cirugía especializada en palmeras},
  language = {es},
  author   = {Sánchez, Pedro Hernández},
  file     = {Sánchez - Cirugía especializada en palmeras.pdf:G\:\\Mi unidad\\Libros\\Papers\\Sánchez - Cirugía especializada en palmeras.pdf:application/pdf}
}


@misc{intendencia_de_montevideo_sistema_nodate,
  title    = {Sistema de {Información} {Geográfica}},
  url      = {https://sig.montevideo.gub.uy/},
  abstract = {[Resumen de la página/contenido]},
  language = {es},
  urldate  = {2025-03-26},
  author   = {{Intendencia de Montevideo}}
}


@misc{tan_efficientnet_2020,
  title      = {{EfficientNet}: {Rethinking} {Model} {Scaling} for {Convolutional} {Neural} {Networks}},
  shorttitle = {{EfficientNet}},
  url        = {http://arxiv.org/abs/1905.11946},
  doi        = {10.48550/arXiv.1905.11946},
  abstract   = {Convolutional Neural Networks (ConvNets) are commonly developed at a fixed resource budget, and then scaled up for better accuracy if more resources are available. In this paper, we systematically study model scaling and identify that carefully balancing network depth, width, and resolution can lead to better performance. Based on this observation, we propose a new scaling method that uniformly scales all dimensions of depth/width/resolution using a simple yet highly effective compound coefficient. We demonstrate the effectiveness of this method on scaling up MobileNets and ResNet. To go even further, we use neural architecture search to design a new baseline network and scale it up to obtain a family of models, called EfficientNets, which achieve much better accuracy and efficiency than previous ConvNets. In particular, our EfficientNet-B7 achieves state-of-the-art 84.3\% top-1 accuracy on ImageNet, while being 8.4x smaller and 6.1x faster on inference than the best existing ConvNet. Our EfficientNets also transfer well and achieve state-of-the-art accuracy on CIFAR-100 (91.7\%), Flowers (98.8\%), and 3 other transfer learning datasets, with an order of magnitude fewer parameters. Source code is at https://github.com/tensorflow/tpu/tree/master/models/official/efficientnet.},
  urldate    = {2025-03-27},
  publisher  = {arXiv},
  author     = {Tan, Mingxing and Le, Quoc V.},
  month      = sep,
  year       = {2020},
  note       = {arXiv:1905.11946 [cs]},
  keywords   = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Statistics - Machine Learning},
  file       = {Tan y Le - 2020 - EfficientNet Rethinking Model Scaling for Convolutional Neural Networks.pdf:G\:\\Mi unidad\\Libros\\Papers\\Tan y Le - 2020 - EfficientNet Rethinking Model Scaling for Convolutional Neural Networks.pdf:application/pdf}
}


@book{mokhtar_image_2023,
  title      = {Image {Processing} {Systems} for {Unmanned} {Aerial} {Vehicle}: {State}-of-the-art},
  shorttitle = {Image {Processing} {Systems} for {Unmanned} {Aerial} {Vehicle}},
  abstract   = {The dependence on Unmanned Aerial Vehicles (UAVs) has dramatically increased in many sectors around the globe. UAVs are in high demand, and their technology is developing quickly due to their sophisticated ability to handle various issues. UAVs are capable of replacing labor-intensive tasks with conducive and safe regulation. Additional tools or sensors need to be added to the UAVs system to ensure the implementation of UAVs able to serve into industrial level. The paper aims to consolidate and present a thorough understanding of the various stages of image processing pipelines deployed in UAV applications, including image acquisition, preprocessing, feature extraction, object detection and tracking, and decision-making processes. Throughout this paper, several aspects were deliberate such as strengths, limitations, and performance metrics of existing approaches, this paper seeks to provide researchers, engineers, and practitioners with valuable insights into the challenges and opportunities of image processing systems for UAVs. Ultimately, the synthesis of this knowledge will contribute to enhancing the effective-ness, autonomy, and applicability of UAVs in diverse fields such as surveillance, agriculture, disaster management, and environmental monitoring.},
  author     = {Mokhtar, Izham and Hameed Sultan, Mohamed Thariq and Shahar, Farah and Nayak, Suhas and Łukaszewicz, Andrzej and Nowakowski, Marek and Giernacki, Wojciech and Holovatyy, Andriy},
  month      = aug,
  year       = {2023},
  doi        = {10.20944/preprints202308.1855.v1},
  file       = {Mokhtar et al. - 2023 - Image Processing Systems for Unmanned Aerial Vehicle State-of-the-art.pdf:G\:\\Mi unidad\\Libros\\Papers\\Mokhtar et al. - 2023 - Image Processing Systems for Unmanned Aerial Vehicle State-of-the-art.pdf:application/pdf}
}


@misc{unmanned_system_technology_uavdrone_nodate,
  title    = {{UAV}/{Drone} {Cameras} for {Commercial}, {Industrial}, and {Military} {Applications}},
  url      = {https://www.unmannedsystemstechnology.com/expo/cameras/},
  abstract = {Discover the advanced technologies behind drone cameras, inc. MWIR, SWIR, \& thermal imaging. Learn about UAV imaging systems for military, industrial, \& commercial use},
  language = {en-US},
  urldate  = {2025-03-27},
  journal  = {Unmanned Systems Technology},
  author   = {{Unmanned System technology}}
}

@misc{2000_aviation_ortoimagenes_nodate,
  title    = {Ortoimágenes – 2000 {Aviation}},
  url      = {https://www.2000aviation.com/ortoimagenes/},
  language = {es},
  urldate  = {2025-03-27},
  author   = {{2000 Aviation}}
}

@misc{dji_support_nodate,
  title    = {Support for {Phantom} 4 {Pro}},
  url      = {https://www.dji.com/global/support/product/photo},
  abstract = {Learn how to use Phantom 4 Pro and get useful tips, tutorial videos, specifications, and after-sales services.},
  language = {en},
  urldate  = {2025-03-27},
  journal  = {DJI},
  author   = {{DJI}}
}


@misc{agisoft_agisoft_nodate,
  title   = {Agisoft {Metashape}: {Agisoft} {Metashape}},
  url     = {https://www.agisoft.com/},
  urldate = {2025-03-27},
  author  = {{Agisoft}},
  file    = {Agisoft Metashape\: Agisoft Metashape:C\:\\Users\\maso\\Zotero\\storage\\6USRHXIM\\www.agisoft.com.html:text/html}
}


@misc{dji_specs_nodate,
  title    = {Specs - {DJI} {Mavic} 3 {Enterprise} - {DJI} {Enterprise}},
  url      = {https://enterprise.dji.com/mavic-3-enterprise/photo},
  abstract = {The Mavic 3 Enterprise Series redefines industry standards for small commercial drones. With a mechanical shutter, a 56× zoom camera, and an RTK module for centimeter-level precision, the Mavic 3E brings mapping and mission efficiency to new heights. A thermal version is available for firefighting, search and rescue, inspection, and night operations.},
  language = {en},
  urldate  = {2025-03-27},
  journal  = {DJI},
  author   = {{DJI}}
}


@misc{wikipedia_rtk_2024,
  title     = {{RTK} (navegación)},
  copyright = {Creative Commons Attribution-ShareAlike License},
  url       = {https://es.wikipedia.org/w/index.php?title=RTK_(navegaci%C3%B3n)&oldid=161740605},
  abstract  = {RTK (del inglés Real Time Kinematic) o navegación cinética satelital en tiempo real, es una técnica usada para la topografía, maquinaria agrícola y navegación marina basado en el uso de medidas de fase de navegadores con señales GPS, GLONASS y/o de Galileo, donde una sola estación de referencia proporciona correcciones en tiempo real, obteniendo una exactitud submetrica. Cuando se refiere al uso particular de la red GPS, el sistema también es llamado comúnmente como DGPS (Corrección de portador de  fase).
               Los receptores "normales" basados navegación por satélite, comparan una señal pseudoaleatoria que es enviada desde el satélite con una copia interna generada por la misma señal. Puesto que la señal del satélite tarda tiempo en alcanzar al receptor, las dos señales no se "alinean" correctamente; la copia del satélite se retrasa en referencia a la copia local. Al retrasar progresivamente la copia local, las dos señales se alinearán correctamente en algún momento. Este retraso es el tiempo necesario para que la señal alcance al receptor, y del resultado de esto puede ser calculada la distancia al satélite.
               La precisión de la medición resultante es generalmente una función de la capacidad electrónica del receptor para comparar exactamente las dos señales. En general, los receptores tradicionales pueden alinear las señales con un porcentaje de 1\% de margen de error.
               Por ejemplo, el código de lectura en bruto (C/A) enviado al sistema del GPS envía un bit de información cada 0.98 microsegundos, de tal modo que un receptor tiene precisión de 0.01 microsegundos, o cercano a los 3 metros en términos de distancia. La señal exclusiva de uso militar P(Y) enviada por los mismos satélites se registra diez veces más rápidamente, así que con técnicas similares el receptor tendrá precisión de cerca de 30 cm. Otros efectos introducen errores mucho mayores que esto, y la exactitud basada en una señal C/A sin corregir es generalmente cerca de 15 M.
               RTK sigue el mismo concepto general, pero usa el portador de satélite como su señal, no los mensajes contenidos en el mismo. La mejora posible usando esta señal es potencialmente muy alta si una continúa asumiendo una exactitud del 1\% en la fijación. Por ejemplo, el código de adquisición de datos en bruto GPS (C/A) transmitidos en señal L1 cambia fase a 1.023 megaciclos (MHz), pero  el portador L1 por sí mismo es de 1575.42 MHz, más de mil veces más rápido. Esta frecuencia corresponde a una longitud de onda de 19 cm para la señal L1. De esta manera un error de ±1\% en la medición de fase del portador L1 corresponde a un error de ±1.9mm en la estimación base.
               La dificultad para implementar un sistema RTK radica en alinear correctamente las señales. Las señales de navegación se codifican deliberadamente para impedir que sean alineadas fácilmente, donde cada ciclo del portador es similar a cada otro. Esto provoca que sea extremadamente difícil saber si se han alineado correctamente las señales o si está corrida en un ciclo y de este modo se está introduciendo un error de 20 cm, o un múltiplo más grande de 20 cm. Este problema de ambigüedad de un número entero se puede abordar a cierto grado con sofisticados métodos estadísticos que comparan las mediciones de las señales C/A y comparando los rangos resultantes entre varios satélites. Sin embargo, ninguno de estos métodos pueden reducir este error a cero.
               En la práctica, los sistemas de RTK utilizan un solo receptor como estación base y un número determinado de unidades móviles. La estación base retransmite la fase del portador que hace mediciones, y las unidades móviles comparan sus propias medidas de fase con las que está recibiendo la estación base. Hay varias maneras de transmitir una señal corregida de la estación base a la estación móvil. La manera más popular de alcanzar una transmisión de señales en tiempo real y de bajo costo es utilizar un módem de radio, típicamente en la banda UHF. En la mayoría de los países, ciertas frecuencias se asignan específicamente para uso de RTK. Gran parte del equipo topográfico terrestre tiene un módem de banda UHF integrado como opción estándar. Hoy en día es muy popular el uso de comunicación GPRS (por vía de internet celular móvil) entre la base y el rover, o bien del rover con respecto a una estación de referencia, que bien puede ser CORS (de operación continua) o VRS (virtual).
               Esto permite que las unidades calculen su posición relativa en milímetros, aunque su posición absoluta sea exacta solamente a la misma exactitud que la posición de la estación base. La exactitud nominal típica para estos sistemas de doble frecuencia es de 1 centímetro ± 2 partes por millón (ppm) horizontalmente y 2 centímetros ± 2 ppm verticalmente.
               Aunque estos parámetros limitan la utilidad de la técnica RTK en términos de navegación general, se adapta perfectamente para fines topográficos. En este caso, la estación base está situada en una ubicación predeterminada y bien referenciada, a menudo una mohonera, y las unidades móviles pueden entonces producir un mapa con alta precision al hacer correcciones en relación con ese punto. También se han encontrado aplicaciones de RTK en sistemas de navegación automática (piloto automático), industria agrícola de precisión y otros fines similares.
               El método de estaciones de referencia virtuales (VRS) aumenta el uso de RTK a un área entera de una red de estaciones de referencia. La confiabilidad operacional y las exactitudes que se alcanzarán dependen de la densidad y las capacidades de la red referencia. Finalmente, las nuevas orientaciones de los levantamientos precisos, serán la inserción de RTK y VRS.},
  language  = {es},
  urldate   = {2025-03-27},
  journal   = {Wikipedia, la enciclopedia libre},
  author    = {{Wikipedia}},
  month     = aug,
  year      = {2024},
  note      = {Page Version ID: 161740605}
}


@misc{wikipedia_networked_2025,
  title     = {Networked {Transport} of {RTCM} via {Internet} {Protocol}},
  copyright = {Creative Commons Attribution-ShareAlike License},
  url       = {https://en.wikipedia.org/w/index.php?title=Networked_Transport_of_RTCM_via_Internet_Protocol&oldid=1277712218},
  abstract  = {The Networked Transport of RTCM via Internet Protocol (NTRIP) is a protocol for streaming differential GPS (DGPS) corrections over the Internet for real-time kinematic positioning.
               NTRIP is a generic, stateless protocol based on the Hypertext Transfer Protocol HTTP/1.1 and is enhanced for GNSS data streams. 
               The specification is standardized by the Radio Technical Commission for Maritime Services (RTCM).
               NTRIP was developed by the German Federal Agency for Cartography and Geodesy (BKG) and the Dortmund University Department of Computer Science. Ntrip was released in September 2004. The 2011 version of the protocol is version 2.0. 
               NTRIP used to be an open standard protocol but it is not available freely (as of 2020). There is an open source implementation available from software.rtcm-ntrip.org from where the protocol can be reverse-engineered.},
  language  = {en},
  urldate   = {2025-03-27},
  journal   = {Wikipedia},
  author    = {{Wikipedia}},
  month     = feb,
  year      = {2025},
  note      = {Page Version ID: 1277712218}
}


@book{sommerville_software_2015,
  title     = {Software {Engineering}},
  publisher = {Addison-Wesley},
  author    = {Sommerville, Ian},
  month     = mar,
  year      = {2015}
}

@misc{github_build_nodate,
  title    = {Build software better, together},
  url      = {https://github.com},
  abstract = {GitHub is where people build software. More than 150 million people use GitHub to discover, fork, and contribute to over 420 million projects.},
  language = {en},
  urldate  = {2025-03-27},
  journal  = {GitHub},
  author   = {{Github}}
}


@article{merkel_docker_2014,
  title   = {Docker: lightweight {Linux} containers for consistent development and deployment},
  volume  = {2014},
  url     = {https://dl.acm.org/citation.cfm?id=2600239.2600241},
  number  = {239},
  journal = {Linux journal},
  author  = {Merkel, Dirk},
  month   = mar,
  year    = {2014},
  pages   = {2--}
}


@misc{docker_docker_0000,
  title    = {Docker documentation},
  url      = {https://docs.docker.com/},
  abstract = {Docker Documentation is the official Docker library of resources, manuals, and guides to help you containerize applications.},
  language = {en},
  urldate  = {2025-03-27},
  journal  = {Docker Documentation},
  author   = {{Docker}},
  year     = {0000}
}



@misc{hashicorp_vagrant_2023,
  title  = {Vagrant},
  author = {{HashiCorp}},
  year   = {2023}
}

@misc{hashicorp_documentation_nodate,
  title    = {Documentation {\textbar} {Vagrant} {\textbar} {HashiCorp} {Developer}},
  url      = {https://developer.hashicorp.com/vagrant/docs},
  abstract = {Welcome to the documentation for Vagrant - the command line utility for managing the lifecycle of virtual machines. This website aims to document every feature of Vagrant from top-to-bottom, covering as much detail as possible.},
  language = {en},
  urldate  = {2025-03-27},
  journal  = {Documentation {\textbar} Vagrant {\textbar} HashiCorp Developer},
  author   = {{HashiCorp}}
}



@misc{red_hat_openshift_2023,
  title  = {{OpenShift}},
  author = {{Red Hat}},
  year   = {2023}
}

@misc{the_helm_authors_helm_2023,
  title  = {Helm},
  author = {{The Helm Authors}},
  year   = {2023}
}

@misc{the_helm_authors_docs_nodate,
  title    = {Docs {Home}},
  url      = {https://helm.sh/docs/},
  abstract = {Everything you need to know about how the documentation is organized.},
  language = {en},
  urldate  = {2025-03-27},
  author   = {{The Helm Authors}}
}

@misc{red_hat_red_nodate,
  title   = {Red {Hat} {Product} {Documentation}},
  url     = {https://docs.redhat.com/en},
  urldate = {2025-03-27},
  author  = {{Red Hat}}
}


@misc{wikipedia_kubernetes_2025,
  title     = {Kubernetes},
  copyright = {Creative Commons Attribution-ShareAlike License},
  url       = {https://es.wikipedia.org/w/index.php?title=Kubernetes&oldid=166046716},
  abstract  = {Kubernetes es una plataforma de sistema distribuido de código libre para la automatización del despliegue, ajuste de escala y manejo de aplicaciones en contenedores[1]​ que fue originalmente diseñado por Google y donado a la Cloud Native Computing Foundation (parte de la Linux Foundation). Soporta diferentes entornos para la ejecución de contenedores, incluido Docker (aunque se desaconseja usarlo desde la versión 1.20, y no se soporta desde la versión 1.24).[2]​[3]​
               Es habitual abreviar el nombre como «K8s» al tomar la primera y última letras y el número de letras omitidas.[4]​},
  language  = {es},
  urldate   = {2025-03-27},
  journal   = {Wikipedia, la enciclopedia libre},
  author    = {{Wikipedia}},
  month     = mar,
  year      = {2025},
  note      = {Page Version ID: 166046716}
}


@misc{noauthor_protocolo_2024,
  title     = {Protocolo ligero de acceso a directorios},
  copyright = {Creative Commons Attribution-ShareAlike License},
  url       = {https://es.wikipedia.org/w/index.php?title=Protocolo_ligero_de_acceso_a_directorios&oldid=162588716},
  abstract  = {El protocolo ligero de acceso a directorios (en inglés: Lightweight Directory Access Protocol, también conocido por sus siglas de LDAP) hace referencia a un protocolo a nivel de aplicación que permite el acceso a un servicio de directorio ordenado y distribuido para buscar diversa información en un entorno de red.
               Un directorio es un conjunto de objetos con atributos organizados en una manera lógica y jerárquica. El ejemplo más común es el directorio telefónico, que consiste en una serie de nombres (personas u organizaciones) que están ordenados alfabéticamente, con cada nombre teniendo una dirección y un número de teléfono adjuntos. Para entender mejor, es un libro o carpeta, en la cual se escriben nombres de personas, teléfonos y direcciones, y se ordena alfabéticamente.
               Un árbol de directorio LDAP a veces refleja varios límites políticos, geográficos u organizacionales, dependiendo del modelo elegido. Los despliegues actuales de LDAP tienden a usar nombres de Sistema de Nombres de Dominio (DNS por sus siglas en inglés) para estructurar los niveles más altos de la jerarquía. Conforme se desciende en el directorio pueden aparecer entradas que representan personas, unidades organizacionales, impresoras, documentos, grupos de personas o cualquier cosa que representa una entrada dada en el árbol (o múltiples entradas).
               Habitualmente, almacena la información de autenticación (usuario y contraseña) y es utilizado para autenticarse aunque es posible almacenar otra información (datos de contacto del usuario, ubicación de diversos recursos de la red, permisos, certificados, etc). A manera de síntesis, LDAP es un protocolo de acceso unificado a un conjunto de información sobre una red.
               La versión actual es LDAPv3, y se encuentra definido en el RFC 4511(una hoja de ruta de las especificaciones técnicas está suministrada por la RFC 4510).},
  language  = {es},
  urldate   = {2025-03-27},
  journal   = {Wikipedia, la enciclopedia libre},
  month     = sep,
  year      = {2024},
  note      = {Page Version ID: 162588716}
}


@misc{lldap_authors_lldap_2023,
  title  = {{LLDAP}: {Light} {LDAP} implementation},
  author = {{LLDAP Authors}},
  year   = {2023}
}


@misc{sekachev_opencvcvat_2020,
  title     = {opencv/cvat: v1.1.0},
  url       = {https://doi.org/10.5281/zenodo.4009388},
  publisher = {Zenodo},
  author    = {Sekachev, Boris and Manovich, Nikita and Zhiltsov, Maxim and Zhavoronkov, Andrey and Kalinin, Dmitry and Hoff, Ben and {TOsmanov} and Kruchinin, Dmitry and Zankevich, Artyom and {DmitriySidnev} and Markelov, Maksim and {Johannes222} and Chenuet, Mathis and {a-andre} and {telenachos} and Melnikov, Aleksandr and Kim, Jijoong and Ilouz, Liron and Glazov, Nikita and {Priya4607} and Tehrani, Rush and Jeong, Seungwon and Skubriev, Vladimir and Yonekura, Sebastian and truong, vugia and {zliang7} and {lizhming} and Truong, Tritin},
  month     = aug,
  year      = {2020},
  doi       = {10.5281/zenodo.4009388}
}


@article{moore_fiftyone_2020,
  title   = {{FiftyOne}},
  journal = {GitHub. Note: https://github.com/voxel51/fiftyone},
  author  = {Moore, B. E. and Corso, J. J.},
  year    = {2020}
}


@misc{minio_inc_minio_2023,
  title  = {{MinIO}: {High} {Performance} {Object} {Storage}},
  author = {{MinIO, Inc.}},
  year   = {2023}
}


@misc{amazon_amazon_2024,
  title     = {Amazon {S3}},
  copyright = {Creative Commons Attribution-ShareAlike License},
  url       = {https://es.wikipedia.org/w/index.php?title=Amazon_S3&oldid=164024822},
  abstract  = {Amazon S3 o Amazon Simple Storage Service es un servicio ofrecido por Amazon Web Services (AWS) que proporciona almacenamiento de objetos a través de una interfaz de servicio web.[1]​ Amazon S3 utiliza la misma infraestructura de almacenamiento escalable que utiliza Amazon.com para ejecutar su red de comercio electrónico. Amazon S3 puede almacenar cualquier tipo de objeto, lo que permite usos como almacenamiento para aplicaciones de Internet, copias de seguridad, recuperación ante desastres, archivos de datos, lagos de datos para análisis y almacenamiento en la nube híbrida .
               AWS lanzó Amazon S3 en los Estados Unidos el 14 de marzo de 2006,[2]​ luego en Europa en noviembre de 2007.},
  language  = {es},
  urldate   = {2025-03-28},
  journal   = {Wikipedia, la enciclopedia libre},
  author    = {{Amazon}},
  month     = dec,
  year      = {2024},
  note      = {Page Version ID: 164024822}
}


@misc{mongodb_inc_mongodb_2023,
  title  = {{MongoDB}: {The} {Developer} {Data} {Platform}},
  author = {{MongoDB, Inc.}},
  year   = {2023}
}


@misc{comnav_technology_ltd_receptor_nodate,
  title    = {Receptor {GNSS} {T300} {Plus}-{Receptor} {GNSS}-{ComNav} {Technology} {Ltd}.},
  url      = {https://www.comnavtech.com/sp/product/receiver/T300Plus.html},
  abstract = {El Receptor GNSS T300 Plus es un receptor RTK de nueva generación que cuenta con seguimiento de constelaciones completas, compensación de inclinación de hasta 120° y un flujo de trabajo sencillo con el software Survey Master basado en Android, para realizar levantamientos de precisión centimétrica según demanda},
  language = {zh-CN},
  urldate  = {2025-03-28},
  author   = {{ComNav Technology Ltd.}}
}
