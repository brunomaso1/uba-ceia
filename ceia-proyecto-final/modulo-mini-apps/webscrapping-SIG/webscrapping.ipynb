{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <div align=\"center\"><b> WEBSCRAPPING - PROYECTO FINAL </b></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"right\">üìù <em><small><font color='Gray'>Nota:</font></small></em></div>\n",
    "\n",
    "<div align=\"right\"> <em><small><font color='Gray'> La funcionalidad de visualizaci√≥n de jupyter notebooks en <a href=\"https://github.com/\" target=\"_blank\">github</a> es solamente un preview.</font></small></em> </div>\n",
    "\n",
    "<div align=\"right\"> <em><small><font color='Gray'> Para mejor visualizaci√≥n se sugiere utilizar el visualizador recomendado por la comunidad: <a href=\"https://nbviewer.org/\" target=\"_blank\">nbviewer</a></font></small></em> </div>\n",
    "\n",
    "<div align=\"right\"> <em><small><font color='Gray'> Puedes a acceder al siguiente enlace para ver este notebook en dicha p√°gina: <a href=\"https://nbviewer.org/ruta/de/archivo.ipynb\">Ruta archivo</a></font></small></em> </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* * *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<style>\n",
    "/* Limitar la altura de las celdas de salida en html */\n",
    ".jp-OutputArea.jp-Cell-outputArea {\n",
    "    max-height: 500px;\n",
    "}\n",
    "</style>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚úã <em><font color='DodgerBlue'>Importaciones:</font></em> ‚úã"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests, re, logging, os, zipfile, sys, datetime, shutil\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv\n",
    "from bs4 import BeautifulSoup\n",
    "from pymongo import MongoClient\n",
    "import boto3\n",
    "from botocore.exceptions import ClientError\n",
    "from typing import Tuple\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üîß <em><font color='tomato'>Configuraciones:</font></em> üîß\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuraci√≥n\n",
    "ENVIRONMENT = \"LOCAL\"  # PRODUCTION, LOCAL, SCALEWAY \n",
    "\n",
    "# Variables entorno\n",
    "# Cargar variables de entorno\n",
    "load_dotenv(\"../.env\")\n",
    "\n",
    "# Variables de la base de datos\n",
    "MONGODB_INITDB_DATABASE = os.getenv(\"MONGODB_INITDB_DATABASE\")\n",
    "MONGODB_SERVER_HOST = os.getenv(\"MONGODB_SERVER_HOST\")\n",
    "MONGODB_SERVER_PORT = os.getenv(\"MONGODB_SERVER_PORT\")\n",
    "MONGODB_USER = os.getenv(\"MONGODB_USER\")\n",
    "MONGODB_PASSWORD = os.getenv(\"MONGODB_PASSWORD\")\n",
    "\n",
    "# Variables de S3\n",
    "S3_BUCKET = os.getenv(\"SCALEWAY_BUCKET\") if ENVIRONMENT == \"SCALEWAY\" else os.getenv(\"MINIO_BUCKET\")\n",
    "S3_ENDPOINT_URL = os.getenv(\"SCALEWAY_ENDPOINT_URL\") if ENVIRONMENT == \"SCALEWAY\" else os.getenv(\"MINIO_ENDPOINT_URL\")\n",
    "S3_ACCESS_KEY = os.getenv(\"SCALEWAY_ACCESS_KEY\") if ENVIRONMENT == \"SCALEWAY\" else os.getenv(\"MINIO_ACCESS_KEY\")\n",
    "S3_SECRET_KEY = os.getenv(\"SCALEWAY_SECRET_KEY\") if ENVIRONMENT == \"SCALEWAY\" else os.getenv(\"MINIO_SECRET_KEY\")\n",
    "S3_REGION = os.getenv(\"SCALEWAY_REGION\") if ENVIRONMENT == \"SCALEWAY\" else os.getenv(\"MINIO_REGION\")\n",
    "\n",
    "# Configura el logger\n",
    "logging.basicConfig(\n",
    "    format=\"%(asctime)s - %(name)s - %(levelname)s - %(funcName)s - %(message)s\",\n",
    ")\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "# Configuracion\n",
    "DOWNLOAD_PATH = \"./downloads\"\n",
    "MONGODB_CONECTION_STRING = f\"mongodb://{MONGODB_USER}:{MONGODB_PASSWORD}@{MONGODB_SERVER_HOST}:{MONGODB_SERVER_PORT}/{MONGODB_INITDB_DATABASE}\"\n",
    "PATCHES_PATH = os.path.join(DOWNLOAD_PATH, \"patches\")\n",
    "TILE_SIZE = (4096, 4096)\n",
    "OVER_LAP = 400\n",
    "PURGE_WHITE_IMAGES = True\n",
    "S3_BUCKET_IMAGES_PATH = \"imagenes\"\n",
    "S3_BUCKET_PATCHES_PATH = \"patches\"\n",
    "S3_BUCKET_METADATA_PATH = \"metadatos\"\n",
    "\n",
    "# Setear la variable de entorno OPENCV_IO_MAX_IMAGE_PIXELS para evitar errores de OpenCV\n",
    "OPENCV_IO_MAX_IMAGE_PIXELS=50000*50000 # Para im√°genes grandes, ej: barrio3Ombues_20180801_dji_pc_3cm.jpg\n",
    "os.environ[\"OPENCV_IO_MAX_IMAGE_PIXELS\"] = str(OPENCV_IO_MAX_IMAGE_PIXELS)  # Para im√°genes grandes, ej: barrio3Ombues_20180801_dji_pc_3cm.jpg\n",
    "# Importar OpenCV\n",
    "import cv2\n",
    "\n",
    "# Constantes\n",
    "URL_MAIN_PAGE = \"https://gis.montevideo.gub.uy/pmapper/map.phtml?&config=default&me=548000,6130000,596000,6162000\"\n",
    "URL_TOC = \"https://intgis.montevideo.gub.uy/pmapper/incphp/xajax/x_toc.php?\"\n",
    "URL_GENERATE_ZIP = \"https://intgis.montevideo.gub.uy/sit/php/common/datos/generar_zip2.php?nom_jpg=/inetpub/wwwroot/sit/mapserv/data/fotos_dron/{id}&tipo=jpg\"\n",
    "URL_DOWNLOAD_ZIP = \"https://intgis.montevideo.gub.uy/sit/tmp/{id}.zip\"\n",
    "URL_JS = \"https://intgis.montevideo.gub.uy/pmapper/config/default/custom.js\"\n",
    "\n",
    "HEADERS_COMMON = {\n",
    "    \"User-Agent\": \"Mozilla/5.0\",\n",
    "}\n",
    "\n",
    "HEADERS_TOC = {\n",
    "    \"User-Agent\": \"Mozilla/5.0\",\n",
    "    \"Referer\": URL_MAIN_PAGE,\n",
    "    \"X-Requested-With\": \"XMLHttpRequest\",\n",
    "    \"Content-Type\": \"application/x-www-form-urlencoded\",\n",
    "}\n",
    "\n",
    "BODY_TOC = {\"dummy\": \"dummy\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- Colab -->\n",
    "<!-- <div align=\"center\"><img src=\"https://drive.google.com/uc?export=view&id=1QSNrTsz1hQbmZwpgwx0qpfpNtLW19Orm\" width=\"600\" alt=\"Figura 1: A data scientist is working on word generation using the Lord of the Rings lore. The image is dark and moody, with a focus on the scientist's computer screen. The screen displays a visualization the one ring, with a map of Middle Earth in the background. - Generada con DALL-E3\"></div> -->\n",
    "\n",
    "<!-- <div align=\"center\"><img src=\"./ceia-materia/resources/portada.jpeg\" width=\"600\" alt=\"Figura 1: A data scientist playing with convolutional neural networks. - Generada con Microsoft Image Creator\"></div>\n",
    "\n",
    "<div align=\"center\"><small><em>Figura 1: A data scientist playing with convolutional neural networks. - Generada con Microsoft Image Creator</em></small></div> -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"center\">‚ú®Datos del proyecto:‚ú®</div>\n",
    "\n",
    "<p></p>\n",
    "\n",
    "<div align=\"center\">\n",
    "\n",
    "| Subtitulo       | webscrappint-SIG                                                                                                                       |\n",
    "| --------------- | -------------------------------------------------------------------------------------------------------------------------------------- |\n",
    "| **Descrpci√≥n**  | Escrapeo web del Sistema de Informaci√≥n Geogr√°fica de la IM                                                                            |\n",
    "| **Integrantes** | Bruno Masoller (brunomaso1@gmail.com)                                                                                                  |\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Consinga"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El objetivo de este proyecto es realizar un webscraping del Sistema de Informaci√≥n Geogr√°fica de la IM (SIGIM) para obtener informaci√≥n sobre los barrios de Montevideo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resoluci√≥n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clase singleton para el cliente S3\n",
    "class S3Client:\n",
    "    __instance = None\n",
    "\n",
    "    @staticmethod\n",
    "    def get_instance():\n",
    "        if S3Client.__instance is None:\n",
    "            S3Client()\n",
    "        return S3Client.__instance\n",
    "\n",
    "    def __init__(self):\n",
    "        if S3Client.__instance is not None:\n",
    "            raise Exception(\"This class is a singleton!\")\n",
    "        else:\n",
    "            S3Client.__instance = self\n",
    "            self.client = boto3.client(\n",
    "                \"s3\",\n",
    "                endpoint_url=S3_ENDPOINT_URL,\n",
    "                aws_access_key_id=S3_ACCESS_KEY,\n",
    "                aws_secret_access_key=S3_SECRET_KEY,\n",
    "                region_name=S3_REGION,\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clase singleton para MongoDB\n",
    "class MongoDB:\n",
    "    __instance = None\n",
    "    @staticmethod\n",
    "    def get_instance():\n",
    "        if MongoDB.__instance is None:\n",
    "            MongoDB()\n",
    "        return MongoDB.__instance\n",
    "    def __init__(self):\n",
    "        if MongoDB.__instance is not None:\n",
    "            raise Exception(\"Esta clase es un singleton!\")\n",
    "        else:\n",
    "            MongoDB.__instance = self\n",
    "            self.client = MongoClient(MONGODB_CONECTION_STRING)\n",
    "            self.db = self.client[MONGODB_INITDB_DATABASE]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_url(url: str, headers: dict) -> str:\n",
    "    try:\n",
    "        response = requests.get(url, headers=headers)\n",
    "        response.raise_for_status()\n",
    "        return response.text\n",
    "    except requests.exceptions.HTTPError as err:\n",
    "        raise Exception(f\"Error al acceder a {url}. Raz√≥n: {err}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_toc_html() -> str:\n",
    "    \"\"\"\n",
    "    Inicia una sesi√≥n, realiza una petici√≥n GET a la p√°gina principal y posteriormente\n",
    "    una petici√≥n POST para obtener el HTML que contiene la Table of Contents (TOC).\n",
    "\n",
    "    Returns:\n",
    "        str: El contenido HTML que representa la estructura TOC de la p√°gina.\n",
    "\n",
    "    Raises:\n",
    "        Exception: Si se produce un error HTTP (4xx o 5xx) al acceder a la URL de TOC,\n",
    "                   se lanza una excepci√≥n con un mensaje indicando la raz√≥n del fallo.\n",
    "    \"\"\"\n",
    "    \n",
    "    session = requests.Session()\n",
    "\n",
    "    try:\n",
    "        session.get(URL_MAIN_PAGE, headers=HEADERS_COMMON)\n",
    "        response = session.post(URL_TOC, headers=HEADERS_TOC, data=BODY_TOC)\n",
    "        response.raise_for_status()\n",
    "        return response.text\n",
    "    except requests.exceptions.HTTPError as err:\n",
    "        raise Exception(f\"Error al acceder a {URL_TOC}. Raz√≥n: {err}\")\n",
    "    finally:\n",
    "        session.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pictures_list_html(html: str) -> str:\n",
    "    \"\"\"\n",
    "    A partir del HTML recibido, localiza el elemento `<li>` con id 'ligrp_grillaFotosDron'\n",
    "    y retorna el contenido HTML del `<ul>` padre que contiene las im√°genes de drones.\n",
    "\n",
    "    Args:\n",
    "        html (str): Cadena que representa el contenido HTML de la p√°gina.\n",
    "\n",
    "    Returns:\n",
    "        str: El contenido HTML del elemento `<ul>` que contiene las im√°genes de drones.\n",
    "\n",
    "    Raises:\n",
    "        ValueError: Si no se encuentra el `<li>` con el id especificado o si no existe\n",
    "                    un `<ul>` padre que lo contenga.\n",
    "    \"\"\"\n",
    "    \n",
    "    li_grilla_fotos_dron = \"ligrp_grillaFotosDron\"\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    \n",
    "    # 1) Buscamos el <li> deseado\n",
    "    li_drones_pictures = soup.find(\"li\", {\"id\": li_grilla_fotos_dron})\n",
    "    if li_drones_pictures is None:\n",
    "        raise ValueError(\"No se encontr√≥ el <li> con id='{li_grilla_fotos_dron}' en el HTML.\")\n",
    "    \n",
    "    # 2) Obtenemos el <ul> que lo contiene\n",
    "    ul_drones_pictures = li_drones_pictures.find_parent(\"ul\")\n",
    "    if ul_drones_pictures is None:\n",
    "        raise ValueError(\"No se encontr√≥ un elemento <ul> padre para el <li> con id='{li_grilla_fotos_dron}'.\")\n",
    "\n",
    "    # Devolvemos el HTML del <ul> que contiene las im√°genes\n",
    "    return str(ul_drones_pictures)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pics_dict(html: str) -> dict:\n",
    "    \"\"\"\n",
    "    Procesa el HTML de un `<ul>` para obtener informaci√≥n sobre cada elemento que\n",
    "    contiene im√°genes de drones. Busca spans con IDs que coincidan con la expresi√≥n\n",
    "    regular `.*fotosDron.*` y extrae un identificador, as√≠ como el atributo `title`.\n",
    "\n",
    "    Args:\n",
    "        html (str): Contenido HTML que representa la lista de im√°genes (un `<ul>`).\n",
    "\n",
    "    Returns:\n",
    "        list: Lista de diccionarios, donde cada diccionario contiene:\n",
    "              - \"id\": Un √≠ndice secuencial que identifica el elemento.\n",
    "              - \"js_name\": Valor derivado del ID del span (remueve el prefijo 'spxg_').\n",
    "              - \"title\": El valor del atributo title del span interno con clase \"grp-title\".\n",
    "\n",
    "    Raises:\n",
    "        (No lanza excepciones expl√≠citas propias, pero podr√≠a propagar excepciones de BeautifulSoup.)\n",
    "    \"\"\"\n",
    "\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "\n",
    "    spans_fotos_dron = soup.find_all(\"span\", id=re.compile(r\".*fotosDron.*\"))\n",
    "\n",
    "    resultados = []\n",
    "\n",
    "    for idx, sp in enumerate(spans_fotos_dron, start=1):\n",
    "        # 1) Obtener el valor a partir del id del span\n",
    "        span_id = sp.get(\"id\", \"\")\n",
    "        value_attr = span_id.replace(\"spxg_\", \"\")\n",
    "\n",
    "        # 2) El <span class=\"grp-title\" title=\"...\"> que est√° dentro\n",
    "        span_child_element = sp.find(\"span\")\n",
    "        if not span_child_element:\n",
    "            # Si no lo encuentra, pasa al siguiente\n",
    "            logger.warning(\n",
    "                f\"No se encontr√≥ <span class='grp-title'> dentro del span con id='{span_id}'.\"\n",
    "            )\n",
    "            continue\n",
    "        title_attr = span_child_element.get(\"title\")\n",
    "\n",
    "        # Agrego la fecha que se obtiene del atributo title\n",
    "        # TODO: Testear esto -> Agregado de fecha.\n",
    "        date_format = \"%d/%m/%Y\"\n",
    "        match = re.search(r\"\\d{2}/\\d{2}/\\d{4}\", title_attr)\n",
    "        date_str = match.group(0) if match else None\n",
    "        if not date_str:\n",
    "            logger.warning(\n",
    "                f\"No se encontr√≥ la fecha en el atributo title='{title_attr}'.\"\n",
    "            )\n",
    "            continue\n",
    "        date_captured = datetime.datetime.strptime(date_str, date_format)\n",
    "        \n",
    "        \n",
    "        # 3) Construir el objeto seg√∫n el formato deseado        \n",
    "        resultados.append(\n",
    "            {\n",
    "                \"image_name\": idx,\n",
    "                \"js_name\": value_attr,\n",
    "                \"title\": title_attr,\n",
    "                \"date_captured\": date_captured,\n",
    "            }\n",
    "        )\n",
    "\n",
    "    return resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_picture(id: str) -> str|None:\n",
    "    \"\"\"\n",
    "    Dada una cadena `id`, realiza un get para generar un archivo ZIP en el servidor y luego\n",
    "    lo descarga si existe. El ZIP contiene las im√°genes correspondientes al identificador.\n",
    "\n",
    "    Args:\n",
    "        id (str): Identificador que se usar√° para generar y descargar el ZIP.\n",
    "\n",
    "    Returns:\n",
    "        str | None: La ruta absoluta del archivo ZIP descargado. Retorna None si no se\n",
    "                    puede generar o descargar el ZIP.\n",
    "\n",
    "    Raises:\n",
    "        (Las excepciones de requests se manejan internamente, produciendo logs de error\n",
    "         o warning en caso de fallos. No se relanza la excepci√≥n.)\n",
    "    \"\"\"\n",
    "\n",
    "    # Ruta para generar el archivo ZIP en el servidor\n",
    "    url_generate_zip = URL_GENERATE_ZIP.format(id=id)\n",
    "\n",
    "    # Ruta para descargar el archivo ZIP\n",
    "    url_download_zip = URL_DOWNLOAD_ZIP.format(id=id)\n",
    "    \n",
    "    try:\n",
    "        response = requests.get(url_generate_zip, headers=HEADERS_COMMON)\n",
    "        response.raise_for_status()\n",
    "        logger.debug(f\"Archivo ZIP generado correctamente en el servidor. URL: {url_generate_zip}\")\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        logger.warning(f\"No se pudo generar el ZIP. URL: {url_generate_zip} - {e}\")\n",
    "    finally:\n",
    "        try:\n",
    "            response = requests.get(url_download_zip, headers=HEADERS_COMMON)\n",
    "            response.raise_for_status()\n",
    "            logger.debug(f\"Archivo ZIP descargado correctamente. URL: {url_download_zip}\")\n",
    "\n",
    "            if response.status_code == 200:\n",
    "                file_name = f\"{id}.zip\"\n",
    "                os.makedirs(DOWNLOAD_PATH, exist_ok=True)\n",
    "\n",
    "                file_path = os.path.join(DOWNLOAD_PATH, file_name)\n",
    "                \n",
    "                with open(file_path, \"wb\") as f:\n",
    "                    f.write(response.content)\n",
    "                logger.debug(f\"Archivo '{file_name}' descargado correctamente en {file_path}\")\n",
    "                \n",
    "                return file_path\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            logger.error(f\"No se pudo descargar el ZIP. URL: {url_download_zip} - {e}\")\n",
    "            return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_js_as_text() -> str:\n",
    "    return get_url(URL_JS, HEADERS_COMMON)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_js_cases(js_code: str) -> dict:\n",
    "    \"\"\"\n",
    "    Parsea el c√≥digo JavaScript para encontrar las l√≠neas que contengan casos y sus file_descarga.\n",
    "    \n",
    "    Args:\n",
    "        js_code (str): El contenido del archivo JavaScript.\n",
    "    Returns:\n",
    "        dict: Diccionario con los casos y sus rutas de archivo correspondientes.\n",
    "    \"\"\"\n",
    "    pattern = r\"\"\"case\\s+'([^']+)':\\s*\n",
    "                  (?:[^\\n]*\\n)?          # Captura opcional cualquier cosa hasta el fin de l√≠nea\n",
    "                  \\s*file_descarga\\s*=\\s*'([^']+)'\\s*;\"\"\"\n",
    "    \n",
    "    # re.VERBOSE permite escribir la regex m√°s legible con comentarios\n",
    "    # re.MULTILINE permite que ^ y $ coincidan con principio y fin de l√≠nea\n",
    "    # re.DOTALL hace que . coincida tambi√©n con saltos de l√≠nea\n",
    "    matches = re.findall(pattern, js_code, re.VERBOSE | re.MULTILINE | re.DOTALL)\n",
    "    \n",
    "    return {case_val: file_descarga for case_val, file_descarga in matches}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_download_file_id(js_text) -> str:\n",
    "    # Si tiene el prefijo \"fotos_dron/\", se lo removemos\n",
    "    return js_text.removeprefix(\"fotos_dron/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_file_download_id(resultados: list, mapping: dict) -> list:\n",
    "    \"\"\"\n",
    "    Agrega la clave 'file_download_id' a cada elemento de la lista `resultados`,\n",
    "    asociando su valor al correspondiente valor de `mapping` si existe.\n",
    "\n",
    "    Args:\n",
    "        resultados (list): Lista de diccionarios con al menos la clave \"js_name\".\n",
    "        mapping (dict): Diccionario con la relaci√≥n \"js_name\" -> \"file_descarga\".\n",
    "\n",
    "    Returns:\n",
    "        list: La lista de diccionarios `resultados` modificada, donde cada elemento\n",
    "              incluye la clave 'file_download_id' extra√≠da mediante la funci√≥n\n",
    "              `obtener_id_file_descarga` si existe, o None en caso contrario.\n",
    "    \"\"\"\n",
    "    \n",
    "    for item in resultados:\n",
    "        value_attr = item[\"js_name\"]\n",
    "        # Si existe la key en el mapping, se la asignamos\n",
    "        if value_attr in mapping:\n",
    "            desc = mapping[value_attr]\n",
    "            item[\"file_download_id\"] = get_download_file_id(desc)\n",
    "            logger.debug(f\"Se asign√≥ el valor '{item['file_download_id']}' a 'file_download_id' para '{value_attr}'.\")\n",
    "        else:\n",
    "            # Si no est√°, ponle None o alg√∫n valor por defecto\n",
    "            item[\"file_download_id\"] = None\n",
    "            logger.warning(f\"No se encontr√≥ un valor para '{value_attr}' en el mapping. Se asign√≥ None.\")\n",
    "    return resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_or_sync_downloaded(results: list) -> list:\n",
    "    \"\"\"\n",
    "    Agrega la clave 'downloaded' (estado) a cada elemento de la lista `resultados`, chequeando en la base de datos, indicando si el archivo ZIP asociado a ese elemento se est√° descargado o no.\n",
    "\n",
    "    Args:\n",
    "        resultados (list): Lista de diccionarios con al menos la clave \"id\".\n",
    "\n",
    "    Returns:\n",
    "        list: La lista de diccionarios `resultados` modificada, donde cada elemento\n",
    "              incluye la clave 'downloaded' con un valor booleano que indica si el archivo\n",
    "              ZIP se descarg√≥ o no.\n",
    "    \"\"\"\n",
    "    imagenes_db = MongoDB.get_instance().db.imagenes\n",
    "    for item in results:\n",
    "        # Busco en la base de datos si existe el item.\n",
    "        # Si existe el item, obtengo el estado.\n",
    "        # Sino, lo agrego a la base de datos y le pongo el estado en False.\n",
    "        item_db = imagenes_db.find_one({\"id\": item[\"file_download_id\"]})\n",
    "        if item_db:\n",
    "            item[\"downloaded\"] = item_db[\"downloaded\"]\n",
    "            logger.info(f\"El archivo ZIP asociado a '{item['file_download_id']}' est√° {'descargado' if item['downloaded'] else 'pendiente'}.\")\n",
    "        else:\n",
    "            logger.info(f\"El archivo ZIP asociado a '{item['file_download_id']}' no est√° en la base de datos. Se agregar√°.\")\n",
    "            imagenes_db.insert_one(\n",
    "                {\n",
    "                    \"id\": item[\"file_download_id\"],\n",
    "                    \"js_name\": item[\"js_name\"],\n",
    "                    \"title\": item[\"title\"],\n",
    "                    \"downloaded\": False\n",
    "                }\n",
    "            )\n",
    "            item[\"downloaded\"] = False        \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_files(zip_path: str, id: str = None) -> Tuple[str | None, str | None]:\n",
    "    \"\"\"\n",
    "    Extrae los archivos JPG y JGW de un archivo ZIP descargado.\n",
    "\n",
    "    Args:\n",
    "        zip_path (str): Ruta absoluta del archivo ZIP descargado.\n",
    "\n",
    "    Returns:\n",
    "        Tuple[str, str]: Tupla con las rutas absolutas de los archivos JPG y JGW extra√≠dos.\n",
    "                         Si no se pudo extraer alguno de los archivos, se devuelve None.\n",
    "\n",
    "    Raises:\n",
    "        Exception: Si el ZIP no contiene exactamente dos archivos, se lanza una excepci√≥n.\n",
    "    \"\"\"\n",
    "    extract_dir = os.path.join(DOWNLOAD_PATH, \"extracted\")\n",
    "    os.makedirs(extract_dir, exist_ok=True)\n",
    "\n",
    "    try:\n",
    "        with zipfile.ZipFile(zip_path, \"r\") as zip_ref:\n",
    "            zip_ref.extractall(extract_dir)\n",
    "\n",
    "        # Encontrar los archivos JPG y JGW extra√≠dos\n",
    "        jpg_file = None\n",
    "        jgw_file = None\n",
    "\n",
    "        # Obtengo los archivos extra√≠dos\n",
    "        jpg_file = os.path.join(extract_dir, f\"{id}.jpg\")\n",
    "        jgw_file = os.path.join(extract_dir, f\"{id}.jgw\")\n",
    "\n",
    "        # Chequeo que existan los archivos\n",
    "        if not os.path.exists(jpg_file) or not os.path.exists(jgw_file):\n",
    "            logger.error(f\"No se encontraron los archivos JPG y JGW extra√≠dos para {id}.\")\n",
    "            return None, None\n",
    "\n",
    "        logger.debug(f\"Archivo ZIP extra√≠do de {id} correctamente. JPG: {jpg_file}, JGW: {jgw_file}\")\n",
    "        return jpg_file, jgw_file\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error extracting files for {id}: {str(e)}\")\n",
    "        return None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def upload_img_to_s3(jpg_path: str, jgw_path: str, id: str) -> bool:\n",
    "    \"\"\"\n",
    "    Sube los archivos a Scaleway Object Storage usando boto3.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Get S3 client\n",
    "        s3_client = S3Client.get_instance().client\n",
    "        \n",
    "        # Upload JPG file\n",
    "        with open(jpg_path, 'rb') as jpg_file:\n",
    "            s3_client.upload_fileobj(\n",
    "                jpg_file,\n",
    "                S3_BUCKET,\n",
    "                f\"{S3_BUCKET_IMAGES_PATH}/{id}.jpg\"\n",
    "            )\n",
    "            \n",
    "        # Upload JGW file\n",
    "        with open(jgw_path, 'rb') as jgw_file:\n",
    "            s3_client.upload_fileobj(\n",
    "                jgw_file,\n",
    "                S3_BUCKET,\n",
    "                f\"{S3_BUCKET_METADATA_PATH}/{id}.jgw\"\n",
    "            )\n",
    "\n",
    "        logger.debug(f\"Files uploaded to Scaleway: {id}\")            \n",
    "        return True\n",
    "        \n",
    "    except ClientError as e:\n",
    "        logger.error(f\"Error uploading to Scaleway: {str(e)}\")\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_width_height_mongoDB(id: str, jpg_path: str) -> bool:\n",
    "    \"\"\"\n",
    "    Actualiza la base de datos MongoDB con el ancho y alto de la imagen JPG.\n",
    "\n",
    "    Args:\n",
    "        id (str): Identificador de la imagen.\n",
    "        jpg_path (str): Ruta del archivo JPG.\n",
    "\n",
    "    Returns:\n",
    "        bool: True si se actualiz√≥ correctamente, False en caso contrario.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Obtener las dimensiones de la imagen\n",
    "        img = cv2.imread(jpg_path)\n",
    "        height, width = img.shape[:2]\n",
    "\n",
    "        # Actualizar en MongoDB\n",
    "        imagenes_db = MongoDB.get_instance().db.imagenes\n",
    "        imagenes_db.update_one({\"id\": id}, {\"$set\": {\"width\": width, \"height\": height}})\n",
    "\n",
    "        logger.debug(f\"Updated width and height for {id} in MongoDB.\")\n",
    "        return True\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error updating width and height for {id}: {str(e)}\")\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_jgw_data(jgw_path: str) -> dict:\n",
    "    \"\"\"\n",
    "    Lee un archivo JGW y devuelve un diccionario con los datos.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with open(jgw_path, \"r\") as jgw_file:\n",
    "            lines = jgw_file.readlines()\n",
    "            data = {\n",
    "                \"x_pixel_size\": float(lines[0].strip()),\n",
    "                \"y_rotation\": float(lines[1].strip()),\n",
    "                \"x_rotation\": float(lines[2].strip()),\n",
    "                \"y_pixel_size\": float(lines[3].strip()),\n",
    "                \"x_origin\": float(lines[4].strip()),\n",
    "                \"y_origin\": float(lines[5].strip())\n",
    "            }\n",
    "            logger.debug(f\"JGW file read successfully: {jgw_path}\")\n",
    "            return data\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error reading JGW file: {str(e)}\")\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_mongodb(id: str, jgw_data: dict) -> bool:\n",
    "    \"\"\"\n",
    "    Agrega metadatos de la imagen a MongoDB y actualiza el estado de descarga a True.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        imagenes_db = MongoDB.get_instance().db.imagenes\n",
    "        result = imagenes_db.update_one(\n",
    "            {\"id\": id},\n",
    "            {\"$set\": {\n",
    "                \"downloaded\": True,\n",
    "                \"jgw_data\": jgw_data,\n",
    "                \"downloaded_date\": datetime.datetime.now()\n",
    "            }}\n",
    "        )\n",
    "        if result.modified_count > 0:\n",
    "            logger.debug(f\"MongoDB updated: {id}\")\n",
    "            return True\n",
    "        else:\n",
    "            logger.warning(f\"MongoDB not updated: {id}\")\n",
    "            return True\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error updating MongoDB: {str(e)}\")\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_up_directory(dir_path: str) -> None:\n",
    "    \"\"\"\n",
    "    Elimina todo el contenido dentro del directorio especificado.\n",
    "    Si ocurre alg√∫n error, lanza la excepci√≥n correspondiente.\n",
    "    \n",
    "    Args:\n",
    "        dir_path (str): Ruta al directorio que se desea limpiar\n",
    "        \n",
    "    Returns:\n",
    "        None\n",
    "            \n",
    "    Raises:\n",
    "        NotADirectoryError: Si la ruta existe pero no es un directorio\n",
    "        PermissionError: Si no hay permisos suficientes\n",
    "        OSError: Para otros errores del sistema operativo\n",
    "    \"\"\"\n",
    "    # Convertir a Path para mejor manejo\n",
    "    path = Path(dir_path)\n",
    "    \n",
    "    # Verificar si el directorio existe\n",
    "    if not path.exists():\n",
    "        logging.info(f\"El directorio {dir_path} no existe. No se requiere limpieza.\")\n",
    "        return\n",
    "        \n",
    "    # Verificar si es un directorio\n",
    "    if not path.is_dir():\n",
    "        raise NotADirectoryError(f\"La ruta {dir_path} no es un directorio\")\n",
    "        \n",
    "    # Verificar permisos\n",
    "    if not os.access(path, os.W_OK):\n",
    "        raise PermissionError(f\"Sin permisos de escritura en {dir_path}\")\n",
    "\n",
    "    # Eliminar contenido\n",
    "    for item in path.iterdir():\n",
    "        if item.is_file():\n",
    "            item.unlink()\n",
    "        elif item.is_dir():\n",
    "            shutil.rmtree(item)\n",
    "\n",
    "    logging.debug(f\"Directorio {dir_path} limpiado exitosamente\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanup_files(*file_paths: str) -> None:\n",
    "    \"\"\"\n",
    "    Removes temporary files after processing.\n",
    "    \"\"\"\n",
    "    for file_path in file_paths:\n",
    "        try:\n",
    "            if file_path and os.path.exists(file_path):\n",
    "                os.remove(file_path)\n",
    "        except Exception as e:\n",
    "            logger.warning(f\"Error cleaning up file {file_path}: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_white_image(image, threshold_percent=50, white_threshold=250):\n",
    "    \"\"\"\n",
    "    Detecta im√°genes blancas bas√°ndose en un porcentaje de p√≠xeles blancos.\n",
    "    \n",
    "    Args:\n",
    "        image: Imagen en formato BGR (OpenCV)\n",
    "        threshold_percent: Porcentaje de blanco requerido\n",
    "        white_threshold: Valor m√≠nimo para considerar un pixel como blanco (0-255)\n",
    "    \n",
    "    Returns:\n",
    "        bool: True si la imagen es considerada blanca\n",
    "        float: Porcentaje de blanco en la imagen\n",
    "    \"\"\"\n",
    "    # Convertir a HSV para mejor manejo del brillo\n",
    "    hsv = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)\n",
    "    \n",
    "    # Criterios para considerar un pixel como blanco\n",
    "    white_mask = cv2.inRange(hsv, \n",
    "                            np.array([0, 0, white_threshold]),  # M√≠nimo HSV\n",
    "                            np.array([180, 30, 255]))  # M√°ximo HSV\n",
    "\n",
    "    total_pixels = image.shape[0] * image.shape[1]\n",
    "    white_pixel_count = cv2.countNonZero(white_mask)\n",
    "    white_percentage = (white_pixel_count / total_pixels) * 100\n",
    "\n",
    "    return white_percentage > threshold_percent, white_percentage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_image_with_overlap(id: str, jpg_path: str, tile_size: Tuple[int, int], overlap: int, purge_white_images: bool) -> Tuple[list, str]:\n",
    "    threshold_percent = 60\n",
    "    white_threshold = 200\n",
    "    # Cargar la imagen\n",
    "    image = cv2.imread(jpg_path)\n",
    "    height, width = image.shape[:2]\n",
    "\n",
    "    # Crear el directorio de salida si no existe\n",
    "    output_dir = os.path.join(PATCHES_PATH, id)\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    metadata = []\n",
    "    patch_id = 1\n",
    "\n",
    "    # Recorrer la imagen en bloques con superposici√≥n\n",
    "    for y in range(0, height, tile_size[1] - overlap):\n",
    "        for x in range(0, width, tile_size[0] - overlap):\n",
    "            # Definir las coordenadas del recorte\n",
    "            x_end = min(x + tile_size[0], width)\n",
    "            y_end = min(y + tile_size[1], height)\n",
    "\n",
    "            # Recortar la regi√≥n\n",
    "            tile = image[y:y_end, x:x_end]\n",
    "\n",
    "            tile_name = f\"{id}_patch_{patch_id}\"\n",
    "            \n",
    "            # Chequear si la mayor√≠a de la imagen es blanca.\n",
    "            img_is_white = is_white_image(tile, threshold_percent, white_threshold)[0]\n",
    "\n",
    "            # Si la mayor√≠a es balnca, y est√° activada la opci√≥n, no guardar la imagen.\n",
    "            if purge_white_images:\n",
    "                if not img_is_white:\n",
    "                    cv2.imwrite(os.path.join(output_dir, tile_name), tile)\n",
    "            else:\n",
    "                cv2.imwrite(os.path.join(output_dir, tile_name), tile)            \n",
    "\n",
    "            # Guardar los metadatos\n",
    "            metadata.append({\n",
    "                \"patch_id\": patch_id,\n",
    "                \"patch_name\": tile_name,\n",
    "                \"x_start\": x,\n",
    "                \"y_start\": y,\n",
    "                \"x_end\": x_end,\n",
    "                \"y_end\": y_end,\n",
    "                \"width\": x_end - x,\n",
    "                \"height\": y_end - y,\n",
    "                \"is_white\": img_is_white,\n",
    "                \"white_threeshold_percent\": threshold_percent,\n",
    "                \"white_threeshold_value\": white_threshold\n",
    "            })\n",
    "\n",
    "            patch_id += 1\n",
    "\n",
    "    return metadata, output_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def upload_patches_to_s3(id: str, patches_dir: str) -> bool:\n",
    "    \"\"\"\n",
    "    Sube los parches a Scaleway Object Storage usando boto3.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Get S3 client\n",
    "        s3_client = S3Client.get_instance().client\n",
    "\n",
    "        # Listar los archivos en el directorio\n",
    "        for patch in os.listdir(patches_dir):\n",
    "            patch_path = os.path.join(patches_dir, patch)\n",
    "            with open(patch_path, \"rb\") as patch_file:\n",
    "                s3_client.upload_fileobj(\n",
    "                    patch_file, S3_BUCKET, f\"{S3_BUCKET_PATCHES_PATH}/{id}/{patch}\"\n",
    "                )\n",
    "\n",
    "        logger.debug(f\"Patches uploaded to Scaleway: {id}\")\n",
    "        return True\n",
    "\n",
    "    except ClientError as e:\n",
    "        logger.error(f\"Error uploading patches to Scaleway: {str(e)}\")\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_mongodb_patches(id: str, patches: list) -> bool:\n",
    "    \"\"\"\n",
    "    Agrega metadatos de los parches a MongoDB.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        imagenes_db = MongoDB.get_instance().db.imagenes\n",
    "        result = imagenes_db.update_one(\n",
    "            {\"id\": id},\n",
    "            {\"$set\": {\n",
    "                \"patches\": patches,\n",
    "                \"patches_uploaded\": True\n",
    "            }}\n",
    "        )\n",
    "        if result.modified_count > 0:\n",
    "            logger.debug(f\"MongoDB updated with patches: {id}\")\n",
    "            return True\n",
    "        else:\n",
    "            logger.warning(f\"MongoDB not updated with patches: {id}\")\n",
    "            return True\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error updating MongoDB with patches: {str(e)}\")\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageProcessingError(Exception):\n",
    "    \"\"\"Excepci√≥n personalizada para errores en el procesamiento de im√°genes\"\"\"\n",
    "\n",
    "    pass\n",
    "\n",
    "\n",
    "def process_and_upload_image(\n",
    "    id: str, patches: bool = False, clean: bool = True\n",
    ") -> bool:\n",
    "    \"\"\"\n",
    "    Downloads, extracts and uploads drone images to S3-Compliant.\n",
    "\n",
    "    Args:\n",
    "        download_id (str): The ID of the image to download\n",
    "\n",
    "    Returns:\n",
    "        bool: True if processing was successful, False otherwise\n",
    "\n",
    "    Raises:\n",
    "        ImageProcessingError: When any step in the process fails\n",
    "    \"\"\"\n",
    "    logger.info(f\"Starting processing for download_id: {id}\")\n",
    "\n",
    "    try:\n",
    "        # Download the ZIP file\n",
    "        zip_path = download_picture(id)\n",
    "        if not zip_path:\n",
    "            raise ImageProcessingError(f\"Failed to download ZIP for {id}\")\n",
    "        logger.info(f\"ZIP downloaded for {id}\")\n",
    "\n",
    "        # Extract files\n",
    "        jpg_path, jgw_path = extract_files(zip_path, id)\n",
    "        if not jpg_path or not jgw_path:\n",
    "            raise ImageProcessingError(f\"Failed to extract files from ZIP for {id}\")\n",
    "        logger.info(f\"Files extracted for {id}\")\n",
    "\n",
    "        # Upload to S3\n",
    "        upload_img_to_s3_result = upload_img_to_s3(jpg_path, jgw_path, id)\n",
    "        if not upload_img_to_s3_result:\n",
    "            raise ImageProcessingError(f\"Failed to upload files to S3 for {id}\")\n",
    "        logger.info(f\"Files uploaded to S3 for {id}\")\n",
    "\n",
    "        # Add witdth and height to metadata\n",
    "        has_updated_mongodb_ok = update_width_height_mongoDB(id, jpg_path)\n",
    "        if not has_updated_mongodb_ok:\n",
    "            raise ImageProcessingError(\n",
    "                f\"Failed to update MongoDB with width and height for {id}\"\n",
    "            )\n",
    "        logger.info(f\"MongoDB updated with width and height for {id}\")\n",
    "\n",
    "        jgw_data = read_jgw_data(jgw_path)\n",
    "        if not jgw_data:\n",
    "            raise ImageProcessingError(f\"Failed to read JGW data for {id}\")\n",
    "        logger.info(f\"JGW data read for {id}\")\n",
    "\n",
    "        if patches:\n",
    "            metadata, output_dir = split_image_with_overlap(\n",
    "                id,\n",
    "                jpg_path,\n",
    "                TILE_SIZE,\n",
    "                OVER_LAP,\n",
    "                PURGE_WHITE_IMAGES,\n",
    "            )\n",
    "            if not metadata:\n",
    "                raise ImageProcessingError(f\"Failed to split image for {id}\")\n",
    "            logger.info(f\"Image split for {id}\")\n",
    "\n",
    "            upload_patches_to_s3_result = upload_patches_to_s3(id, output_dir)\n",
    "            if not upload_patches_to_s3_result:\n",
    "                raise ImageProcessingError(f\"Failed to upload patches to S3 for {id}\")\n",
    "            logger.info(f\"Patches uploaded to S3 for {id}\")\n",
    "\n",
    "            update_mongodb_patches_result = update_mongodb_patches(id, metadata)\n",
    "            if not update_mongodb_patches_result:\n",
    "                raise ImageProcessingError(f\"Failed to update MongoDB patches for {id}\")\n",
    "            logger.info(f\"MongoDB updated with patches for {id}\")\n",
    "\n",
    "        # Update MongoDB downloaded\n",
    "        update_mongodb_result = update_mongodb(id, jgw_data)\n",
    "        if not update_mongodb_result:\n",
    "            raise ImageProcessingError(f\"Failed to update MongoDB downloaded for {id}\")\n",
    "        logger.info(f\"MongoDB updated for {id}\")\n",
    "\n",
    "        return True\n",
    "\n",
    "    except ImageProcessingError as e:\n",
    "        logger.error(str(e))\n",
    "        return False\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Unexpected error processing download_id {id}: {str(e)}\")\n",
    "        return False\n",
    "    finally:\n",
    "        try:\n",
    "            if clean:\n",
    "                clean_up_directory(DOWNLOAD_PATH)\n",
    "                logger.info(f\"Directory cleaned up for {id}\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error cleaning up directory for {id}: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_and_upload_patches(id: str, clean: bool = False) -> bool:\n",
    "    \"\"\"Descarga la imagen desde el bucket de S3, la divide en parches y los sube a S3. Finalmente, actualiza \n",
    "    la base de datos con los metadatos de los parches.\n",
    "\n",
    "    Args:\n",
    "        id (str): ID de la imagen a procesar.\n",
    "        clean (bool): Indica si se debe limpiar el directorio de descargas al finalizar el procesamiento.\n",
    "\n",
    "    Returns:\n",
    "        bool: True si el procesamiento fue exitoso, False en caso contrario.\n",
    "    \"\"\"\n",
    "    logger.info(f\"Starting processing for download_id: {id}\")\n",
    "\n",
    "    try:\n",
    "        # Download the image from S3\n",
    "        jpg_path = os.path.join(PATCHES_PATH, f\"{id}.jpg\")\n",
    "\n",
    "        s3_client = S3Client.get_instance().client\n",
    "        s3_client.download_file(S3_BUCKET, f\"{S3_BUCKET_IMAGES_PATH}/{id}.jpg\", jpg_path)\n",
    "        logger.info(f\"Image downloaded from S3 for {id}\")\n",
    "\n",
    "        # Split image into patches\n",
    "        metadata, output_dir = split_image_with_overlap(\n",
    "            id,\n",
    "            jpg_path,\n",
    "            TILE_SIZE,\n",
    "            OVER_LAP,\n",
    "            PURGE_WHITE_IMAGES,\n",
    "        )\n",
    "        if not metadata:\n",
    "            raise ImageProcessingError(f\"Failed to split image for {id}\")\n",
    "        logger.info(f\"Image split for {id}\")\n",
    "\n",
    "        # Upload patches to S3\n",
    "        upload_patches_to_s3_result = upload_patches_to_s3(id, output_dir)\n",
    "        if not upload_patches_to_s3_result:\n",
    "            raise ImageProcessingError(f\"Failed to upload patches to S3 for {id}\")\n",
    "        logger.info(f\"Patches uploaded to S3 for {id}\")\n",
    "\n",
    "        # Update MongoDB with patches\n",
    "        update_mongodb_patches_result = update_mongodb_patches(id, metadata)\n",
    "        if not update_mongodb_patches_result:\n",
    "            raise ImageProcessingError(f\"Failed to update MongoDB patches for {id}\")\n",
    "        logger.info(f\"MongoDB updated with patches for {id}\")\n",
    "\n",
    "        return True\n",
    "\n",
    "    except ImageProcessingError as e:\n",
    "        logger.error(str(e))\n",
    "        return False\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Unexpected error processing download_id {id}: {str(e)}\")\n",
    "        return False\n",
    "    finally:\n",
    "        try:\n",
    "            if clean:\n",
    "                clean_up_directory(DOWNLOAD_PATH)\n",
    "                logger.info(f\"Directory cleaned up for {id}\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error cleaning up directory for {id}: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_images() -> list:\n",
    "    \"\"\"\n",
    "    Returns all images from MongoDB.\n",
    "    \"\"\"\n",
    "    imagenes_db = MongoDB.get_instance().db.imagenes\n",
    "    return list(imagenes_db.find())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_log_to_file():\n",
    "    logger = logging.getLogger()\n",
    "    today = datetime.date.today()\n",
    "\n",
    "    log_file = f\"webscrapping_{today}.log\"\n",
    "    formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(funcName)s - %(message)s')\n",
    "    file_handler = logging.FileHandler(log_file)\n",
    "    file_handler.setFormatter(formatter)\n",
    "\n",
    "    logger.addHandler(file_handler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imprimir todas las variables de entorno para debug\n",
    "# logger.info(\"Variables de entorno:\")\n",
    "# for key, value in os.environ.items():\n",
    "#     logger.info(f\"{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-18 12:43:45,348 - root - INFO - <module> - Connected to MongoDB\n",
      "2025-03-18 12:43:45,356 - root - INFO - <module> - Connected to S3\n",
      "2025-03-18 12:43:45,360 - root - INFO - <module> - Starting processing and uploading images\n",
      "  0%|          | 0/111 [00:00<?, ?it/s]2025-03-18 12:43:45,362 - root - INFO - <module> - Processing and uploading image Barrio6dediciembre_20211210_dji_pc_5c\n",
      "2025-03-18 12:43:45,362 - root - INFO - process_and_upload_image - Starting processing for download_id: Barrio6dediciembre_20211210_dji_pc_5c\n",
      "2025-03-18 12:43:54,262 - root - INFO - process_and_upload_image - ZIP downloaded for Barrio6dediciembre_20211210_dji_pc_5c\n",
      "2025-03-18 12:43:54,421 - root - INFO - process_and_upload_image - Files extracted for Barrio6dediciembre_20211210_dji_pc_5c\n",
      "2025-03-18 12:43:57,446 - root - INFO - process_and_upload_image - Files uploaded to S3 for Barrio6dediciembre_20211210_dji_pc_5c\n",
      "2025-03-18 12:43:57,447 - root - INFO - process_and_upload_image - JGW data read for Barrio6dediciembre_20211210_dji_pc_5c\n",
      "2025-03-18 12:43:59,298 - root - INFO - process_and_upload_image - Image split for Barrio6dediciembre_20211210_dji_pc_5c\n",
      "2025-03-18 12:44:03,061 - root - INFO - process_and_upload_image - Patches uploaded to S3 for Barrio6dediciembre_20211210_dji_pc_5c\n",
      "2025-03-18 12:44:03,063 - root - INFO - process_and_upload_image - MongoDB updated with patches for Barrio6dediciembre_20211210_dji_pc_5c\n",
      "2025-03-18 12:44:03,065 - root - INFO - process_and_upload_image - MongoDB updated for Barrio6dediciembre_20211210_dji_pc_5c\n",
      "2025-03-18 12:44:03,078 - root - INFO - process_and_upload_image - Directory cleaned up for Barrio6dediciembre_20211210_dji_pc_5c\n",
      " 10%|‚ñâ         | 11/111 [00:17<02:41,  1.61s/it]2025-03-18 12:44:03,079 - root - INFO - <module> - Processing and uploading image BarrioAquilesLanza_20230815_dji_pc_5cm\n",
      "2025-03-18 12:44:03,080 - root - INFO - process_and_upload_image - Starting processing for download_id: BarrioAquilesLanza_20230815_dji_pc_5cm\n",
      "2025-03-18 12:44:12,187 - root - INFO - process_and_upload_image - ZIP downloaded for BarrioAquilesLanza_20230815_dji_pc_5cm\n",
      "2025-03-18 12:44:12,323 - root - INFO - process_and_upload_image - Files extracted for BarrioAquilesLanza_20230815_dji_pc_5cm\n",
      "2025-03-18 12:44:14,963 - root - INFO - process_and_upload_image - Files uploaded to S3 for BarrioAquilesLanza_20230815_dji_pc_5cm\n",
      "2025-03-18 12:44:14,964 - root - INFO - process_and_upload_image - JGW data read for BarrioAquilesLanza_20230815_dji_pc_5cm\n",
      "2025-03-18 12:44:16,689 - root - INFO - process_and_upload_image - Image split for BarrioAquilesLanza_20230815_dji_pc_5cm\n",
      "2025-03-18 12:44:20,060 - root - INFO - process_and_upload_image - Patches uploaded to S3 for BarrioAquilesLanza_20230815_dji_pc_5cm\n",
      "2025-03-18 12:44:20,062 - root - INFO - process_and_upload_image - MongoDB updated with patches for BarrioAquilesLanza_20230815_dji_pc_5cm\n",
      "2025-03-18 12:44:20,064 - root - INFO - process_and_upload_image - MongoDB updated for BarrioAquilesLanza_20230815_dji_pc_5cm\n",
      "2025-03-18 12:44:20,078 - root - INFO - process_and_upload_image - Directory cleaned up for BarrioAquilesLanza_20230815_dji_pc_5cm\n",
      " 11%|‚ñà         | 12/111 [00:34<05:34,  3.38s/it]2025-03-18 12:44:20,080 - root - INFO - <module> - Processing and uploading image barrioAsocCivilEsperanza_20190531_dji_pc_5cm\n",
      "2025-03-18 12:44:20,080 - root - INFO - process_and_upload_image - Starting processing for download_id: barrioAsocCivilEsperanza_20190531_dji_pc_5cm\n",
      "2025-03-18 12:44:20,207 - root - INFO - process_and_upload_image - ZIP downloaded for barrioAsocCivilEsperanza_20190531_dji_pc_5cm\n",
      "2025-03-18 12:44:20,217 - root - INFO - process_and_upload_image - Files extracted for barrioAsocCivilEsperanza_20190531_dji_pc_5cm\n",
      "2025-03-18 12:44:20,251 - root - INFO - process_and_upload_image - Files uploaded to S3 for barrioAsocCivilEsperanza_20190531_dji_pc_5cm\n",
      "2025-03-18 12:44:20,252 - root - ERROR - read_jgw_data - Error reading JGW file: list index out of range\n",
      "2025-03-18 12:44:20,252 - root - ERROR - process_and_upload_image - Failed to read JGW data for barrioAsocCivilEsperanza_20190531_dji_pc_5cm\n",
      "2025-03-18 12:44:20,253 - root - INFO - process_and_upload_image - Directory cleaned up for barrioAsocCivilEsperanza_20190531_dji_pc_5cm\n",
      " 12%|‚ñà‚ñè        | 13/111 [00:34<04:46,  2.93s/it]2025-03-18 12:44:20,254 - root - INFO - <module> - Processing and uploading image BarrioBoixYMerino_20230928_dji_pc_5cm\n",
      "2025-03-18 12:44:20,255 - root - INFO - process_and_upload_image - Starting processing for download_id: BarrioBoixYMerino_20230928_dji_pc_5cm\n",
      "2025-03-18 12:44:34,461 - root - INFO - process_and_upload_image - ZIP downloaded for BarrioBoixYMerino_20230928_dji_pc_5cm\n",
      "2025-03-18 12:44:34,679 - root - INFO - process_and_upload_image - Files extracted for BarrioBoixYMerino_20230928_dji_pc_5cm\n",
      "2025-03-18 12:44:39,536 - root - INFO - process_and_upload_image - Files uploaded to S3 for BarrioBoixYMerino_20230928_dji_pc_5cm\n",
      "2025-03-18 12:44:39,537 - root - INFO - process_and_upload_image - JGW data read for BarrioBoixYMerino_20230928_dji_pc_5cm\n",
      "2025-03-18 12:44:42,096 - root - INFO - process_and_upload_image - Image split for BarrioBoixYMerino_20230928_dji_pc_5cm\n",
      "2025-03-18 12:44:48,651 - root - INFO - process_and_upload_image - Patches uploaded to S3 for BarrioBoixYMerino_20230928_dji_pc_5cm\n",
      "2025-03-18 12:44:48,653 - root - INFO - process_and_upload_image - MongoDB updated with patches for BarrioBoixYMerino_20230928_dji_pc_5cm\n",
      "2025-03-18 12:44:48,654 - root - INFO - process_and_upload_image - MongoDB updated for BarrioBoixYMerino_20230928_dji_pc_5cm\n",
      "2025-03-18 12:44:48,676 - root - INFO - process_and_upload_image - Directory cleaned up for BarrioBoixYMerino_20230928_dji_pc_5cm\n",
      " 13%|‚ñà‚ñé        | 14/111 [01:03<11:38,  7.20s/it]2025-03-18 12:44:48,677 - root - INFO - <module> - Processing and uploading image barrioBrazosUnidos_20171129_a_ajust_5cm\n",
      "2025-03-18 12:44:48,678 - root - INFO - process_and_upload_image - Starting processing for download_id: barrioBrazosUnidos_20171129_a_ajust_5cm\n",
      "2025-03-18 12:44:48,809 - root - INFO - process_and_upload_image - ZIP downloaded for barrioBrazosUnidos_20171129_a_ajust_5cm\n",
      "2025-03-18 12:44:48,819 - root - INFO - process_and_upload_image - Files extracted for barrioBrazosUnidos_20171129_a_ajust_5cm\n",
      "2025-03-18 12:44:48,853 - root - INFO - process_and_upload_image - Files uploaded to S3 for barrioBrazosUnidos_20171129_a_ajust_5cm\n",
      "2025-03-18 12:44:48,854 - root - ERROR - read_jgw_data - Error reading JGW file: list index out of range\n",
      "2025-03-18 12:44:48,854 - root - ERROR - process_and_upload_image - Failed to read JGW data for barrioBrazosUnidos_20171129_a_ajust_5cm\n",
      "2025-03-18 12:44:48,855 - root - INFO - process_and_upload_image - Directory cleaned up for barrioBrazosUnidos_20171129_a_ajust_5cm\n",
      " 14%|‚ñà‚ñé        | 15/111 [01:03<09:21,  5.85s/it]2025-03-18 12:44:48,856 - root - INFO - <module> - Processing and uploading image barrioCachimbaDelPiojo_20180801_dji_pc_3cm\n",
      "2025-03-18 12:44:48,857 - root - INFO - process_and_upload_image - Starting processing for download_id: barrioCachimbaDelPiojo_20180801_dji_pc_3cm\n",
      "2025-03-18 12:44:48,993 - root - INFO - process_and_upload_image - ZIP downloaded for barrioCachimbaDelPiojo_20180801_dji_pc_3cm\n",
      "2025-03-18 12:44:49,003 - root - INFO - process_and_upload_image - Files extracted for barrioCachimbaDelPiojo_20180801_dji_pc_3cm\n",
      "2025-03-18 12:44:49,042 - root - INFO - process_and_upload_image - Files uploaded to S3 for barrioCachimbaDelPiojo_20180801_dji_pc_3cm\n",
      "2025-03-18 12:44:49,043 - root - ERROR - read_jgw_data - Error reading JGW file: list index out of range\n",
      "2025-03-18 12:44:49,043 - root - ERROR - process_and_upload_image - Failed to read JGW data for barrioCachimbaDelPiojo_20180801_dji_pc_3cm\n",
      "2025-03-18 12:44:49,044 - root - INFO - process_and_upload_image - Directory cleaned up for barrioCachimbaDelPiojo_20180801_dji_pc_3cm\n",
      " 14%|‚ñà‚ñç        | 16/111 [01:03<07:19,  4.62s/it]2025-03-18 12:44:49,045 - root - INFO - <module> - Processing and uploading image barrioCampGalusso_20190621_dji_pc_5cm\n",
      "2025-03-18 12:44:49,046 - root - INFO - process_and_upload_image - Starting processing for download_id: barrioCampGalusso_20190621_dji_pc_5cm\n",
      "2025-03-18 12:44:54,790 - root - INFO - process_and_upload_image - ZIP downloaded for barrioCampGalusso_20190621_dji_pc_5cm\n",
      "2025-03-18 12:44:54,871 - root - INFO - process_and_upload_image - Files extracted for barrioCampGalusso_20190621_dji_pc_5cm\n",
      "2025-03-18 12:44:56,025 - root - INFO - process_and_upload_image - Files uploaded to S3 for barrioCampGalusso_20190621_dji_pc_5cm\n",
      "2025-03-18 12:44:56,026 - root - INFO - process_and_upload_image - JGW data read for barrioCampGalusso_20190621_dji_pc_5cm\n",
      "2025-03-18 12:44:56,663 - root - INFO - process_and_upload_image - Image split for barrioCampGalusso_20190621_dji_pc_5cm\n",
      "2025-03-18 12:44:58,747 - root - INFO - process_and_upload_image - Patches uploaded to S3 for barrioCampGalusso_20190621_dji_pc_5cm\n",
      "2025-03-18 12:44:58,750 - root - INFO - process_and_upload_image - MongoDB updated with patches for barrioCampGalusso_20190621_dji_pc_5cm\n",
      "2025-03-18 12:44:58,751 - root - INFO - process_and_upload_image - MongoDB updated for barrioCampGalusso_20190621_dji_pc_5cm\n",
      "2025-03-18 12:44:58,756 - root - INFO - process_and_upload_image - Directory cleaned up for barrioCampGalusso_20190621_dji_pc_5cm\n",
      " 15%|‚ñà‚ñå        | 17/111 [01:13<09:07,  5.82s/it]2025-03-18 12:44:58,758 - root - INFO - <module> - Processing and uploading image barrioCampichuelo_20191209_dji_pc_5cm\n",
      "2025-03-18 12:44:58,758 - root - INFO - process_and_upload_image - Starting processing for download_id: barrioCampichuelo_20191209_dji_pc_5cm\n",
      "2025-03-18 12:45:01,179 - root - INFO - process_and_upload_image - ZIP downloaded for barrioCampichuelo_20191209_dji_pc_5cm\n",
      "2025-03-18 12:45:01,226 - root - INFO - process_and_upload_image - Files extracted for barrioCampichuelo_20191209_dji_pc_5cm\n",
      "2025-03-18 12:45:01,651 - root - INFO - process_and_upload_image - Files uploaded to S3 for barrioCampichuelo_20191209_dji_pc_5cm\n",
      "2025-03-18 12:45:01,652 - root - INFO - process_and_upload_image - JGW data read for barrioCampichuelo_20191209_dji_pc_5cm\n",
      "2025-03-18 12:45:01,915 - root - INFO - process_and_upload_image - Image split for barrioCampichuelo_20191209_dji_pc_5cm\n",
      "2025-03-18 12:45:02,644 - root - INFO - process_and_upload_image - Patches uploaded to S3 for barrioCampichuelo_20191209_dji_pc_5cm\n",
      "2025-03-18 12:45:02,646 - root - INFO - process_and_upload_image - MongoDB updated with patches for barrioCampichuelo_20191209_dji_pc_5cm\n",
      "2025-03-18 12:45:02,647 - root - INFO - process_and_upload_image - MongoDB updated for barrioCampichuelo_20191209_dji_pc_5cm\n",
      "2025-03-18 12:45:02,650 - root - INFO - process_and_upload_image - Directory cleaned up for barrioCampichuelo_20191209_dji_pc_5cm\n",
      " 16%|‚ñà‚ñå        | 18/111 [01:17<08:16,  5.34s/it]2025-03-18 12:45:02,651 - root - INFO - <module> - Processing and uploading image barrioCauceglia_20190717_dji_pc_5cm\n",
      "2025-03-18 12:45:02,652 - root - INFO - process_and_upload_image - Starting processing for download_id: barrioCauceglia_20190717_dji_pc_5cm\n",
      "2025-03-18 12:45:26,271 - root - INFO - process_and_upload_image - ZIP downloaded for barrioCauceglia_20190717_dji_pc_5cm\n",
      "2025-03-18 12:45:26,585 - root - INFO - process_and_upload_image - Files extracted for barrioCauceglia_20190717_dji_pc_5cm\n",
      "2025-03-18 12:45:33,790 - root - INFO - process_and_upload_image - Files uploaded to S3 for barrioCauceglia_20190717_dji_pc_5cm\n",
      "2025-03-18 12:45:33,791 - root - INFO - process_and_upload_image - JGW data read for barrioCauceglia_20190717_dji_pc_5cm\n",
      "2025-03-18 12:45:38,461 - root - INFO - process_and_upload_image - Image split for barrioCauceglia_20190717_dji_pc_5cm\n",
      "2025-03-18 12:45:47,430 - root - INFO - process_and_upload_image - Patches uploaded to S3 for barrioCauceglia_20190717_dji_pc_5cm\n",
      "2025-03-18 12:45:47,433 - root - INFO - process_and_upload_image - MongoDB updated with patches for barrioCauceglia_20190717_dji_pc_5cm\n",
      "2025-03-18 12:45:47,435 - root - INFO - process_and_upload_image - MongoDB updated for barrioCauceglia_20190717_dji_pc_5cm\n",
      "2025-03-18 12:45:47,465 - root - INFO - process_and_upload_image - Directory cleaned up for barrioCauceglia_20190717_dji_pc_5cm\n",
      " 17%|‚ñà‚ñã        | 19/111 [02:02<24:13, 15.79s/it]2025-03-18 12:45:47,467 - root - INFO - <module> - Processing and uploading image BarrioCauceglia_20230808_dji_pc_5cm\n",
      "2025-03-18 12:45:47,467 - root - INFO - process_and_upload_image - Starting processing for download_id: BarrioCauceglia_20230808_dji_pc_5cm\n",
      "2025-03-18 12:46:02,343 - root - INFO - process_and_upload_image - ZIP downloaded for BarrioCauceglia_20230808_dji_pc_5cm\n",
      "2025-03-18 12:46:02,496 - root - INFO - process_and_upload_image - Files extracted for BarrioCauceglia_20230808_dji_pc_5cm\n",
      "2025-03-18 12:46:05,601 - root - INFO - process_and_upload_image - Files uploaded to S3 for BarrioCauceglia_20230808_dji_pc_5cm\n",
      "2025-03-18 12:46:05,602 - root - INFO - process_and_upload_image - JGW data read for BarrioCauceglia_20230808_dji_pc_5cm\n",
      "2025-03-18 12:46:07,182 - root - INFO - process_and_upload_image - Image split for BarrioCauceglia_20230808_dji_pc_5cm\n",
      "2025-03-18 12:46:11,118 - root - INFO - process_and_upload_image - Patches uploaded to S3 for BarrioCauceglia_20230808_dji_pc_5cm\n",
      "2025-03-18 12:46:11,120 - root - INFO - process_and_upload_image - MongoDB updated with patches for BarrioCauceglia_20230808_dji_pc_5cm\n",
      "2025-03-18 12:46:11,121 - root - INFO - process_and_upload_image - MongoDB updated for BarrioCauceglia_20230808_dji_pc_5cm\n",
      "2025-03-18 12:46:11,135 - root - INFO - process_and_upload_image - Directory cleaned up for BarrioCauceglia_20230808_dji_pc_5cm\n",
      " 18%|‚ñà‚ñä        | 20/111 [02:25<27:13, 17.96s/it]2025-03-18 12:46:11,136 - root - INFO - <module> - Processing and uploading image barrioCanadaMatildePacheco-Cont_20181228_dji_pc_5c\n",
      "2025-03-18 12:46:11,137 - root - INFO - process_and_upload_image - Starting processing for download_id: barrioCanadaMatildePacheco-Cont_20181228_dji_pc_5c\n",
      "2025-03-18 12:46:23,574 - root - INFO - process_and_upload_image - ZIP downloaded for barrioCanadaMatildePacheco-Cont_20181228_dji_pc_5c\n",
      "2025-03-18 12:46:23,692 - root - INFO - process_and_upload_image - Files extracted for barrioCanadaMatildePacheco-Cont_20181228_dji_pc_5c\n"
     ]
    }
   ],
   "source": [
    "# Pasos iniciales\n",
    "# Chequear conexi√≥n a MongoDB\n",
    "try:\n",
    "    MongoDB.get_instance()\n",
    "    # Listar las colecciones\n",
    "    collections = MongoDB.get_instance().db.list_collection_names()\n",
    "    logger.info(\"Connected to MongoDB\")\n",
    "except Exception as e:\n",
    "    logger.error(f\"Error connecting to MongoDB: {str(e)}\")\n",
    "    sys.exit(1)\n",
    "\n",
    "# Chequear conexi√≥n a S3\n",
    "try:\n",
    "    S3Client.get_instance()\n",
    "    # Listar los buckets\n",
    "    s3_client = S3Client.get_instance().client\n",
    "    response = s3_client.list_buckets()\n",
    "    logger.info(\"Connected to S3\")\n",
    "except Exception as e:\n",
    "    logger.error(f\"Error connecting to S3: {str(e)}\")\n",
    "    sys.exit(1)\n",
    "\n",
    "# logger.setLevel(logging.INFO)\n",
    "# results = get_pics_dict(get_pictures_list_html(get_toc_html()))\n",
    "# mapping = parse_js_cases(get_js_as_text())\n",
    "\n",
    "# output = add_or_sync_downloaded(add_file_download_id(results, mapping))\n",
    "\n",
    "# Set log to file\n",
    "set_log_to_file()\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "# Get all images from MongoDB\n",
    "images = get_all_images()\n",
    "\n",
    "# Process and upload images with tqdm\n",
    "logger.info(\"Starting processing and uploading images\")\n",
    "for image in tqdm(images):\n",
    "    if image[\"downloaded\"]:\n",
    "        logger.debug(f\"Skipping image {image['id']} as it is already downloaded\")\n",
    "    else:\n",
    "        logger.info(f\"Processing and uploading image {image['id']}\")\n",
    "        process_and_upload_image(image[\"id\"], patches=True, clean=True)\n",
    "logger.info(\"Finished processing and uploading images\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
