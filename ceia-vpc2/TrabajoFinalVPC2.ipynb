{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <div align=\"center\"><b> Trabajo Final - Visi√≥n por computadora 2  - CEIA </b></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"right\">üìù <em><small><font color='Gray'>Nota:</font></small></em></div>\n",
    "\n",
    "<div align=\"right\"> <em><small><font color='Gray'> La funcionalidad de visualizaci√≥n de jupyter notebooks en <a href=\"https://github.com/\" target=\"_blank\">github</a> es solamente un preview.</font></small></em> </div>\n",
    "\n",
    "<div align=\"right\"> <em><small><font color='Gray'> Para mejor visualizaci√≥n se sugiere utilizar el visualizador recomndado por la comunidad: <a href=\"https://nbviewer.org/\" target=\"_blank\">nbviewer</a></font></small></em> </div>\n",
    "\n",
    "<div align=\"right\"> <em><small><font color='Gray'> Puedes a acceder al sigiente enlace para ver este notebook en dicha p√°gina: <a href=\"https://nbviewer.org/github/brunomaso1/uba-ceia/blob/ceia-nlp/ceia-nlp/Desafio%201.ipynb\">Desafio 1</a></font></small></em> </div> \n",
    "\n",
    "<!-- TODO: Arreglar el link -->\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* * *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<style>\n",
    "/* Limitar la altura de las celdas de salida en html */\n",
    ".jp-OutputArea.jp-Cell-outputArea {\n",
    "    max-height: 500px;\n",
    "}\n",
    "</style>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- Colab -->\n",
    "<!-- <div align=\"center\"><img src=\"https://drive.google.com/uc?export=view&id=1tUnWeK372o_yu0LxkMwtC4IR2jorDipN\" width=\"600\" alt=\"Figura 1: Detecci√≥n de palmeras mediante modelos de aprendizaje profundo.\"></div> -->\n",
    "\n",
    "<div align=\"center\"><img src=\"./resources/portada.jpeg\" width=\"600\" alt=\"Figura 1: Detecci√≥n de palmeras mediante modelos de aprendizaje profundo.\"></div>\n",
    "\n",
    "<div align=\"center\"><small><em>Figura 1: Detecci√≥n de palmeras mediante modelos de aprendizaje profundo.</em></small></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"center\">‚ú®Datos del proyecto:‚ú®</div>\n",
    "\n",
    "<p></p>\n",
    "\n",
    "<div align=\"center\">\n",
    "\n",
    "| Subtitulo       | Trabajo final - VPC2 - FIUBA                                                                                                           |\n",
    "| --------------- | -------------------------------------------------------------------------------------------------------------------------------------- |\n",
    "| **Descrpci√≥n**  | Palm tree detector                                                                                                                     |\n",
    "| **Integrantes** | - Bruno Masoller (brunomaso1@gmail.com) </br> - Sim√≥n Rodriguez (simon.andre.r@gmail.com)                                              |\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚úã <em><font color='DodgerBlue'>Importaciones:</font></em> ‚úã"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inicialmente, cargamos los datos desde Google Drive (si no est√° disponible el link, se puede desacargar desde Kaggle tambi√©n -hay informaci√≥n de esto m√°s adelante-):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Mejorar comentarios\n",
    "# TODO: Poner funciones dentro de los tags correspondientes\n",
    "# TODO: Refactoring de funciones\n",
    "# TODO: Codigo repetido?\n",
    "# TODO: Organizar importaciones\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import warnings\n",
    "import zipfile\n",
    "import torch\n",
    "import torchvision\n",
    "from torchvision.transforms import functional as F\n",
    "from torchvision.transforms import v2 as T\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "import xml.etree.ElementTree as ET\n",
    "import cv2\n",
    "import fiftyone as fo\n",
    "import shutil\n",
    "import random\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision.io import read_image\n",
    "from torchvision import tv_tensors\n",
    "from torchinfo import summary\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "DOWNLOAD_RESOURCES = False # Permite indicar si descargar el conjunto o no\n",
    "RUTA_CARPETA_RESOURCES = './resources'\n",
    "RUTA_ARCHIVO_REQUERIMIENTOS = os.path.join(RUTA_CARPETA_RESOURCES, 'requirements.txt')\n",
    "RUTA_CARPETA_UTILS = os.path.join(RUTA_CARPETA_RESOURCES, 'utils')\n",
    "if DOWNLOAD_RESOURCES:\n",
    "    print(\"Descargando recursos...\")\n",
    "    %pip install gdown\n",
    "    !gdown https://drive.google.com/drive/folders/1DFfgoGQ8-zLwQU85Uyq0X72GX4IITInV?usp=sharing --folder\n",
    "\n",
    "# Verificamos si la carpeta ha sido descargada correctamente\n",
    "if os.path.exists(RUTA_CARPETA_RESOURCES):\n",
    "    print(f\"La carpeta {RUTA_CARPETA_RESOURCES} fue encontrada.\")\n",
    "\n",
    "    # Verificamos si el archivo de requerimientos existe\n",
    "    if os.path.exists(RUTA_ARCHIVO_REQUERIMIENTOS):\n",
    "        print(\"Archivo de requerimientos encontrado, instalando dependencias...\")\n",
    "        %pip install -r {RUTA_ARCHIVO_REQUERIMIENTOS}\n",
    "    else:\n",
    "        warnings.warn(f\"El archivo {RUTA_ARCHIVO_REQUERIMIENTOS} no fue encontrado. No se instalar√°n los requerimientos.\")\n",
    "\n",
    "    if os.path.exists(RUTA_CARPETA_UTILS):\n",
    "        sys.path.append(os.path.abspath('./resources/utils'))\n",
    "        print(f\"Se agreg√≥ la ruta {RUTA_CARPETA_UTILS} al system path.\")\n",
    "    else:\n",
    "        raise FileNotFoundError(f\"La carpeta {RUTA_CARPETA_UTILS} no fue encontrada. Verifique la descarga.\")\n",
    "else:\n",
    "    raise FileNotFoundError(f\"La carpeta {RUTA_CARPETA_RESOURCES} no fue encontrada. Verifique la descarga.\")\n",
    "\n",
    "from engine import train_one_epoch, evaluate # type: ignore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- TODO: arreglar esto -->\n",
    "<code>\n",
    "os.system(\"wget https://raw.githubusercontent.com/pytorch/vision/main/references/detection/engine.py\")\n",
    "os.system(\"wget https://raw.githubusercontent.com/pytorch/vision/main/references/detection/utils.py\")\n",
    "os.system(\"wget https://raw.githubusercontent.com/pytorch/vision/main/references/detection/coco_utils.py\")\n",
    "os.system(\"wget https://raw.githubusercontent.com/pytorch/vision/main/references/detection/coco_eval.py\")\n",
    "os.system(\"wget https://raw.githubusercontent.com/pytorch/vision/main/references/detection/transforms.py\")\n",
    "</code>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üîß <em><font color='tomato'>Configuraciones:</font></em> üîß\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Par√°metros\n",
    "DOWNLOAD_RESOURCES = False # Permite indicar si descargar el conjunto o no\n",
    "\n",
    "VERBOSE = True # Muestra √©poca a √©poca la evoluci√≥n\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "RANDOM_SEED = 42\n",
    "EPOCHS = 1\n",
    "\n",
    "PALMERAS_KAGGLE_DATASET = 'palmeras_kaggle'\n",
    "\n",
    "# Rutas\n",
    "RUTA_CARPETA_COMPRESSED = os.path.join(RUTA_CARPETA_RESOURCES, 'datasets', 'compressed')\n",
    "RUTA_CARPETA_RAW = os.path.join(RUTA_CARPETA_RESOURCES, 'datasets', 'raw')\n",
    "RUTA_CARPETA_KAGGLE_DATASET = os.path.join(RUTA_CARPETA_RAW, PALMERAS_KAGGLE_DATASET)\n",
    "\n",
    "FORMATOS_IMAGENES = ('.jpg', '.jpeg', '.png')\n",
    "FORMATOS_LABELS = ('.xml')\n",
    "IMG_SIZE_W, IMG_SIZE_H = 1000, 1000\n",
    "\n",
    "LABELS = {\n",
    "    'Palm' : 1\n",
    "}\n",
    "\n",
    "TRAIN_TEST_SPLIT_RATIO = [0.8, 0.2]\n",
    "\n",
    "random.seed(RANDOM_SEED)\n",
    "\n",
    "print(f'Dispositivo actual: {DEVICE}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En el marco de la especializaci√≥n en Inteligencia Artificial de la Facultad de Ingenier√≠a de la Universidad de Buenos Aires, se plantea, como trabajo final de la materia Visi√≥n por Computadora 2, desarrollar un modelo de visi√≥n por computadora que utilice aprendizaje profundo para detectar las ubicaciones de palmeras en diversas im√°genes √°ereas.\n",
    "\n",
    "Metodol√≥gicamente, se aplican principios de [CRISP-ML(Q)](https://ml-ops.org/content/crisp-ml) para el desarrollo del problema."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Business and data understanding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La detecci√≥n de palmeras es ampliamente estudiado en el √°rea de visi√≥n por computadora, dado su potencial como insumo de otras soluciones, como por ejemplo, aquellas que intentan mejorar la salud de las palmeras, donde inicalmente es deseable una detecci√≥n aera para luego realizar una inspecci√≥n manual (con el objetivo de reducir costos).\n",
    "\n",
    "Muchos estudios (como [este](https://www.researchgate.net/publication/382742087_Implementation_of_Slicing_Aided_Hyper_Inference_SAHI_in_YOLOv8_to_Counting_Oil_Palm_Trees_Using_High-Resolution_Aerial_Imagery_Data) por ejemplo) se realizan en paises en donde la mayor√≠a de las palmeras son materia prima para la producci√≥n del \"aceite de palmera\". Sin embargo, en paises no productores, el cuidado de las palmeras tambi√©n es importante, dado su impacto en el medio ambiente. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Los datos fueron obenidos del sito de Kaggle: https://www.kaggle.com/datasets/riotulab/aerial-images-of-palm-trees, sin embargo, para una mejor manejo, se los importan desde Google Drive (https://drive.google.com/drive/folders/1DFfgoGQ8-zLwQU85Uyq0X72GX4IITInV)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Luego de descargado los datos, tenemos la siguiente estructura de directorios:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- https://tree.nathanfriend.io/\n",
    "- resources\n",
    "  - datasets\n",
    "    - compressed\n",
    "      - palmeras_kaggle.zip\n",
    " -->\n",
    "\n",
    "<pre>\n",
    "resources/\n",
    "‚îî‚îÄ‚îÄ datasets/\n",
    "    ‚îî‚îÄ‚îÄ compressed/\n",
    "        ‚îî‚îÄ‚îÄ palmeras_kaggle.zip\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Descomprimimos los datos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funci√≥n para descomprimir archivos .zip\n",
    "def descomprimir_archivos(carpeta_origen, carpeta_destino):\n",
    "    archivos = [f for f in os.listdir(carpeta_origen) if f.endswith('.zip')]\n",
    "    \n",
    "    if not archivos:\n",
    "        warnings.warn(\"No se encontraron archivos .zip para descomprimir.\")\n",
    "        return\n",
    "    \n",
    "    for archivo in archivos:\n",
    "        ruta_archivo = os.path.join(carpeta_origen, archivo)\n",
    "        print(f\"Descomprimiendo {archivo} en {carpeta_destino}...\")\n",
    "        \n",
    "        # Descomprimir archivo\n",
    "        with zipfile.ZipFile(ruta_archivo, 'r') as zip_ref:\n",
    "            zip_ref.extractall(carpeta_destino)\n",
    "        print(f\"{archivo} descomprimido exitosamente.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear la carpeta 'raw' si no existe\n",
    "os.makedirs(RUTA_CARPETA_RAW, exist_ok=True)\n",
    "\n",
    "# Verificar si la carpeta compressed existe\n",
    "if os.path.exists(RUTA_CARPETA_COMPRESSED):\n",
    "    descomprimir_archivos(RUTA_CARPETA_COMPRESSED, RUTA_CARPETA_RAW)\n",
    "else:\n",
    "    warnings.warn(f\"La carpeta {RUTA_CARPETA_COMPRESSED} no fue encontrada. Verifique la estructura\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una vez descomprimido los archivos, podemos observar que el conjunto est√° en formato PascalVOC (aunque tambi√©n tiene dos archivos .csv que pueden utilizarse para otros formatos, ej: YOLO)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ajustamos los archvios para tener una carpeta \"data\" y otra \"labels\", de la siguiente forma:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!--\n",
    "- Palm-Counting-349images/\n",
    "  - data/\n",
    "    - img1.jpg\n",
    "    - img2.jpg\n",
    "    - ...\n",
    "  - labels/\n",
    "    - img1.xml\n",
    "    - img2.xml\n",
    "    - ...\n",
    " -->\n",
    "\n",
    " <pre>\n",
    "palmeras_kaggle/\n",
    "‚îú‚îÄ‚îÄ data/\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ img1.jpg\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ img2.jpg\n",
    "‚îÇ   ‚îî‚îÄ‚îÄ ...\n",
    "‚îî‚îÄ‚îÄ labels/\n",
    "    ‚îú‚îÄ‚îÄ img1.xml\n",
    "    ‚îú‚îÄ‚îÄ img2.xml\n",
    "    ‚îî‚îÄ‚îÄ ...\n",
    " </pre>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funci√≥n para mover archivos a la carpeta correspondiente\n",
    "def mover_archivos(source_dir, data_dest, label_dest):\n",
    "    for filename in os.listdir(source_dir):\n",
    "        file_path = os.path.join(source_dir, filename)\n",
    "        \n",
    "        # Verifica si es un archivo de imagen (extensi√≥n .jpg o similar)\n",
    "        if filename.endswith(FORMATOS_IMAGENES):\n",
    "            shutil.move(file_path, os.path.join(data_dest, filename))\n",
    "        # Verifica si es un archivo XML\n",
    "        elif filename.endswith(FORMATOS_LABELS):\n",
    "            shutil.move(file_path, os.path.join(label_dest, filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define los paths de las carpetas\n",
    "base_dir = os.path.join(RUTA_CARPETA_RAW, \"Palm-Counting-349images\")\n",
    "train_dir = os.path.join(RUTA_CARPETA_KAGGLE_DATASET, \"train\")\n",
    "test_dir = os.path.join(RUTA_CARPETA_KAGGLE_DATASET, \"test\")\n",
    "\n",
    "# Renombrar la carpeta Palm-Counting-349images a palmeras_kaggle\n",
    "if os.path.exists(base_dir):\n",
    "    if not os.path.exists(RUTA_CARPETA_KAGGLE_DATASET):\n",
    "        os.rename(base_dir, RUTA_CARPETA_KAGGLE_DATASET)\n",
    "\n",
    "        # Define los nuevos directorios dentro de la nueva carpeta\n",
    "        data_dir = os.path.join(RUTA_CARPETA_KAGGLE_DATASET, \"data\")\n",
    "        label_dir = os.path.join(RUTA_CARPETA_KAGGLE_DATASET, \"labels\")\n",
    "\n",
    "        # Crea las nuevas carpetas si no existen\n",
    "        os.makedirs(data_dir, exist_ok=True)\n",
    "        os.makedirs(label_dir, exist_ok=True)\n",
    "\n",
    "        # Mover archivos de train y test a las nuevas carpetas\n",
    "        try:\n",
    "            mover_archivos(train_dir, data_dir, label_dir)\n",
    "            mover_archivos(test_dir, data_dir, label_dir)\n",
    "        except:\n",
    "            warnings.warn(\n",
    "                f\"Error al mover los archivos. Las carpetas {train_dir} y {test_dir} existen? O ya fueron borradas?\"\n",
    "            )\n",
    "\n",
    "        # Eliminar las carpetas de train y test\n",
    "        if os.path.exists(train_dir):\n",
    "            shutil.rmtree(train_dir)\n",
    "        if os.path.exists(test_dir):\n",
    "            shutil.rmtree(test_dir)\n",
    "\n",
    "        # Elimina los archivos .csv\n",
    "        csv_files = [\"train_labels.csv\", \"test_labels.csv\"]\n",
    "        for csv_file in csv_files:\n",
    "            csv_path = os.path.join(RUTA_CARPETA_KAGGLE_DATASET, csv_file)\n",
    "            if os.path.exists(csv_path):\n",
    "                os.remove(csv_path)\n",
    "\n",
    "        print(\n",
    "            \"Archivos movidos, archivos CSV eliminados y la carpeta renombrada correctamente.\"\n",
    "        )\n",
    "    else:\n",
    "        warnings.warn(\n",
    "            f\"Ya existe la ruta {RUTA_CARPETA_KAGGLE_DATASET}, no se hizo nada...\"\n",
    "        )\n",
    "else:\n",
    "    raise FileNotFoundError(f'No se descomprimi√≥ el dataset {base_dir}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualizamos la cantidad de im√°genes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funci√≥n para contar archivos .jpg en una carpeta dada\n",
    "def contar_imagenes(directorio):\n",
    "    return len([f for f in os.listdir(directorio) if f.endswith(FORMATOS_IMAGENES)])\n",
    "\n",
    "# Contar im√°genes en las carpetas 'test' y 'train'\n",
    "cant_imagenes = contar_imagenes(os.path.join(RUTA_CARPETA_KAGGLE_DATASET, 'data'))\n",
    "\n",
    "print(f'Total de im√°genes: {cant_imagenes}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si inspeccionamos algunas im√°genes, vemos que tienen un tag `polygon` en el xml. Este tag lo vamos a eliminar para mejor manejo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: eliminar tag path\n",
    "# Funci√≥n para eliminar los objetos con polygon\n",
    "def remove_objects_with_polygon(xml_path):\n",
    "    tree = ET.parse(xml_path)\n",
    "    root = tree.getroot()\n",
    "\n",
    "    # Encontrar todos los objetos\n",
    "    for obj in root.findall(\"object\"):\n",
    "        # Si el objeto tiene un tag polygon, eliminar el objeto\n",
    "        if obj.find(\"polygon\") is not None:\n",
    "            root.remove(obj)\n",
    "\n",
    "    # Guardar los cambios en el archivo\n",
    "    tree.write(xml_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directorio de etiquetas (carpeta que contiene los archivos XML)\n",
    "label_dir = os.path.join(RUTA_CARPETA_KAGGLE_DATASET, \"labels\")\n",
    "\n",
    "# Recorrer todos los archivos XML en la carpeta de etiquetas\n",
    "for xml_file in os.listdir(label_dir):\n",
    "    if xml_file.endswith(\".xml\"):\n",
    "        xml_file_path = os.path.join(label_dir, xml_file)\n",
    "        # Aplicar la funci√≥n a cada archivo XML\n",
    "        remove_objects_with_polygon(xml_file_path)\n",
    "        print(f\"Se han eliminado los objetos con polygon de {xml_file_path}\")\n",
    "\n",
    "\n",
    "print(\"Proceso completado para todos los archivos.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos observar una imagen de ejemplo y su detecci√≥n:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Mejorar las etiquetas y visualizacion.\n",
    "# Funci√≥n para dibujar bounding boxes, ya sea desde archivos o dataset\n",
    "def draw_bbox(img, target_or_annotation_path, ax, from_dataset=False):\n",
    "    if from_dataset:\n",
    "        # Si es desde el dataset, el formato de 'img' ya es un tensor\n",
    "        img_np = img.permute(1, 2, 0).numpy()  # Convertir a formato de Matplotlib\n",
    "        ax.imshow(img_np)\n",
    "        ax.axis(\"off\")\n",
    "\n",
    "        # Dibujar los bounding boxes del dataset\n",
    "        for box in target_or_annotation_path[\"boxes\"]:\n",
    "            xmin, ymin, xmax, ymax = box\n",
    "            rect = patches.Rectangle((xmin, ymin), xmax - xmin, ymax - ymin,\n",
    "                                     linewidth=2, edgecolor=\"r\", facecolor=\"none\")\n",
    "            ax.add_patch(rect)\n",
    "\n",
    "        # A√±adir etiquetas\n",
    "        for box, label in zip(target_or_annotation_path[\"boxes\"], target_or_annotation_path[\"labels\"]):\n",
    "            xmin, ymin, _, _ = box\n",
    "            ax.text(xmin, ymin - 10, \"Palmera\", color=\"white\", fontsize=12, backgroundcolor=\"red\")\n",
    "    else:\n",
    "        # Si es desde archivos, cargar la imagen y parsear el archivo XML\n",
    "        image = cv2.cvtColor(cv2.imread(img), cv2.COLOR_BGR2RGB)\n",
    "        ax.imshow(image)\n",
    "        ax.axis(\"off\")\n",
    "\n",
    "        # Parsear el archivo de anotaci√≥n PascalVOC\n",
    "        tree = ET.parse(target_or_annotation_path)\n",
    "        root = tree.getroot()\n",
    "        for obj in root.findall(\"object\"):\n",
    "            bbox = obj.find(\"bndbox\")\n",
    "            xmin = int(bbox.find(\"xmin\").text)\n",
    "            ymin = int(bbox.find(\"ymin\").text)\n",
    "            xmax = int(bbox.find(\"xmax\").text)\n",
    "            ymax = int(bbox.find(\"ymax\").text)\n",
    "\n",
    "            # Dibujar el bounding box\n",
    "            rect = patches.Rectangle((xmin, ymin), xmax - xmin, ymax - ymin,\n",
    "                                     linewidth=2, edgecolor=\"r\", facecolor=\"none\")\n",
    "            ax.add_patch(rect)\n",
    "\n",
    "            # Mostrar etiqueta\n",
    "            label = obj.find(\"name\").text\n",
    "            ax.text(xmin, ymin - 10, label, color=\"white\", fontsize=12, backgroundcolor=\"red\")\n",
    "\n",
    "# Funci√≥n para dibujar una grilla de im√°genes con bounding boxes\n",
    "def draw_grid(images, targets, grid_size=(3, 3), from_dataset=False):\n",
    "    fig, axs = plt.subplots(grid_size[0], grid_size[1], figsize=(15, 15))\n",
    "\n",
    "    for i in range(grid_size[0] * grid_size[1]):\n",
    "        img = images[i]\n",
    "        target = targets[i]\n",
    "        ax = axs[i // grid_size[1], i % grid_size[1]]\n",
    "        draw_bbox(img, target, ax, from_dataset=from_dataset)\n",
    "\n",
    "    # Ajustar el layout para evitar superposici√≥n\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Funci√≥n para mostrar una sola imagen\n",
    "def show_single_image(image, target, from_dataset=False):\n",
    "    fig, ax = plt.subplots(figsize=(8, 8))\n",
    "    draw_bbox(image, target, ax, from_dataset=from_dataset)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Funci√≥n para obtener las rutas de im√°genes y anotaciones\n",
    "def get_image_annotation_paths(image_dir, label_dir, max_files=9):\n",
    "    # Listar todos los archivos de im√°genes (filtrar por extensi√≥n de imagen)\n",
    "    image_files = [f for f in os.listdir(image_dir) if f.endswith(FORMATOS_IMAGENES)]\n",
    "    # Ordenar los archivos para garantizar el orden\n",
    "    image_files.sort()\n",
    "\n",
    "    # Tomar las primeras 'max_files' im√°genes\n",
    "    image_files = image_files[:max_files]\n",
    "\n",
    "    # Generar las rutas para las im√°genes y las anotaciones XML correspondientes\n",
    "    image_paths = [os.path.join(image_dir, img) for img in image_files]\n",
    "\n",
    "    # Reemplazar la extensi√≥n de la imagen por .xml para encontrar las anotaciones\n",
    "    annotation_paths = [\n",
    "        os.path.join(label_dir, os.path.splitext(img)[0] + \".xml\")\n",
    "        for img in image_files\n",
    "    ]\n",
    "\n",
    "    return image_paths, annotation_paths\n",
    "\n",
    "# Funci√≥n para obtener una imagen y su anotaci√≥n aleatoria\n",
    "def get_random_image_and_annotation(image_dir, label_dir):\n",
    "    # Listar todas las im√°genes disponibles en la carpeta\n",
    "    image_files = [f for f in os.listdir(image_dir) if f.endswith(FORMATOS_IMAGENES)]\n",
    "    \n",
    "    # Seleccionar una imagen aleatoria\n",
    "    random_image = random.choice(image_files)\n",
    "    \n",
    "    # Generar las rutas para la imagen y la anotaci√≥n correspondiente\n",
    "    image_path = os.path.join(image_dir, random_image)\n",
    "    annotation_path = os.path.join(label_dir, os.path.splitext(random_image)[0] + \".xml\")\n",
    "    \n",
    "    return image_path, annotation_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_dir = os.path.join(RUTA_CARPETA_KAGGLE_DATASET, \"data\")\n",
    "label_dir = os.path.join(RUTA_CARPETA_KAGGLE_DATASET, \"labels\")\n",
    "\n",
    "image_path, annotation_path = get_random_image_and_annotation(image_dir, label_dir)\n",
    "show_single_image(image_path, annotation_path, from_dataset=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tambi√©n visualizamos varias im√°genes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_paths, annotation_paths = get_image_annotation_paths(image_dir, label_dir, max_files=9)\n",
    "draw_grid(image_paths, annotation_paths, grid_size=(3, 3), from_dataset=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tambi√©n podemos utilizar la librer√≠a de FiftyOne (https://docs.voxel51.com/) para una mejor visualizaci√≥n. Otra ventaja de esta librer√≠a es que tiene un m√≥dulo de exportaci√≥n a varios formatos conocidos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Investigar m√°s esto.\n",
    "# TODO: Arreglar annotaciones, es posible con esto?\n",
    "# Crear el dataset en FiftyOne desde un formato PascalVOC\n",
    "fiftyone_dataset = fo.Dataset.from_dir(\n",
    "    dataset_type=fo.types.VOCDetectionDataset,\n",
    "    dataset_dir=RUTA_CARPETA_KAGGLE_DATASET\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos observar los datos del conjunto:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(fiftyone_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Mejorar esta imagen, est√° cargando antes de tiempo... Poner un sleep?\n",
    "# Lanzar la aplicaci√≥n FiftyOne desde Jupyter\n",
    "session = fo.launch_app(fiftyone_dataset)\n",
    "\n",
    "session.freeze()\n",
    "session.open_tab()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- TODO: Balance de clases? An√°lisis de datos? -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine learning model engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como experimentos, dado que inicialmente el modelo no tiene como objetivo el funcionamiento en tiempo real, se plantea utilizar modelos de detecci√≥n en dos etapas (ej: Faster R-CNN). Sin embargo, tambi√©n se tienen en cuenta modelos de una sola etapa (ej: YOLO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Faster R-CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preparar el datset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En el formato actual del conjunto, es necesario generar una clase Dataset Customizada para el entrenamiento. Para eso podemos tomar como ejemplo este tutorial: https://pytorch.org/tutorials/intermediate/torchvision_tutorial.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üìù <em><font color='Gray'>Nota:</font></em>\n",
    "<em><font color='Gray'><p>\n",
    "\n",
    "Para entrenar la red Faster R-CNN, es necesario que se cumplan algunos requerimientos, entre los cuales se encuentra una definici√≥n de la estructura del Dataset como la siguiente:\n",
    "\n",
    "<cite>\n",
    "The only specificity that we require is that the dataset __getitem__ should return a tuple:\n",
    "\n",
    "- image: torchvision.tv_tensors.Image of shape [3, H, W], a pure tensor, or a PIL Image of size (H, W)\n",
    "- target: a dict containing the following fields\n",
    "    - `boxes`, torchvision.tv_tensors.BoundingBoxes of shape [N, 4]: the coordinates of the N bounding boxes in [x0, y0, x1, y1] format, ranging from 0 to W and 0 to H\n",
    "    - `labels`, integer torch.Tensor of shape [N]: the label for each bounding box. 0 represents always the background class.\n",
    "    - `image_id`, int: an image identifier. It should be unique between all the images in the dataset, and is used during evaluation\n",
    "    - `area`, float torch.Tensor of shape [N]: the area of the bounding box. This is used during evaluation with the COCO metric, to separate the metric scores between small, medium and large boxes.\n",
    "    - `iscrowd`, uint8 torch.Tensor of shape [N]: instances with iscrowd=True will be ignored during evaluation.\n",
    "    - (optionally) `masks`, torchvision.tv_tensors.Mask of shape [N, H, W]: the segmentation masks for each one of the objects\n",
    "\n",
    "</cite>\n",
    "\n",
    "Tambi√©n destacar que el modelo considera a la clase 0 como el fondo, lo que implica consideraci√≥n en las labels\n",
    "\n",
    "</p></font></em>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PalmTreeDataset(Dataset):\n",
    "    def __init__(self, root):\n",
    "        self.root = root\n",
    "        self.imgs = list(sorted(os.listdir(os.path.join(root, \"data\"))))\n",
    "        self.labels = list(sorted(os.listdir(os.path.join(root, \"labels\"))))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.imgs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Cargar imagen\n",
    "        img_path = os.path.join(self.root, \"data\", self.imgs[idx])\n",
    "        label_path = os.path.join(self.root, \"labels\", self.labels[idx])\n",
    "        img = read_image(img_path)\n",
    "\n",
    "        # Parsear archivo XML (PascalVOC)\n",
    "        tree = ET.parse(label_path)\n",
    "        root = tree.getroot()\n",
    "        boxes = []\n",
    "        labels = []\n",
    "        isCrowd = []\n",
    "        for obj in root.findall('object'):\n",
    "            label = obj.find('name').text\n",
    "            if label in LABELS.keys():\n",
    "                bbox = obj.find('bndbox')\n",
    "                xmin = int(bbox.find('xmin').text)\n",
    "                ymin = int(bbox.find('ymin').text)\n",
    "                xmax = int(bbox.find('xmax').text)\n",
    "                ymax = int(bbox.find('ymax').text)\n",
    "\n",
    "                boxes.append([xmin, ymin, xmax, ymax])\n",
    "                labels.append(LABELS[label]) # Hay solo una clase: Palmera\n",
    "                isCrowd.append(0) # Seteamos como falso.\n",
    "\n",
    "        # Convertimos a tensores\n",
    "        isCrowd = torch.as_tensor(isCrowd, dtype=torch.int64)        \n",
    "        labels = torch.as_tensor(labels, dtype=torch.int64)\n",
    "        img = tv_tensors.Image(img)\n",
    "        boxes = tv_tensors.BoundingBoxes(boxes, format=\"XYXY\", canvas_size=F.get_image_size(img))\n",
    "\n",
    "        image_id = idx\n",
    "        area = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0])\n",
    "    \n",
    "        target = {}\n",
    "        target[\"boxes\"] = boxes\n",
    "        target[\"labels\"] = labels\n",
    "        target[\"image_id\"] = image_id\n",
    "        target[\"area\"] = area\n",
    "        target[\"iscrowd\"] = isCrowd\n",
    "\n",
    "        return img, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformedPalmDataset(Dataset):\n",
    "    def __init__(self, dataset, transforms=None):\n",
    "        self.dataset = dataset\n",
    "        self.transforms = transforms\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img, target = self.dataset[idx]\n",
    "        if self.transforms is not None:\n",
    "            img, target = self.transforms(img, target)\n",
    "        return img, target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Definir las transformaciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_transforms(train):\n",
    "    transforms = []\n",
    "    if train:\n",
    "        # TODO: Train augmentantions\n",
    "        pass\n",
    "    transforms.append(T.ToDtype(torch.float, scale=True))\n",
    "    transforms.append(T.ToPureTensor())\n",
    "    return T.Compose(transforms)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creamos los datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_dataset = PalmTreeDataset(RUTA_CARPETA_KAGGLE_DATASET)\n",
    "train_dataset, test_dataset = torch.utils.data.random_split(full_dataset, TRAIN_TEST_SPLIT_RATIO, torch.Generator().manual_seed(RANDOM_SEED))\n",
    "\n",
    "train_dataset = TransformedPalmDataset(train_dataset, transforms=get_transforms(train=True))\n",
    "test_dataset = TransformedPalmDataset(test_dataset, transforms=get_transforms(train=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualizamos las transformaciones:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_grid(\n",
    "    [train_dataset[i][0] for i in range(9)],\n",
    "    [train_dataset[i][1] for i in range(9)],\n",
    "    grid_size=(3, 3),\n",
    "    from_dataset=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Definici√≥n del modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(num_classes):\n",
    "    # Cargar un modelo preentrenado de Faster R-CNN\n",
    "    model = torchvision.models.detection.fasterrcnn_resnet50_fpn_v2(weights='DEFAULT')\n",
    "    \n",
    "    # Obtener el n√∫mero de entradas de la capa de clasificaci√≥n\n",
    "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "    \n",
    "    # Reemplazar la cabeza del clasificador por una que tenga el n√∫mero de clases deseado (n clase + fondo)\n",
    "    model.roi_heads.box_predictor = torchvision.models.detection.faster_rcnn.FastRCNNPredictor(in_features, num_classes + 1)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instanciamos el modelo.\n",
    "model = get_model(len(LABELS.keys()))\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img, _ = train_dataset[0]\n",
    "input_size = (1, *img.shape)\n",
    "summary(model, input_size=input_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testeamos la inferencia:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_loader = torch.utils.data.DataLoader(\n",
    "    test_dataset, batch_size=2, shuffle=True, collate_fn=lambda x: tuple(zip(*x))\n",
    ")\n",
    "\n",
    "# Training\n",
    "images, targets = next(iter(data_loader))\n",
    "output = model(images, targets)  # Returns losses and detections\n",
    "images = list(image for image in images)\n",
    "targets = [{k: v for k, v in t.items()} for t in targets]\n",
    "print(f\"Training test: {output}\")\n",
    "\n",
    "# Inferencia\n",
    "model.eval()\n",
    "x = [torch.rand(3, 300, 400), torch.rand(3, 500, 400)]\n",
    "predictions = model(x)  # Returns predictions\n",
    "print(f\"Inference test: {predictions[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preparar el entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(train_dataset, batch_size=2, shuffle=True, collate_fn=lambda x: tuple(zip(*x)))\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=1, shuffle=False, collate_fn=lambda x: tuple(zip(*x)))\n",
    "\n",
    "# Enviar modelo a GPU si est√° disponible\n",
    "model.to(DEVICE)\n",
    "\n",
    "# Optimizer\n",
    "params = [p for p in model.parameters() if p.requires_grad]\n",
    "optimizer = torch.optim.SGD(params, lr=0.005, momentum=0.9, weight_decay=0.0005)\n",
    "\n",
    "# Aprender tasa de aprendizaje (LR) programada\n",
    "lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=3, gamma=0.1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Entrenar el modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 1\n",
    "for epoch in range(EPOCHS):\n",
    "    train_one_epoch(model, optimizer, train_dataloader, DEVICE, epoch, print_freq=10)\n",
    "    lr_scheduler.step()\n",
    "    evaluate(model, test_dataloader, DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### YOLO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- ## Quality assurance for machine learning applications -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- ## Deployment -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- ## Monitoring and maintenance -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Referencias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Adel Ammar, and Anis Koubaa. (2023). Aerial images of palm trees [Data set]. Kaggle. https://doi.org/10.34740/KAGGLE/DSV/6382990\n",
    "- TorchVision Object Detection Finetuning Tutorial ‚Äî PyTorch Tutorials 2.4.0+cu121 documentation. (n.d.). https://pytorch.org/tutorials/intermediate/torchvision_tutorial.html"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
