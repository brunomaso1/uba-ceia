{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "51zGEEc86yfw"
      },
      "source": [
        "# <h1><center>Rain in Australia</center></h1>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hHf3lwp16yfy"
      },
      "source": [
        "<center>\n",
        "<img src=\"https://drive.google.com/thumbnail?id=1jeVAK1A6OcNi-bz8ha4_NFbWVXGMJ7Zw&sz=w3000\" width=\"500\" alt=\"Figura 1: Datos meterológicos de Australia del 2008 al 2009, obtenidos de http://www.bom.gov.au/climate/history/enso/\">\n",
        "\n",
        "<small><em>Figura 1: Datos meterológicos de Australia del 2008 al 2009, obtenidos de http://www.bom.gov.au/climate/history/enso/</em></small>\n",
        "</center>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bp8iLkP_6yf0"
      },
      "source": [
        "<center>\n",
        "<em>Datos del proyecto:</em>\n",
        "\n",
        "| Subtitulo   | Trabajo final de Aprendizaje de MAquina I - FIUBA                                                                                                     |\n",
        "| ----------- | ---------------------------------------------------------------------------------------------------------------------------------------------- |\n",
        "| **Descrpción**  | Análisis de datos meteorológicos de Australia con el objetivo de predecir si lloverá al otro día                          |\n",
        "| **Integrantes** | • Juan Cruz Ferreyra (ferreyra.juancruz95@gmail.com)<br>• Simón Rodriguez (simon.andre.r@gmail.com)<br>• Bruno Masoller (brunomaso1@gmail.com)<br>• David Canal (jose.david.canal.89@gmail.com) |\n",
        "\n",
        "</center>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wJHYo9L46yf3"
      },
      "source": [
        "## 1. Metodología"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FBbRkZu1FMRV"
      },
      "source": [
        "En el campo de aprendizaje de máquina, la comunidad todavía está definiendo un proceso sistemático y estructurado para el cliclo de vida de soluciones basadas en aprendizaje automático. El objetivo es enfocar los procedimientos y estándares de calidad de la ingeniería de software clásica a metodologías de este sub-campo de la inteligencia artificial.\n",
        "\n",
        "Es en este punto que surge [CRISP-ML(Q)](https://arxiv.org/pdf/2003.05155.pdf), como una metodología que integra las mejores prácticas de la ingeniería de software sobre todo el ciclo de vida de soluciones enfocadas en resolver problemas con aprendizaje de máquina.\n",
        "\n",
        "CRISP-ML(Q) surge a partir de [CRISP-DM](https://es.wikipedia.org/wiki/Cross_Industry_Standard_Process_for_Data_Mining) como un intento de ampliar dicho \"framework\" al área de \"machine learning\".\n",
        "\n",
        "En palabras de los autores (Stefan Studer et al),  CRISP-ML(Q) propone un modelo de proceso al que llaman modelo de proceso estándar \"CRoss-Industry\" para el desarrollo de aplicaciones de \"Machine Learning\" con metodología de aseguramiento de la Calidad, donde resalta su compatibilidad con CRISP-DM. Está diseñado para el desarrollo de aplicaciones de máquina, es decir, escenarios de aplicaciones donde se implementa y mantiene un modelo de ML\n",
        "como parte de un producto o servicio.\n",
        "\n",
        "Consta de seis fases:\n",
        "<em>\n",
        "\n",
        "1. Business and Data Understanding\n",
        "2. Data Engineering (Data Preparation)\n",
        "3. Machine Learning Model Engineering (Modeling)\n",
        "4. Quality Assurance for Machine Learning Applications\n",
        "5. Deployment\n",
        "6. Monitoring and Maintenance.\n",
        "\n",
        "</em>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8QsDZEPhAKp9"
      },
      "source": [
        "<center>\n",
        "<img src=\"https://drive.google.com/thumbnail?id=1Aoiu62mQCrICj34T6eTHFRJLfmsXxkfS&sz=w2000\" width=\"500\" alt=\"Figura 2: Machine Learning Development Life Cycle Process, obtenido de https://ml-ops.org/content/crisp-ml\">\n",
        "\n",
        "<small><em>Figura 2: Machine Learning Development Life Cycle Process, obtenido de https://ml-ops.org/content/crisp-ml</em></small>\n",
        "</center>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XquI0FpRAKp9"
      },
      "source": [
        "En donde cada fase requiere el siguiente proceso:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tLT-k3pEAKp-"
      },
      "source": [
        "<center>\n",
        "<img src=\"https://drive.google.com/thumbnail?id=1BtP076AUQEzeK3gQO0fP8DuOZHQhnS56&sz=w1000\" width=\"500\" alt=\"Figura 3: Proceso dentro de cada fase\">\n",
        "\n",
        "<small><em>Figura 3: Proceso dentro de cada fase</em></small>\n",
        "</center>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CqylHIxaAKp-"
      },
      "source": [
        "Resumen de tareas de cada fase:\n",
        "*También se agrega infromación adiccional. Toada esta tabla se puede tomar como una especie de \"checklist\".*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XRhWSWM96yf6"
      },
      "source": [
        "| CRISP-ML(Q) Phase                | Tasks                                                                                                                                                                             |\n",
        "| -------------------------------- | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |\n",
        "| *Business and Data Understanding*  | • Define the Scope of the ML Application<br>• Sucess criteria<br>• Feasibility<br>• Data collection<br>• Data quality verification<br>• Review of output documents                |\n",
        "| *Data Engineering*                 | • Select data<br>• Clean data<br>• Construct data<br>• Standarize data                                                                                                            |\n",
        "| *ML Model Engineering*             | • Modeling<br>• Assure reproducibility                                                                                                                                            |\n",
        "| *ML Model Evaluation*              | • Validate performance<br>• Determine robustness<br>•Increase explainability for ML practitioner and end user<br>• Compare results with defined success criteria                  |\n",
        "| *Model Deployment*                 | • Define inference hardware<br>• Model evaluation under production<br>• Assure user acceptance and usability<br>• Minimize the risks of unforseen errors<br>• Deployment strategy |\n",
        "| *Model Monitoring and Maintenance* | • Monitor<br>• Update                                                                                                                                                             |"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T8V3Zre56yf7"
      },
      "source": [
        "<small>*Tabla 1: Fases y tareas de CRISP-ML(Q)*</small>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DEaomgw-6yf8"
      },
      "source": [
        "Un ejemplo de algunos de los modelos más utilizados se puede observar en la siguiente imagen:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-COu424RAKqA"
      },
      "source": [
        "<center>\n",
        "<img src=\"https://drive.google.com/thumbnail?id=1-el9MGC3Ouc0IFCaQD9B9477x4H561IP&sz=w1000\" width=\"500\" alt=\"Figura 4: Machine learning models example\">\n",
        "\n",
        "<small><em>Figura 4: Machine learning models example</em></small>\n",
        "</center>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mx2_fn5B6yf8"
      },
      "source": [
        "## 2. CRISP-ML(Q)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Flaj9ZT6yf9"
      },
      "source": [
        "> *Notas sobre la aplicación del Método*: Dada la acotación planteada para el trabajo, no se tienen en cuenta todas las fases (solamente aquellas que son acotadas a los temas vistos en el curso), ni tampoco el análisis de riesgo que plantea el modelo dentro de cada fase. Se plantean como mejoras posteriores."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "9dal2D5sTEtf",
        "outputId": "442129e7-44cc-430f-fbbe-873d7a192e47"
      },
      "outputs": [],
      "source": [
        "# Pre-requisitos\n",
        "# Chequeo de sistema operativo\n",
        "import platform\n",
        "match platform.system():\n",
        "    case 'Windows':\n",
        "        print('Windows')\n",
        "    case 'Linux':\n",
        "        print('Linux')\n",
        "        !apt-get -qq install -y libspatialindex-dev # TODO: Investigar librería https://pypi.org/project/Rtree/\n",
        "    case _:\n",
        "        print('Otro')\n",
        "\n",
        "# Instalacion de paquetes de python\n",
        "%pip install gdown\n",
        "%pip install ydata-profiling\n",
        "%pip install -q -U osmnx"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "B7EELiJIAKqB",
        "outputId": "0cd9510d-8a3c-4d09-f444-227760078b43"
      },
      "outputs": [],
      "source": [
        "# Descargamos el conjunto y los auxiliares\n",
        "!gdown https://drive.google.com/drive/folders/1Ln-dn1HeWQMJCNsFmyrRwNWOKUs1fijs --folder\n",
        "!gdown 1eNWQJR08ajXPx9tiT1KK1ylEY1uZiiMb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JWNbxJeJ6yf9"
      },
      "outputs": [],
      "source": [
        "# Importacion de librerias\n",
        "import seaborn as sns  # Visualización de datos estadísticos\n",
        "import re  # Expresiones regulares\n",
        "import random  # Generar números aleatorios\n",
        "import pandas as pd  # Procesamiento de datos\n",
        "import osmnx as ox  # OpenStreetMap\n",
        "import os  # Interactuar con el sistema operativo\n",
        "import numpy as np  # Albegra lineal\n",
        "import matplotlib.pyplot as plt  # Visualización de datos\n",
        "import json  # Interactuar con archivos JSON\n",
        "import geopandas as gpd  # Georeferenciacion\n",
        "import pickle\n",
        "from datetime import datetime\n",
        "from ydata_profiling import ProfileReport  # Reporte (profiling)\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler, FunctionTransformer  # Preprocesamiento de datos\n",
        "from sklearn.model_selection import train_test_split  # Dividir datos en conjuntos de entrenamiento y prueba\n",
        "from sklearn.metrics import (accuracy_score, f1_score)  # Métricas de evaluación\n",
        "from sklearn.linear_model import LogisticRegression  # Regresión logística\n",
        "from sklearn.naive_bayes import MultinomialNB  # Naive Bayes\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier  # Random Forest\n",
        "from sklearn.decomposition import PCA  # Análisis de componentes principales\n",
        "from sklearn.impute import SimpleImputer  # Imputación de datos\n",
        "from sklearn.svm import SVC\n",
        "from shapely.geometry import Point  # Geometría espacial\n",
        "from itertools import chain, combinations  # Iteradores\n",
        "from IPython import display  # Mostrar información en pantalla\n",
        "from geopandas.datasets import get_path  # Ruta de los datos geográficos\n",
        "from sklearn.pipeline import Pipeline  # Pipelines\n",
        "from sklearn.compose import ColumnTransformer  # Transformaciones de columnas\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "from utils import *  # Importación de funciones de utilidades\n",
        "\n",
        "display.clear_output()\n",
        "%matplotlib inline\n",
        "# Linea mágica para mostrar los graficos dentro del notebook"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "umYsLp83WfRW"
      },
      "outputs": [],
      "source": [
        "# Configuración del notebook\n",
        "pd.options.mode.chained_assignment = None\n",
        "\n",
        "debug_mode = False # Modo más informativo\n",
        "show_profile = False # Muestra el perfilado de los datos\n",
        "spectral_palette = [ \"#9e0142\", \"#d53e4f\", \"#f46d43\", \"#fdae61\", \"#fee08b\",\n",
        "                    \"#ffffbf\", \"#e6f598\", \"#abdda4\", \"#66c2a5\", \"#3288bd\", \"#5e4fa2\"] # Paleta de colores\n",
        "random_state = 42 # Semilla para reproducibilidad"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a2wj7VaF6yf-"
      },
      "source": [
        "### 2.1. Entendimiento del negocio y los datos (Business and Data Understanding)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tXLfthsbMTLD",
        "outputId": "7b420135-873f-41c5-bd87-d4e9ce351045"
      },
      "outputs": [],
      "source": [
        "# Importamos el conjunto\n",
        "file_name = './data/weatherAUS.csv'\n",
        "try:\n",
        "    df = pd.read_csv(file_name)\n",
        "    print('Dataset local')\n",
        "except:\n",
        "    raise Exception('Error al encontrar el archivo')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wbFftKSz6yf_"
      },
      "source": [
        "#### 2.1.1. Definir el alcance de la solución"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R3o-0UF86yf_"
      },
      "source": [
        "Este proyecto se plantea en el marco de la materia Aprendizaje de Máquinas I de la especialización en Inteligencia Artifical de la Facultad de Ingeniería de la Universidad de Buenos Aries; por lo tanto, es un proyecto con fines académicos.\n",
        "\n",
        "El conjunto a analizar corresponde a los datos meterológicos de Australia recolectados durante 10 años de varias ubicaciones. Los datos fueron recolectados en base a observaciones diarias, por la oficina de meteorolgía de Australia (Bureau of Meteorology), los cuales están disponibles al público desde su página: http://www.bom.gov.au/\n",
        "\n",
        "Como objetivo principal, se plantea lo siguiente:\n",
        "\n",
        "El objetivo es predecir si lloverá o no al día siguiente (variable RainTomorrow), en función de los datos meterológicos del día actual. El resultado es una clasificación binaria con las posibles respuestas \"Sí\" o \"No\".\n",
        "\n",
        "Predecir el estado del tiempo es algo que afecta el día a día de todas las personas. Tener la posibilidad de predecir si lloverá o no al día siguiente, en base a los datos del día anterior, tiene un alto valor para los servicios metereológicos y puede ayudar a mejorar las predicciones generales de una zona particular.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yCOFfFx16yf_"
      },
      "source": [
        "#### 2.1.2. Criterios de éxito"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XvjjtY9a6ygA"
      },
      "source": [
        "Como criterio de éxito orientado al negocio, en este caso, dado que es un proyecto académico, se enfoca en cumplir con los objetivos del trabajo práctico:\n",
        "\n",
        "- Describir y justificar la investigación y la fuente de datos elegida.\n",
        "- Definir un algoritmo para la resolución del problema y justificar su selección.\n",
        "- Explicar el resultado de la investigación aportando métricas adecuadas para demostrar el desempeño del algoritmo elegido.\n",
        "- Ofrecer una conclusión final de acuerdo con los resultados y proponer nuevos caminos de resolución en caso de ser necesario.\n",
        "\n",
        "Como criterio de éxito orientado al modelo de aprendizaje, se propone obtener una presición mayor al 90%, donde el enfoque sea en reducir la tasa de falsos negativos, ya que en este caso es más el impacto del usuario para dichos escenarios, en donde es preferible llevar un paraguas y que no llueva, a que no llevar un paraguas y que llueva.\n",
        "\n",
        "Como criterio de éxito orientado a lo económico, dado que es un proyecto académico, se plantea que el tiempo de dedicación se mantenga uniforme entre todos los integrantes del proyecto. Así como también completar el proyecto antes de una fecha límite.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CUv8u3CS6ygA"
      },
      "source": [
        "#### 2.1.3. Factibilidad"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iZinSrS06ygA"
      },
      "source": [
        "Dado que ya existe un conjunto de datos (https://www.kaggle.com/datasets/jsphyg/weather-dataset-rattle-package/data), y que dichos datos provienen de una buena fuente como lo es la oficina de meterología de Australia (también es un conjunto ámpliamente usado y calificado en \"kaggle\"), se considera factible los objetivos planteados."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HwSX6FS96ygA",
        "outputId": "0533ede6-0531-470d-c4f7-c585403dcaf6"
      },
      "outputs": [],
      "source": [
        "print('Tamaño del conjunto:', len(df))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZWEO4S3H6ygB"
      },
      "source": [
        "<p><em>\n",
        "El tamaño del conjunto es de <code>145460</code>, por lo que no se identifican problemas de disponibilidad ni de tamaño de los datos.\n",
        "</em><p>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9SKYnSee6ygB"
      },
      "source": [
        "#### 2.1.4. Recolección de datos"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BhfSJhyo6ygB"
      },
      "source": [
        "En este caso, el conjunto de datos ya fue recolectado y se descargaron de https://www.kaggle.com/datasets/jsphyg/weather-dataset-rattle-package/data, donde se brindan competencias sobre conjuntos determinados.\n",
        "\n",
        "Los datos no fueron actualizados desde que se compartieron y no se pretende que se actualicen, por lo que no es necesario un sistema de control de versiones de los datos."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aRkFXCOc6ygC"
      },
      "source": [
        "#### 2.1.5. Verificación la de calidad de los datos"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "diG_IjQY6ygC"
      },
      "source": [
        "##### 2.1.5.1. Exploración de los datos"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iTUpFSHsTrSo"
      },
      "source": [
        "Perfilado de los datos:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a-ho2L8mSyeF"
      },
      "outputs": [],
      "source": [
        "profile = ProfileReport(df, title='Reporte') # Cargamos el reporte\n",
        "if show_profile:\n",
        "  profile.to_notebook_iframe() # Lo mostramos en pantalla (tiempo aproximado 5 minutos)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UoErcZNgRvj-"
      },
      "source": [
        "###### Atributos y significados"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lMDCF6NQR40R"
      },
      "source": [
        "> En este paso, se describen los atributos según el conocimiento del problema."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 413
        },
        "id": "7jKbciIPSwN0",
        "outputId": "cdf398d1-c37f-494e-d95b-6da29b81e3ef"
      },
      "outputs": [],
      "source": [
        "# Mostramos los datos.\n",
        "df.sample(10, random_state=random_state)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3qbbhCNvV2Og"
      },
      "source": [
        "<p><em>\n",
        "Podemos observar que el dataset contiene 23 columnas, o sea 23 atributos. También algunos otros datos como que de por sí el conjunto tiene <code>NaN</code> como datos, lo que indica que probablemente sea más fácil cambiar estos datos a los análogos en <code>numpy</code>.\n",
        "</em></p>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TmKeHMapXdZz",
        "outputId": "a3f9123b-4c55-4078-ba9e-cf02c383a633"
      },
      "outputs": [],
      "source": [
        "print('Atributos:')\n",
        "df.columns"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wamqnh6FXntO"
      },
      "source": [
        "<p><em>\n",
        "Según el conocimiento del negocio, tenemos la descripción de los siguientes atributos:\n",
        "\n",
        "- `Date` → Tipo: Fecha (formato: `YYYY-MM-DD`) | Fecha de cuando se tomó la observación.\n",
        "- `Location` → Tipo: Cualitativa-Nominal | Ubicación de la estación meterológica\n",
        "- `MinTemp` → Tipo: Cuantitativa-Continua | Temperatura mínima (mínimo histórico de -23°)\n",
        "- `MaxTemp` → Tipo: Cuantitativa-Continua | Temperatura máxima (máximo histórico de 51°)\n",
        "- `Rainfall` → Tipo: Cuantitativa-Continua | Cuanta lluvia calló (máximo histórico en 375mm)\n",
        "- `Evaporation` → Tipo: Cuantitativa-Continua | Cuanto fue la evaporación\n",
        "- `Sunshine` → Tipo: Cuantitativa-Continua | Número de horas solares del día\n",
        "- `WindGustDir` → Tipo: Cualitativa-Nominal | Dirección del viento más fuerte\n",
        "- `WindGustSpeed` → Tipo: Cuantitativa-Continua | Velocidad del viento más fuerte (máximo registrado 408 km/h, en un cliclón, no se toman en cuenta tornados que vuela todo)\n",
        "- `WindDir9am` → Tipo: Cualitativa-Nominal | Datos específicos según hora del día\n",
        "- `WindDir3pm` → Tipo: Cualitativa-Nominal | Datos específicos según hora del día\n",
        "- `WindSpeed9am` → Tipo: Cuantitativa-Continua | Datos específicos según hora del día\n",
        "- `WindSpeed3pm` → Tipo: Cuantitativa-Continua | Datos específicos según hora del día\n",
        "- `Humidity9am` → Tipo: Cuantitativa-Continua | Datos específicos según hora del día\n",
        "- `Humidity3pm` → Tipo: Cuantitativa-Continua | Datos específicos según hora del día\n",
        "- `Pressure9am` → Tipo: Cuantitativa-Continua | Datos específicos según hora del día\n",
        "- `Pressure3pm` → Tipo: Cuantitativa-Continua | Datos específicos según hora del día\n",
        "- `Cloud9am` → Tipo: Cuantitativa-Continua | Datos específicos según hora del día\n",
        "- `Cloud3pm` → Tipo: Cuantitativa-Continua | Datos específicos según hora del día\n",
        "- `Temp9am` → Tipo: Cuantitativa-Continua | Datos específicos según hora del día\n",
        "- `Temp3pm` → Tipo: Cuantitativa-Continua | Datos específicos según hora del día\n",
        "- `RainToday` → Tipo: Booleano | Indica si llovío en el día\n",
        "- `RainTomorrow` → Tipo: Booleano | Indicador de riesgo si lloverá mañana o no. Es la variable objetivo.\n",
        "\n",
        "De este análisis preliminar, podemos deducir las siguientes suposiciones:\n",
        "- Podría haber una correlación entre la cantidad de lluvia y si llovió hoy. O sea, entre `Rainfall` y `RainToday`.\n",
        "</em></p>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B5mDchVFb5WF"
      },
      "source": [
        "###### Datos duplicados"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Av6tHqAQcczK",
        "outputId": "e725dad2-ce5b-438e-d7b5-04ab45bd9554"
      },
      "outputs": [],
      "source": [
        "# Información básica de los datos\n",
        "df.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TND-19L0dBkt",
        "outputId": "34272266-a9df-4619-9031-c22bef7f6969"
      },
      "outputs": [],
      "source": [
        "# Chequeamos si hay datos duplicados en las columnas Date y Location\n",
        "df = df.sort_values(by=[\"Location\", \"Date\"]).reset_index(drop=True)\n",
        "\n",
        "is_any_duplicated = df.duplicated(subset=[\"Date\", \"Location\"], keep=False).any()\n",
        "print(\"Observaciones duplicadas para una fecha o localidad: \", is_any_duplicated)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5QOAmHFydI-p"
      },
      "source": [
        "<p><em>\n",
        "Encontramos que la mayoria de las columnas corresponden a variables cuantitativas. Podemos observar que cuatro de ellas poseen una gran proporción de valores nulos: <code>Evaporation</code>, <code>Sunshine</code>, <code>Cloud9am</code>, <code>Cloud3pm</code>. El resto de las variables numéricas poseen cerca del 10% de valores faltantes o menos.\n",
        "\n",
        "Entre las variables de tipo cualitativas identificamos a <code>Location</code> y <code>Date</code> como columnas identificatorias de una observación, es decir, no hay filas con valores repetidos si tomamos el subset de esas dos columnas. Tampoco se observan valores nulos en niguna de ambas columnas. La importancia de estas variables no reside sólo en su carácter identificatorio, sino además proveen información, espacial y temporal, que vamos a procesar más adelante y que puede resultar útil para nuestro modelo.\n",
        "\n",
        "Observamos además que, entre las variables de tipo cualitativa tenemos la variable target <code>RainTomorrow</code>, binaria por definición del problema a resolver, y la variable <code>RainToday</code> que, intuitivamente, sigue una misma codificación que la variable target mencionada. Es importante notar que ambas variables binarias cuentan con valores nulos en algunas observaciones.\n",
        "\n",
        "Finalmente, encontramos tres variables cualitativas relacionadas con la dirección del viento en diferentes momentos del día. Podemos considerar a estas variables como categóricas ordinales en el sistema de coordenadas polares, no existiendo relación de menor-mayor entre ellas pero si una relación secuencial en dos dimensiones.\n",
        "</em></p>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5cFrW5tm1nU_"
      },
      "source": [
        "###### Tipos de datos"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cV4W3Rzu16C8"
      },
      "source": [
        "> En este punto se investiga los tipos de datos y se asigna el tipo correcto según el caso."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NUUoweVkAKqK",
        "outputId": "cf83f4c6-edeb-4c92-f83b-ac08b4b6f2f5"
      },
      "outputs": [],
      "source": [
        "# Visualizar los tipos de datos\n",
        "df.dtypes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m7bNkDhU11H_"
      },
      "source": [
        "<p><em>\n",
        "Según lo analizado anteriormente, tenemos la siguiente clasificación de tipos de datos:\n",
        "</em></p>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZLf857ed3Nnz"
      },
      "source": [
        "| **Variable**      | **Tipo actual** | **Tipo correcto** |\n",
        "|-------------------|-----------------|-------------------|\n",
        "| **Date**          | object          | datetime64        |\n",
        "| **Location**      | object          | category          |\n",
        "| **MinTemp**       | float64         | ✔                 |\n",
        "| **MaxTemp**       | float64         | ✔                 |\n",
        "| **Rainfall**      | float64         | ✔                 |\n",
        "| **Evaporation**   | float64         | ✔                 |\n",
        "| **Sunshine**      | float64         | ✔                 |\n",
        "| **WindGustDir**   | object          | category          |\n",
        "| **WindGustSpeed** | float64         | ✔                 |\n",
        "| **WindDir9am**    | object          | category          |\n",
        "| **WindDir3pm**    | object          | category          |\n",
        "| **WindSpeed9am**  | float64         | ✔                 |\n",
        "| **WindSpeed3pm**  | float64         | ✔                 |\n",
        "| **Humidity9am**   | float64         | ✔                 |\n",
        "| **Humidity3pm**   | float64         | ✔                 |\n",
        "| **Pressure9am**   | float64         | ✔                 |\n",
        "| **Pressure3pm**   | float64         | ✔                 |\n",
        "| **Cloud9am**      | float64         | ✔                 |\n",
        "| **Cloud3pm**      | float64         | ✔                 |\n",
        "| **Temp9am**       | float64         | ✔                 |\n",
        "| **Temp3pm**       | float64         | ✔                 |\n",
        "| **RainToday**     | object          | bool              |\n",
        "| **RainTomorrow**  | object          | bool              |"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UPRXkxjd4559"
      },
      "outputs": [],
      "source": [
        "# Se realizan las asignaciones de datos correspondientes\n",
        "columns_types = {\n",
        "    'cat_columns': ['Location', 'WindGustDir', 'WindDir9am', 'WindDir3pm'],\n",
        "    'bool_columns': ['RainToday'],\n",
        "    'date_columns': ['Date'],\n",
        "    'cont_columns': ['MinTemp', 'MaxTemp', 'Rainfall', 'Evaporation', 'Sunshine',\n",
        "                     'WindGustSpeed', 'WindSpeed9am', 'WindSpeed3pm', 'Humidity9am',\n",
        "                     'Humidity3pm', 'Pressure9am', 'Pressure3pm', 'Cloud9am',\n",
        "                     'Cloud3pm', 'Temp9am', 'Temp3pm'],\n",
        "    'target_columns': ['RainTomorrow']\n",
        "}\n",
        "\n",
        "cat_columns = columns_types['cat_columns']\n",
        "bool_columns = columns_types['bool_columns']\n",
        "date_columns = columns_types['date_columns']\n",
        "cont_columns = columns_types['cont_columns']\n",
        "target_columns = columns_types['target_columns']\n",
        "\n",
        "df[cat_columns] = df[cat_columns].astype('category')\n",
        "df[date_columns] = df[date_columns].astype('datetime64[ns]')\n",
        "\n",
        "mapping_dict = {\"Yes\": 1, \"No\": 0}\n",
        "df[bool_columns + target_columns] = df[bool_columns + target_columns].applymap(lambda x: mapping_dict.get(x, x))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e_Km8a8nzSIz"
      },
      "outputs": [],
      "source": [
        "# Guardamos los tipos de columnas\n",
        "with open(\"./data/columnsTypes.json\", \"w\") as outfile:\n",
        "    json.dump(columns_types, outfile)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wCgbaCKQAKqL",
        "outputId": "6da71202-b10b-4948-fe4e-afe7d9cf17ce"
      },
      "outputs": [],
      "source": [
        "# Comprobamos que los tipos hayan quedado correctamente\n",
        "df.dtypes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EgiqXsNuCpcp"
      },
      "outputs": [],
      "source": [
        "# Finalmente agrego los tipos en estructuras separadas para facilitar el tratamiento\n",
        "cat_columns += bool_columns + target_columns"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oTwoWTbuWUIT"
      },
      "source": [
        "Chequeamos que los valores faltantes tengan el tipo adecuado (también se arreglan valores que tienen incorrecto -basado en intución y conocimiento del negocio)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CEwLVi8FWmez",
        "outputId": "6772b817-10b8-4365-c6c9-4d6031f7f658"
      },
      "outputs": [],
      "source": [
        "# Para columnas categóricas, simplemente nos fijamos en los valores que toma\n",
        "# Crear un diccionario para almacenar los valores únicos de cada columna categórica\n",
        "unique_values = {col: df[col].unique() for col in cat_columns}\n",
        "print_unique_values(unique_values)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bqi_vTNpcpC-"
      },
      "source": [
        "<p><em>\n",
        "Podemos observar que los valores tienen adecuandamente el tipo <code>np.nan</code>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fhtppuPbcyu5"
      },
      "source": [
        "Para las columnas continuas, simplemente corroboramos que todas tegan el mismo tipo de datos."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 571
        },
        "id": "SYUP1hpI7ZT1",
        "outputId": "28e95ee2-74d0-4a52-e4ca-e2e054b53da7"
      },
      "outputs": [],
      "source": [
        "# Llamar a la función y guardar la información en un DataFrame\n",
        "type_info_df = show_unique_types_as_df(df, cont_columns)\n",
        "type_info_df = type_info_df.groupby('Column').agg({'Type': concatenate_values}).reset_index()\n",
        "type_info_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_q-wT2NWehjY"
      },
      "source": [
        "<p><em>\n",
        "Podemos corroborar que únicamente se encuentran tipos <code>float</code>. Nota: Los valores <code>NaN</code> en pandas son del tipo <code>float</code> y se representan como <code>numpy.float64</code>. Para diferenciarlos específicamente como <code>NaN</code>, podemos usar <code>numpy.isnan</code> para identificar estos valores y considerarlos por separado. Esto lo hacemos en la función y por esto se muestra este tipo a parte.\n",
        "</em></p>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lgM77Fow_uMC"
      },
      "source": [
        "###### Momentos"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3aQr482G__BV"
      },
      "source": [
        "> En esta sección se analizan los momentos y datos estadísticos de las variables."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 390
        },
        "id": "X-oX8DruAE9c",
        "outputId": "7a5454da-f9f5-4318-b355-bda5551d6ebc"
      },
      "outputs": [],
      "source": [
        "# Datos estadísticos del conjunto\n",
        "df.describe()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ipdDzcvwETIq"
      },
      "source": [
        "<p><em>\n",
        "Dados los datos estadísticos de las variables continuas, podemos ver que los máximos y mínimos de las variables de temperatura son acordes con los datos históricos, así como la velocidad del viento. Otro punto a observar son que los máximos y mínimos de las presiones también están acordes con los datos de la presiona atmosférica promedio, medida en hecto-pascales.\n",
        "\n",
        "Otro punto a destacar es que los máximos y mínimos no se alejan tanto de la media con excepción de algunos atributos como `Rainfall` que el máximo se aleja muchísimo más de la media (mucho más de tres desviaciones), siendo posibles casos de análisis de valores atípicos.\n",
        "</em></p>\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 101
        },
        "id": "ym4x8uW4BNtD",
        "outputId": "76704800-b415-449b-863b-a0b26387eb0e"
      },
      "outputs": [],
      "source": [
        "# Valor más frecuente para variables categóricas\n",
        "df[cat_columns].mode()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 527
        },
        "id": "fJm5jjcZPKym",
        "outputId": "a6f70e45-b835-4ce4-f0f4-7f8022bf3bd3"
      },
      "outputs": [],
      "source": [
        "# Creamos una grilla de gráficos de barras\n",
        "plot_graph_on_grid(df, columns=cat_columns, num_cols=3, graph_type='bar')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sUmKwXGdIwzr"
      },
      "source": [
        "<p><em>\n",
        "Podemos observar que hay un gran desbalance de clases en la variable <code>RainToday</code> y en la variable objetivo <code>RainTomorrow</code>, en donde el porcentaje es el siguiente:\n",
        "</em></p>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 440
        },
        "id": "dAVyYkjXJplT",
        "outputId": "906efc63-3d9e-4b4d-fea0-db19452716b3"
      },
      "outputs": [],
      "source": [
        "# Creamos una grilla de graficos pie\n",
        "plot_graph_on_grid(df, columns=['RainToday', 'RainTomorrow'], num_cols=2, graph_type='pie', figsize=(8, 4))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nzu2ChruQT-S"
      },
      "source": [
        "<p><em>\n",
        "Este punto lo tenemos que tratar en otra sección: <strong>Clases desbalanceadas</strong>\n",
        "</p></em>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SyTJ_c6fRNIc"
      },
      "source": [
        "Analizamos la oblicuidad utilizando el estimador por defecto:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7rAZDouoRh6y",
        "outputId": "d6cc898c-acfd-4412-ac11-adecf9963e39"
      },
      "outputs": [],
      "source": [
        "# Calculamos la oblicuidad de cada columna.\n",
        "skewness = df.skew(numeric_only=True)\n",
        "skewness"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_ak-ch6oRueq"
      },
      "source": [
        "<p><em>\n",
        "Para mayor facilidad, analizamos los valores que son están alejados 0.5 del 0:\n",
        "</p></em>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4Ve-TPOmSTR7",
        "outputId": "88a8bcf9-ed04-4923-bda2-84e209095698"
      },
      "outputs": [],
      "source": [
        "skewed_colums = skewness[abs(skewness) > 0.5]\n",
        "skewed_colums"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a12Sy8vZTALd"
      },
      "source": [
        "<p><em>\n",
        "Podemos observar que las columnas con mas oblicuidad son <code>Rainfall</code> y <code>Evaporation</code>. Ambos sesgados a la derecha (cola pesada hacia la derecha).\n",
        "</em></p>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7_1PuA7LT5Co"
      },
      "source": [
        "Analizamos la curtosis para las columnas continuas (utilizando el estimador por defecto):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MC6z8gMfUD1v",
        "outputId": "ae576485-1439-47cc-82a3-7005ee05c748"
      },
      "outputs": [],
      "source": [
        "kurtosis = df.kurtosis(numeric_only=True)\n",
        "kurtosis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C3fL-zv-VnjS"
      },
      "source": [
        "<p><em>\n",
        "Realizamos el mismo proceso que en el paso anterior y analizamos aquellas variables que tienen una crutosis que se aleja más de 5 del 0:\n",
        "</em></p>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O8sPjiqgYxsJ",
        "outputId": "861bdbaa-5750-44f8-af61-2c19bfc1fa93"
      },
      "outputs": [],
      "source": [
        "kurtosis_colums = kurtosis[abs(kurtosis) > 5]\n",
        "kurtosis_colums"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3a7sRnhMY5ji"
      },
      "source": [
        "<p><em>\n",
        "Podemos ver que los atributos <code>Rainfall</code> y <code>Evaporation</code> son los que tienen mayor distancia de 0. Ambas leptocúrticas.\n",
        "\n",
        "Como también están sesgadas, hay una gran posibilidad de que se tenga que hacer un tratamiento diferentes de los datos.\n",
        "</em></p>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YQ0NTnGSZbIL"
      },
      "source": [
        "Verificamos los histogramas de las variables para validar las métricas anteriores:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "uOTf7qNWV1SD",
        "outputId": "ca0e42fb-b212-450e-c702-96d16fe4ae8d"
      },
      "outputs": [],
      "source": [
        "plot_graph_on_grid(df, columns=cont_columns, num_cols=3, graph_type='hist', figsize=(12, 17))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O6IFdFiNnVSX"
      },
      "source": [
        "<p><em>\n",
        "Luego de analizado los histogrmas para verificar las métricas anteriores, notamos algo importante. Las columnas <code>Clould9am</code> y <code>Cloud3pm</code> tienen una baja cardinalidad. Es más, los únicos valores son:\n",
        "</em></p>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qaz_l6YqpPtc",
        "outputId": "3db6ddb2-810b-4fc6-d412-30a031f63cbe"
      },
      "outputs": [],
      "source": [
        "unique_values = {col: df[col].unique() for col in ['Cloud9am', 'Cloud3pm']}\n",
        "print_unique_values(unique_values)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c_7jC2j9qf2v"
      },
      "source": [
        "<p><em>\n",
        "Por lo que al pricipio fueron definidas como \"Cuantitativa-Continua\" sería más bien una variable \"Cuantitativa-Discreta\":\n",
        "\n",
        "- `Cloud9am` → Tipo: Cuantitativa-Discreta | Datos específicos según hora del día\n",
        "- `Cloud3pm` → Tipo: Cuantitativa-Discreta | Datos específicos según hora del día\n",
        "\n",
        "</em></p>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VVJcwRX8JRej"
      },
      "source": [
        "Otro punto para verificar la normalidad de los datos, es hacer gráficos de QQ-plot:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "t10iMDroKPaT",
        "outputId": "03184c4e-738a-4a37-80e1-7d4c05b19589"
      },
      "outputs": [],
      "source": [
        "plot_graph_on_grid(df, columns=cont_columns, num_cols=3, graph_type='qq-plot', figsize=(12, 17))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VPvcL_HxLnX7"
      },
      "source": [
        "<p><em>\n",
        "Del gráfico podemos verificar las medidas anteriores obtenidas (la no normalidad de las columnas antes analizadas).\n",
        "</em></p>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DqaSSongeJQq"
      },
      "source": [
        "###### Valores faltantes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L_rdbeiVDeu4"
      },
      "source": [
        "Analizamos los valores faltantes de todas las columnas:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 791
        },
        "id": "VzUpmT5xD3aL",
        "outputId": "6d4e3134-a536-4130-cbae-2271806f6921"
      },
      "outputs": [],
      "source": [
        "missing_values_df = df.isna().sum().reset_index()\n",
        "missing_values_df.columns = ['Columna', 'Valores faltantes']\n",
        "\n",
        "# Calcular el porcentaje de valores faltantes por columna\n",
        "missing_percentage = (df.isna().sum() / len(df))\n",
        "missing_values_df['Proporción de faltantes'] = missing_percentage.values.round(2)\n",
        "\n",
        "# Ordenar de forma descendente según el porcentaje\n",
        "missing_values_df = missing_values_df.sort_values(by='Proporción de faltantes',\n",
        "                                                  ascending=False)\n",
        "\n",
        "missing_values_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Eeh_WeEkIrAj"
      },
      "source": [
        "<p><em>\n",
        "Como podemos observar, hay muchos datos faltantes que deben ser analizados en las siguientes secciones.\n",
        "</em></p>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AnKfb3_uhQym"
      },
      "source": [
        "Analizamos los valores faltantes de las columnas <code>RainToday</code> y <code>RainTomorrow</code>."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HALC91SZhgof",
        "outputId": "79401b97-57f9-492c-fb03-7932b38e1558"
      },
      "outputs": [],
      "source": [
        "columns = [\"RainToday\", \"RainTomorrow\"]\n",
        "for column in columns:\n",
        "    print(df[column].value_counts())\n",
        "    print_missing_perc(df, column)\n",
        "    print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UQuTbziPi6UB"
      },
      "source": [
        "<p><em>\n",
        "Se observa que la distribución de los valores que asumen las columnas son muy similares, lo cual tiene sentido debido a que el dataset elegido es una serie temporal y el valor de <code>RainTomorrow</code> en una observación es el valor de <code>RainToday</code> para el día siguiente.\n",
        "\n",
        "Encontramos además la misma proporción de valores nulos en ambas columnas.\n",
        "</em></p>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ynRlFmDircvT"
      },
      "source": [
        "Analizamos las columnas <code>Date</code> y <code>Location</code>."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i_GSeKSQrp6-"
      },
      "source": [
        "Para evaluar la coherencia interna del dataset verificamos que no haya observaciones con valor nulo en <code>RainTomorrow</code>, teniendo la observación del día siguiente con valor existente en la columna <code>RainToday</code>, para una misma estación meteorológica."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CbeHfnj2jUbG"
      },
      "outputs": [],
      "source": [
        "diff_one_day = df[\"Date\"].shift(-1) - df[\"Date\"] == pd.Timedelta(\"1 day\")\n",
        "same_location = df[\"Location\"] == df[\"Location\"].shift(-1)\n",
        "na_today_value_tomorrow = df[\"RainTomorrow\"].isna() & ~(df[\"RainToday\"].shift(-1).isna())\n",
        "\n",
        "filt = diff_one_day & same_location & na_today_value_tomorrow\n",
        "assert df.loc[filt, :].shape[0] == 0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7q3FAglIjUme"
      },
      "source": [
        "Graficamos las observaciones diarias en función de la estación meteorológica para ver con que registros contamos."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 715
        },
        "id": "3IJVRIkdsL21",
        "outputId": "9c17b1a1-3735-46b0-eb79-6e5c73f011c7"
      },
      "outputs": [],
      "source": [
        "plot_locations_over_time(df, color=spectral_palette[7])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iA5XJhvztZ6A"
      },
      "source": [
        "<p><em>\n",
        "Se observa que tenemos registros faltantes correspondientes a meses enteros para todas las estaciones meteorológicas. Además podemos observar que el inicio de la serie temporal para cada una de ellas difiere, siendo la observación más antigua de noviembre 2007.\n",
        "\n",
        "Encontramos que en algunas de las localidades contamos con mayor cantidad de valores nulos en la columna target <code>RainTomorrow</code>. A priori esperábamos encontrar que los valores nulos para esa variable se encontraran dónde la serie temporal se corta, sin embargo podemos ver que aparecen aleatoriamente a lo largo de la serie temporal, y que en algunas locaciones incluso tenemos datos válidos en la variable <code>RainTomorrow</code> a pesar de no contar con una observación para el día siguiente al registrado.\n",
        "\n",
        "Las estaciones con mayor cantidad de datos faltantes en la variable target son Melbourne y Wiliamtown. En lo que a Melbourne concierne, podemos observar que hay registros también en el aeropuerto de esa ciudad, por lo que podemos inferir que los patrones cimáticos de una de esas estaciones puede brindar información de utilidad para predecir si llueve al día siguiente en la otra. Extendiendo a todas las estaciones meteorológicas entendemos que incorporar la posición geográfica puede resultar de gran utilidad para el desarrollo de nuestro modelo predictivo.\n",
        "</em></p>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z31jeZ_-ePav"
      },
      "source": [
        "###### Valores atípicos"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pjvijx7k5XMt"
      },
      "source": [
        "Para la detección de valores atípicos, primeramente realizamos diagramas de cajas para visualizar los datos. Estos diagramas de cajas también nos permiten las características anteriormente medidas, como la oblicuidad, media, mediana, etc; y así también validar estas métricas antes obtenidas."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "BkWV5inyCHt1",
        "outputId": "394dbc42-6352-4756-921e-dfe1d8309bbc"
      },
      "outputs": [],
      "source": [
        "plot_graph_on_grid(df, columns=cont_columns, num_cols=3, graph_type='box-plot', figsize=(12, 17))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aWXSb7MHeRsi"
      },
      "source": [
        "<p><em>\n",
        "Podemos observar que las siguentes columnas tienen outlíers:\n",
        "</em></p>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wz0syVeOmzgB"
      },
      "source": [
        "- <code>MinTemp</code>\n",
        "- <code>MaxTemp</code>\n",
        "- <code>RainFall</code>\n",
        "- <code>Evaporation</code>\n",
        "- <code>WindGustSpeed</code>\n",
        "- <code>WindSpeed9am</code>\n",
        "- <code>WindSpeed3pm</code>\n",
        "- <code>Humidity9am</code>\n",
        "- <code>Pressure9am</code>\n",
        "- <code>Pressure3pm</code>\n",
        "- <code>Temp9am</code>\n",
        "- <code>Temp3pm</code>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1_qy1VVkm6Br"
      },
      "source": [
        "Porcentaje de outliers según IRQ y desviacón estándar:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 571
        },
        "id": "3sBgTQsnt__e",
        "outputId": "e672e2ca-b5de-4105-80c8-c75c82106b99"
      },
      "outputs": [],
      "source": [
        "outliers = outliers_iqr(df, cont_columns).sort_values(by='IRQ-Percentage', ascending=False)\n",
        "outliers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cACRoTEW5dsb"
      },
      "source": [
        "<p><em>\n",
        "Con la tabla podemos observar que aquellas columnas que se alejan de la distribución normal, como por ejemplo <code>Rainfall</code>, lo correcto sería tratar muchos más outliers mediante el rango inter cuartílico (que utiliza la mediana) que mediante la desviación estandar (que utiliza la media).\n",
        "</em></p>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CL2po6yoeUME"
      },
      "source": [
        "###### Correlación entre datos"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eDe_STj1tGZb"
      },
      "source": [
        "Análisis de correlación linear de los datos:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 634
        },
        "id": "_VWeWu5eAKqY",
        "outputId": "95d426b8-d990-4565-8b80-f099bcd79429"
      },
      "outputs": [],
      "source": [
        "# Correlación de los datos\n",
        "correlation_matrix = df.corr(numeric_only=True, method='pearson')\n",
        "correlation_matrix"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h8D4t4jEAKqY"
      },
      "source": [
        "Para mejor visualización realizamos un mapa de calor:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 794
        },
        "id": "jleUONxEuQ94",
        "outputId": "52875ffb-35b3-4c65-f6f4-80541a664fc6"
      },
      "outputs": [],
      "source": [
        "plot_heatmap(correlation_matrix)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y5VyjUfmusHL"
      },
      "source": [
        "<p><em>\n",
        "Podemos apreciar que hay datos que están bastante corelacionados. Para mejor observación, graficamos las columnas cuya correlación es mayor a 0.75 (establecidos como correlación de intensidad fuerte):\n",
        "</em></p>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 547
        },
        "id": "qxz4w-9cviQI",
        "outputId": "9b619cda-451e-4b36-f93d-3e88fb6ac68b"
      },
      "outputs": [],
      "source": [
        "# Eliminar la diagonal principal (auto-correlaciones)\n",
        "mask = np.eye(len(correlation_matrix), dtype=bool)\n",
        "correlation_matrix_no_diag = correlation_matrix.where(~mask)\n",
        "\n",
        "# Encontrar columnas altamente correlacionadas\n",
        "high_corr_columns = correlation_matrix_no_diag.columns[correlation_matrix_no_diag.abs().max() > 0.75]\n",
        "high_corr_matrix = correlation_matrix.loc[high_corr_columns, high_corr_columns]\n",
        "\n",
        "plot_heatmap(high_corr_matrix, figsize=(5, 5))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tUUUfkOA0XLS"
      },
      "source": [
        "<p><em>\n",
        "En este punto podemos ver las siguientes correlaciones fuertes:\n",
        "\n",
        "<code>MinTemp</code> ⟷ <code>Temp9am</code>\n",
        "\n",
        "<code>MaxTemp</code> ⟷ <code>Temp3am</code> | <code>Temp9am</code>\n",
        "\n",
        "<code>Pressure9am</code> ⟷ <code>Pressure3pm</code>\n",
        "\n",
        "<code>Temp3am</code> ⟷ <code>Temp9am</code>\n",
        "\n",
        "La relación en todos los casos es directa.\n",
        "\n",
        "Estas correlaciones hay que tenerlas en cuenta a la hora de entrenamor modelos que son suceptibles a este tipo de correlación (ej: modelos lineales). También hay que tener en cuenta estas correlaciones en el caso de necesitar imputación de datos.\n",
        "</em></p>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n3TD3F2C4Vv_"
      },
      "source": [
        "Otra forma de ver la correlación y afirmar aún más nuestras hipotesis, es graficando par a par (pairplot):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "fSqyj6qh4gGp",
        "outputId": "64ef7a8c-82cc-46e6-d327-1e8dd7f1b21d"
      },
      "outputs": [],
      "source": [
        "# sns.pairplot(df)\n",
        "columnas = list(set(cont_columns) -\n",
        "                set(['Cloud9am', 'Cloud3pm']) | set([\"RainTomorrow\"]))\n",
        "sns.pairplot(\n",
        "    df[columnas].sample(10000, random_state=random_state),\n",
        "    hue=\"RainTomorrow\",\n",
        "    palette=[spectral_palette[9], spectral_palette[1]],\n",
        "    diag_kind=\"kde\",\n",
        "    plot_kws={'alpha': 0.2}\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IRASf4IXo19p"
      },
      "source": [
        "<p><em>\n",
        "En este ploteo podemos observar la correlación lineal entre ciertas variables analizadas anteriormente. Además, encontramos que las variables <code>Humidity3pm</code> y <code>Sunshine</code> resultan sumamente promimsorias para la separación de clases en la variable objetivo. Sin embargo, recordamos que la variable <code>Sunshine</code> posee datos faltantes en casi la mitad de los registros. Realizar un buen trabajo de imputación de datos faltantes resulta entonces indispensable para obtener una buena performance en nuestro tarea predictiva.\n",
        "</em></p>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-g2W-4j9ub5L"
      },
      "source": [
        "Análisis de la correlación según <code>Date</code> y <code>Location</code>."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oBUPmIz1uu_P"
      },
      "source": [
        "Geolocalizamos las estaciones meteorológicas utilizando el servicio Open Street Map para obtener la posición geográfica de cada una de ellas."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "zHgI9_pBuxg7",
        "outputId": "be92c01e-25f8-4984-b807-10d0c3020dba"
      },
      "outputs": [],
      "source": [
        "country = \"Australia\"\n",
        "\n",
        "world = gpd.read_file(get_path('naturalearth_lowres'))\n",
        "gdf_australia = world[world.name == country]\n",
        "\n",
        "# Solve manually some mistaken names\n",
        "mapping_dict = {\"Dartmoor\": \"DartmoorVillage\", \"Richmond\": \"RichmondSydney\"}\n",
        "\n",
        "# Una vez aplicado map, quedan valores NaN, por lo que se completa con los valores que ya tenía,\n",
        "# o sea para cualquier valor que sea NaN después de aplicar map(),\n",
        "# usa el valor original que estaba en esa posición en la columna 'Location\n",
        "df[\"Location\"] = df[\"Location\"].map(mapping_dict).fillna(df[\"Location\"])\n",
        "\n",
        "locations = df[\"Location\"].unique()\n",
        "\n",
        "# Separa las ubicaciones en camelCase con un espacio. Ej: NorthRyde -> North Ryde\n",
        "locations = [re.sub(r'([a-z])([A-Z])', r'\\1 \\2', l) for l in locations]\n",
        "\n",
        "locs = []\n",
        "lats = []\n",
        "lons = []\n",
        "for location in locations:\n",
        "    try:\n",
        "        lat, lon = ox.geocode(location + f\", {country}\")\n",
        "\n",
        "        locs.append(location.replace(\" \", \"\"))\n",
        "        lats.append(lat)\n",
        "        lons.append(lon)\n",
        "    except Exception as e:\n",
        "        print(f\"Error retrieving coordinates for {location}: {e}\")\n",
        "\n",
        "df_locations = pd.DataFrame({\n",
        "    'Location': locs,\n",
        "    'Lat': lats,\n",
        "    'Lon': lons\n",
        "})\n",
        "geometry = [Point(lon, lat) for lon, lat in zip(\n",
        "    df_locations['Lon'], df_locations['Lat'])]\n",
        "gdf_locations = gpd.GeoDataFrame(\n",
        "    df_locations, geometry=geometry, crs=\"EPSG:4326\")\n",
        "gdf_locations.to_file('./data/gdf_locations.geojson', driver='GeoJSON')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 518
        },
        "id": "zb-NlmuFvvLQ",
        "outputId": "0e0d9602-c5e7-4d89-8ad8-ad453c082ec6"
      },
      "outputs": [],
      "source": [
        "fig, ax = plt.subplots(figsize=(10, 6))\n",
        "gdf_australia.plot(ax=ax, edgecolor='k', facecolor=spectral_palette[4])\n",
        "\n",
        "# Plot locations\n",
        "gdf_locations.plot(ax=ax, marker='o', color='black',\n",
        "                   markersize=10, label='Locations')\n",
        "\n",
        "for idx, row in gdf_locations.iterrows():\n",
        "    ax.text(\n",
        "        row['geometry'].x,\n",
        "        row['geometry'].y + .2,\n",
        "        row['Location'],\n",
        "        fontsize=5,\n",
        "        ha='center',\n",
        "        va='bottom'\n",
        "    )\n",
        "\n",
        "ax.set_xticks([])\n",
        "ax.set_yticks([])\n",
        "\n",
        "ax.set_aspect('equal', adjustable='box')\n",
        "\n",
        "plt.title(\n",
        "    \"Ubicación de las estaciones meteorológicas observadas\",\n",
        "    fontsize=10,\n",
        ")\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8FKeflvkv3V8"
      },
      "source": [
        "<p><em>\n",
        "Podemos ver una gran cantidad de estaciones meteorológicas concentradas en la costa sureste de Australia, y en menor medida en la costa suroeste. Además notamos que algunas de las estaciones se encuentran muy cercanas entre si, lo cual puede traer aparejado una correlación entre los patrones climáticos.\n",
        "</em></p>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RbhsUb2pwajh"
      },
      "source": [
        "Analizamos el coeficiente Phi para la variable <code>RainToday</code> comparando entre pares de estaciones meteorológicas y cruzando con la información de la distancia euclideana entre ellas."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EeJbV6cjwwac"
      },
      "outputs": [],
      "source": [
        "locations = df[\"Location\"].unique()\n",
        "location_pairs = list(combinations(locations, 2))\n",
        "\n",
        "df_location_pairs = pd.DataFrame(location_pairs, columns=['LocationA', 'LocationB'])\n",
        "df_location_pairs['phi'] = np.nan\n",
        "df_location_pairs['pvalue'] = np.nan\n",
        "\n",
        "for index, row in df_location_pairs.iterrows():\n",
        "    loc1, loc2 = row['LocationA'], row['LocationB']\n",
        "    df_pair = df[df['Location'].isin([loc1, loc2])]\n",
        "    df_pivot = df_pair.pivot(index='Date', columns='Location', values='RainToday').dropna()\n",
        "\n",
        "    if not df_pivot.empty:\n",
        "        confusion_matrix = pd.crosstab(df_pivot[loc1], df_pivot[loc2])\n",
        "        phi, pvalue = phi_coefficient(confusion_matrix.values)\n",
        "        df_location_pairs.at[index, 'phi'] = phi\n",
        "        df_location_pairs.at[index, 'pvalue'] = pvalue"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hJZT0b9ixRC8"
      },
      "outputs": [],
      "source": [
        "df_location_a = df_location_pairs[[\"LocationA\"]].merge(\n",
        "    gdf_locations[[\"Location\", \"geometry\"]],\n",
        "    how=\"left\",\n",
        "    left_on=\"LocationA\",\n",
        "    right_on=\"Location\",\n",
        ")\n",
        "gdf_location_a = gpd.GeoDataFrame(df_location_a, geometry=\"geometry\")\n",
        "gdf_location_a_gda94 = gdf_location_a.to_crs(epsg=3112)\n",
        "\n",
        "df_location_b = df_location_pairs[[\"LocationB\"]].merge(\n",
        "    gdf_locations[[\"Location\", \"geometry\"]],\n",
        "    how=\"left\",\n",
        "    left_on=\"LocationB\",\n",
        "    right_on=\"Location\",\n",
        ")\n",
        "gdf_location_b = gpd.GeoDataFrame(df_location_b, geometry=\"geometry\")\n",
        "gdf_location_b_gda94 = gdf_location_b.to_crs(epsg=3112)\n",
        "\n",
        "distance_ab = gdf_location_a_gda94[\"geometry\"].distance(gdf_location_b_gda94[\"geometry\"])\n",
        "\n",
        "df_location_pairs[\"distance\"] = distance_ab / 1000 # distance in kilometers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 383
        },
        "id": "g2qb6uA-xUd3",
        "outputId": "8d9a3c93-cc19-49ae-8c47-ef1122191434"
      },
      "outputs": [],
      "source": [
        "df_location_pairs.sort_values(by=\"phi\", ascending=False).head(10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gVJpVhLYxeKF"
      },
      "source": [
        "<p><em>\n",
        "Observamos que, tal como intuíamos, las estaciones con mayor coeficiente Phi tienen entre si para la variable <code>RainToday</code>, son aquellos que se encuentran cerca geográficamente.\n",
        "</em></p>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pmsOvR7RxvWv"
      },
      "source": [
        "Vamos a gráficar la relación entre la distancia geográfica y la correlación calculada mediante el coeficiente Phi para todos los pares de estaciones meteorológicas en el dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 760
        },
        "id": "FXV3b4Dgxydk",
        "outputId": "502aeb46-6eed-4d2a-8b12-e1f516444824"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(8, 8))\n",
        "\n",
        "df_location_pairs[\"significant\"] = df_location_pairs['pvalue'] <= .05\n",
        "colors = [spectral_palette[10] if sig else spectral_palette[2] for sig in df_location_pairs['significant']]\n",
        "\n",
        "sc = plt.scatter(df_location_pairs['phi'], np.log(df_location_pairs['distance']),\n",
        "                 s=12, c=colors, alpha=.8, edgecolors='w', linewidth=0.5)\n",
        "\n",
        "plt.legend(handles=[\n",
        "    plt.Line2D([0], [0], marker='o', color='w', markerfacecolor=spectral_palette[10], markersize=5, label='p-value <= 0.05'),\n",
        "    plt.Line2D([0], [0], marker='o', color='w', markerfacecolor=spectral_palette[2], markersize=5, label='p-value > 0.05')\n",
        "])\n",
        "\n",
        "plt.xlabel('Phi Coefficient')\n",
        "plt.ylabel('Log de la distancia en km')\n",
        "plt.title('Phi coefficient entre dos localidades vs distancia entre ellas\\nutilizando la variable binaria RainToday para cada día')\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uIY345BryArr"
      },
      "source": [
        "<p><em>\n",
        "Efectivamente, encontramos una importante correlación visual entre ambas variables graficadas: la distancia y la correlación para la variable <code>RainToday</code> para cada par de estaciones.\n",
        "\n",
        "Esta situación resulta interesante en cuanto refuerza nuestra hipótesis de que la codificación espacial de las estaciones meteorológicas resulta informativa para un modelo encargado de predecir lluvia para el día siguiente.\n",
        "\n",
        "Además, entendemos que debemos de estar atentos al momento de dividir nuestro dataset para entrenamiento y testeo del mismo. Si entrenamos nuestro modelo con datos de un día específico en estaciones meteorológicas cercanas entre si, y luego lo testeamos con una observación del mismo día pero de otra estación también cercana, corremos el riesgo de que el modelo aprenda que en esa zona geográfica llovió al día siguiente en lugar de realmente realizar una predicción desconociendo el futuro. Por esto consideramos que nuestro set de testeo y validación debe comprender un periodo de tiempo definido e incluir a todas las estaciones para ese período.\n",
        "</em></p>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kaxI-1NBvc7_"
      },
      "source": [
        "Finalmente vamos a analizar la relación existente entre la variables categóricas relacionadas a la dirección que puede tomar el viento. Como vimos anteriormente son 16 las categorías posibles."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8lrG39d4vlUz",
        "outputId": "3d9ad1fe-b6c1-44ba-d66f-05da30fd4d38"
      },
      "outputs": [],
      "source": [
        "wind_dir_columns = [\"WindGustDir\", \"WindDir9am\", \"WindDir3pm\"]\n",
        "\n",
        "uniques_dirs = set(chain.from_iterable(df[column].unique() for column in wind_dir_columns))\n",
        "print(\"Valores tipo string en las columnas WindDir: \", ' - '.join(d for d in uniques_dirs if isinstance(d, str)))\n",
        "print(\"Otros valores en las columnas WindDir: \",' - '.join(str(d) for d in uniques_dirs if not isinstance(d, str)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rfJOFhJbwRAn"
      },
      "source": [
        "Se observan 16 categorías, cada una de ellas representando una dirección posible del viento con una resolución de 22.5°. Vamos a transformar la dirección del viento a grados considerando la dirección 'E' como nuestro 0° (y por lo tanto también nuestro 360°). Este método nos permitirá ordenar los valores para poder analizar graficamente la relación entre las variables WindDir9am y WindDir3pm."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j_p3rhM4wTGP"
      },
      "outputs": [],
      "source": [
        "df_wind_dir = df[wind_dir_columns]\n",
        "\n",
        "dirs = [\"E\", \"ENE\", \"NE\", \"NNE\", \"N\", \"NNW\", \"NW\", \"WNW\", \"W\", \"WSW\", \"SW\", \"SSW\", \"S\", \"SSE\", \"SE\", \"ESE\"]\n",
        "angles = list(np.arange(0, 360, 22.5))\n",
        "mapping_dict = {d: a for (d, a) in zip(dirs, angles)}\n",
        "\n",
        "df_wind_dir[wind_dir_columns] = df_wind_dir[wind_dir_columns].applymap(lambda x: mapping_dict.get(x, x))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 738
        },
        "id": "C6iORz9hxAHO",
        "outputId": "15c9b31c-e573-490c-cb59-3515b84ff6a7"
      },
      "outputs": [],
      "source": [
        "confusion_matrix = pd.crosstab(df_wind_dir[\"WindDir9am\"], df_wind_dir[\"WindDir3pm\"])\n",
        "\n",
        "# Plot confusion matrix as a heatmap\n",
        "plt.figure(figsize=(8, 8))\n",
        "sns.heatmap(confusion_matrix, annot=True, fmt='d', cmap='coolwarm', xticklabels=dirs, yticklabels=dirs, cbar=False)\n",
        "plt.xlabel('WindDir9am')\n",
        "plt.ylabel('WindDir3pm')\n",
        "plt.title('Confusion Matrix Heatmap')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zs9K6fwuxXLn"
      },
      "source": [
        "_Se observa una estrecha relación visual entre la dirección del viento registrada a las 9am y la dirección del viento registrada a las 3pm. Debido a la condición circular de las posibles direcciones que el viento puede tomar, aparecen valores altos en las esquinas inferior izquierda y superior derecha del gráfico._"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q_L5zlewRoQc"
      },
      "source": [
        "##### 2.1.5.2. Requerimientos de datos"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0eYzv2e2AKqd"
      },
      "source": [
        "Como resumen del análisis de datos, éstos presentan la siguiente información:\n",
        "\n",
        "<font color='blue'>ℹ</font> El conjunto contiene un total de <code>145460</code> observaciones.\n",
        "\n",
        "<font color='blue'>ℹ</font> Las columnas cualitativas-nominales son: <code>WindGustDir</code> | <code>WindDir9am</code> | <code>WindDir3pm</code> | <code>Location</code>\n",
        "\n",
        "<font color='blue'>ℹ</font> Las columnas cuantitativas-continuas son: <code>MinTemp</code> | <code>MaxTemp</code> | <code>Rainfall</code> | <code>Evaporation</code> | <code>Sunshine</code> | <code>WindGustSpeed</code> | <code>WindSpeed9am</code> | <code>WindSpeed3pm</code> | <code>Humidity9am</code> | <code>Humidity3pm</code> | <code>Pressure9am</code> | <code>Pressure3pm</code> | <code>Cloud9am</code> | <code>Cloud3pm</code> | <code>Temp9am</code> | <code>Temp3pm</code>\n",
        "\n",
        "<font color='blue'>ℹ</font> Hay una columna booleana: <code>RainToday</code>\n",
        "\n",
        "<font color='blue'>ℹ</font> Hay una columna del tipo fecha: <code>Date</code>\n",
        "\n",
        "<font color='green'>✔</font> La variable objetivo es del tipo booleanna: <code>RainTomorrow</code>\n",
        "\n",
        "<font color='green'>✔</font> No presentan datos duplicados.\n",
        "\n",
        "<font color='red'>❌</font> Gran propoción de valores nulos en: <code>Evaporation</code> | <code>Sunshine</code> | <code>Cloud9am</code> | <code>Cloud3pm</code>\n",
        "\n",
        "<font color='green'>✔</font> No hay valores nulos en las columnas: <code>Date</code> | <code>Location</code>\n",
        "\n",
        "<font color='green'>✔</font> Máximos y mínimos acordes.\n",
        "\n",
        "<font color='yellow'>⚠</font> Hay un desbalance de clases en la variable target.\n",
        "\n",
        "<font color='green'>✔</font> Hay muchas columnas que se pueden estandarizar y siguen una distribución cuasi-normal como: <code>MinTemp</code> | <code>MaxTemp</code> | <code>Humidity3pm</code> | <code>Pressure9am</code> | <code>Pressure3pm</code> | <code>Temp9am</code>\n",
        "\n",
        "<font color='yellow'>⚠</font> Hay columnas que son no-normales como: <code>Rainfall</code> | <code>Evaporation</code>\n",
        "\n",
        "<font color='yellow'>⚠</font> Hay columnas que tienen colas pesadas y livianas como: <code>Sunshine</code> | <code>WindGustSpeed</code> | <code>WindSpeed9am</code> | <code>WindSpeed3pm</code> | <code>Humidity9am</code>\n",
        "\n",
        "<font color='yellow'>⚠</font> Las columnas <code>Colud9am</code> y <code>Cloud3pm</code> podrían llevar un tratamiento de cuantitativas-discretas.\n",
        "\n",
        "<font color='yellow'>⚠</font> Hay valores faltantes en el resto de las columnas, pero la mayoría (del 40 para arriba son las columnas): <code>Evaporation</code> | <code>Sunshine</code> | <code>Cloud9am</code> | <code>Cloud3pm</code>\n",
        "\n",
        "<font color='blue'>ℹ</font> Hay relación entre los datos faltantes (<code>RainTomorrow</code>), fechas y localidades.\n",
        "\n",
        "<font color='red'>❌</font> La columna <code>RainFall</code> contiene muchos outliers.\n",
        "\n",
        "<font color='yellow'>⚠</font> Hay variables áltamente co-relacionadas como:\n",
        "- <code>MinTemp</code> ⟷ <code>Temp9am</code>\n",
        "- <code>MaxTemp</code> ⟷ <code>Temp3am</code> | <code>Temp9am</code>\n",
        "- <code>Pressure9am</code> ⟷ <code>Pressure3pm</code>\n",
        "- <code>Temp3am</code> ⟷ <code>Temp9am</code>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C6ZbqjAO6ygC"
      },
      "source": [
        "#### 2.1.6. Revisión de los documentos de salida\n",
        "\n",
        "<!-- TODO: Borrar? -->"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0u8rYBIMDXFJ"
      },
      "source": [
        "Actualmente, el documento de salida es únicamente este notebook autocontenido."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ir2QRoQ26ygD"
      },
      "source": [
        "### 2.2. Ingeniería de datos (Data Engineering)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<!-- <font color='red'>TODO:</font>\n",
        "<font color='red'>Estaría faltando probar Outliers de normales</font>\n",
        "<font color='red'>Estaría faltando probar MICE</font>\n",
        "<font color='red'>Luego el tratamiento de datos co-relacionados?</font>\n",
        "<font color='red'>Normailización de variables?</font> -->"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9ofuA4tLVNy4"
      },
      "source": [
        "Cargamos los archivos de la etapa anterior:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z5rw7jhLLwMb",
        "outputId": "5f550fb2-1c65-4af0-c88c-a6816fd20bbf"
      },
      "outputs": [],
      "source": [
        "# Importamos el conjunto\n",
        "file_name = './data/weatherAUS.csv'\n",
        "try:\n",
        "    df = pd.read_csv(file_name)\n",
        "    print('Dataset local')\n",
        "except:\n",
        "    raise Exception('Error al encontrar el archivo')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GaWkvT_5VNy5",
        "outputId": "feac9ff4-1340-4947-de6a-ba93f62384d9"
      },
      "outputs": [],
      "source": [
        "# Importamos los tipos de datos\n",
        "file_name = './data/columnsTypes.json'\n",
        "try:\n",
        "    with open(file_name, 'r') as fp:\n",
        "        columns_types = json.load(fp)\n",
        "    print('columnsTypes local')\n",
        "except:\n",
        "    raise Exception('Error al encontrar el archivo')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "02gaPNjXVNy5",
        "outputId": "129a6297-a400-4e98-d1d3-cdf02f3916c8"
      },
      "outputs": [],
      "source": [
        "# Importamos las coordenadas\n",
        "file_name = './data/gdf_locations.geojson'\n",
        "try:\n",
        "    gdf_locations = gpd.read_file(file_name)\n",
        "    print('gdf_locations local')\n",
        "except:\n",
        "    raise Exception('Error al encontrar el archivo')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aZIeMKWR6ygD"
      },
      "source": [
        "#### 2.2.1. Seleccionar datos"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mNIdUhg7s248"
      },
      "source": [
        "Dado que la variabable objetivo tiene datos *nulos*, primeramente, antes de realizar las diversas transformaciones al conjunto, eliminamos dichos registros del conjunto:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iNs3izDkswnt"
      },
      "outputs": [],
      "source": [
        "df.dropna(subset=['RainTomorrow'], inplace=True, ignore_index=True)\n",
        "df_pre_processed = df.copy()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rnq-srUM6ygE"
      },
      "source": [
        "#### 2.2.2. Limpiar datos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QyNSpsq2N2Ua"
      },
      "outputs": [],
      "source": [
        "# Creamos el pipeline vacio\n",
        "data_pipeline = Pipeline(steps=[])\n",
        "target_pipeline = Pipeline(steps=[])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2oMW4ECFwukB"
      },
      "source": [
        "##### 2.2.2.1 Tipos de datos"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fhhpDBPXVNy6"
      },
      "source": [
        "Cargamos las variables que mantiene los tipos de datos:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e65GOIiUVNy6"
      },
      "outputs": [],
      "source": [
        "cat_columns = columns_types['cat_columns']\n",
        "bool_columns = columns_types['bool_columns']\n",
        "date_columns = columns_types['date_columns']\n",
        "cont_columns = columns_types['cont_columns']\n",
        "target_columns = columns_types['target_columns']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qzh3OAO1VNy7"
      },
      "source": [
        "Transformamos los datos a sus para ver si no tenemos problemas de datos:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_L2_O4OsVNy7"
      },
      "outputs": [],
      "source": [
        "col_types_transf = ColumnTransformer(\n",
        "    [('categories', FunctionTransformer(to_category), cat_columns),\n",
        "     ('date', FunctionTransformer(to_datetime), date_columns),\n",
        "     ('bool', FunctionTransformer(map_bool), bool_columns)],\n",
        "    remainder='passthrough',\n",
        "    verbose_feature_names_out=False\n",
        ").set_output(transform='pandas')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eYP0pFcJVNy7"
      },
      "source": [
        "Probamos el paso:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 444
        },
        "id": "Cj5qsrSnVNy8",
        "outputId": "012452db-e333-46d6-c540-f5de5115293f"
      },
      "outputs": [],
      "source": [
        "df = col_types_transf.fit_transform(df)\n",
        "df['RainTomorrow'] = map_bool(df['RainTomorrow'])\n",
        "df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8UYL7rADVNy8"
      },
      "source": [
        "Agregamos estos pasos al pipeline:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 112
        },
        "id": "pXHuJmr2VNy8",
        "outputId": "9ce77ca4-7c05-465c-ffb8-2413c25978d5"
      },
      "outputs": [],
      "source": [
        "target_pipeline.steps.append(('mapping', FunctionTransformer(map_bool)))\n",
        "target_pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 178
        },
        "id": "qqUec817VNy8",
        "outputId": "54559ee3-37cb-4519-e903-b1c9251c5995"
      },
      "outputs": [],
      "source": [
        "data_pipeline.steps.append(('feature_transf', col_types_transf))\n",
        "data_pipeline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### 2.2.2.2. Tratamiento de valores a tipicos"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Las columnas identificadas anteriormente que contenían valores atípicos fueron las siguientes:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- <code>MinTemp</code>\n",
        "- <code>MaxTemp</code>\n",
        "- <code>RainFall</code>\n",
        "- <code>Evaporation</code>\n",
        "- <code>WindGustSpeed</code>\n",
        "- <code>WindSpeed9am</code>\n",
        "- <code>WindSpeed3pm</code>\n",
        "- <code>Humidity9am</code>\n",
        "- <code>Pressure9am</code>\n",
        "- <code>Pressure3pm</code>\n",
        "- <code>Temp9am</code>\n",
        "- <code>Temp3pm</code>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "De estas columnas, las siguientes tienen un comportamiento cuasi-normal, por lo que podemos calcular los valores atípicos utilizando la desviación estandar:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- <code>MinTemp</code>\n",
        "- <code>MaxTemp</code>\n",
        "- <code>Humidity9am</code>\n",
        "- <code>Pressure9am</code>\n",
        "- <code>Pressure3pm</code>\n",
        "- <code>Temp9am</code>\n",
        "- <code>Temp3pm</code>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Para el resto de las columnas que presentan valores atípicos, se puede utilizar el rango intercuartílico, ya que claramente no siguen una distribución normal:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- <code>Rainfall</code>\n",
        "- <code>Evaporation</code>\n",
        "- <code>WindGustSpeed</code>\n",
        "- <code>WindSpeed9am</code>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "columns = ['Rainfall', 'Evaporation', 'WindGustSpeed', 'WindSpeed9am']\n",
        "clap_outliers_irq_transf = ClapOutliersTransformerIRQ(columns=columns)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Probamos el paso\n",
        "df = clap_outliers_irq_transf.fit_transform(df)\n",
        "df[columns].describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Agregamos al pipeline\n",
        "data_pipeline.steps.append(('clap_outliers_irq_transf', clap_outliers_irq_transf))\n",
        "data_pipeline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "55mbnx1K-Y51"
      },
      "source": [
        "##### 2.2.2.2 Valores faltantes\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WBBtI3sGEQPR"
      },
      "source": [
        "Como se observó en la sección anterior, se tienen variables tanto numéricas como categóricas a las que les faltan algunos valores."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mIyG6Nii3nk0"
      },
      "source": [
        "###### Considerando las variables categóricas\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Iju--NvEWK3"
      },
      "source": [
        "Se toma la moda para completar los valores faltantes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o-oOm1dq3qXq"
      },
      "outputs": [],
      "source": [
        "df_cat_imputed = df.copy()\n",
        "\n",
        "# Imputación con la Moda para Variables Categóricas\n",
        "categorical_imputer = SimpleImputer(strategy='most_frequent')\n",
        "df_cat_imputed[cat_columns] = categorical_imputer.fit_transform(df_cat_imputed[cat_columns])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 791
        },
        "id": "FFDCprwBDCq6",
        "outputId": "b08393bc-c745-4d11-ca9c-c7d4b9a15522"
      },
      "outputs": [],
      "source": [
        "missing_values = df_cat_imputed.isna().sum()\n",
        "observations = len(df_cat_imputed)\n",
        "\n",
        "missing_values_df = pd.DataFrame({\n",
        "    'Variable': missing_values.index,\n",
        "    'Valores faltantes': missing_values.values,\n",
        "    'Cantidad de observaciones': observations,\n",
        "    'Porcentaje valores faltantes': (missing_values.values / observations) * 100\n",
        "})\n",
        "\n",
        "missing_values_df.sort_values(by=\"Porcentaje valores faltantes\", ascending=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 791
        },
        "id": "fD0xC08tI8lD",
        "outputId": "bab306c8-518e-45cd-bbef-b5a94bf62b1d"
      },
      "outputs": [],
      "source": [
        "# Chequeamos los valores de nuevo\n",
        "missing_values = df_cat_imputed.isna().sum()\n",
        "observations = len(df_cat_imputed)\n",
        "\n",
        "missing_values_df = pd.DataFrame({\n",
        "    'Variable': missing_values.index,\n",
        "    'Valores faltantes': missing_values.values,\n",
        "    'Cantidad de observaciones': observations,\n",
        "    'Porcentaje valores faltantes': (missing_values.values / observations) * 100\n",
        "})\n",
        "\n",
        "missing_values_df.sort_values(by=\"Porcentaje valores faltantes\", ascending=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yizWkeewVNy_"
      },
      "source": [
        "Creamos el paso para variables categóricas:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WIA0q_KDVNy_"
      },
      "outputs": [],
      "source": [
        "cat_imputer = ('cat_missing_values_imputer',\n",
        "               SimpleImputer(strategy='most_frequent'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eHiwF-Cx3d88"
      },
      "source": [
        "###### Considerando las variables numéricas"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QQXVzToiG1Ox"
      },
      "source": [
        "\n",
        "Considerando que las faltas son por razones aleatorias, y, dado que la mayoría de las variables presentan oblicuidad, se considera la mediana como un buen candidato para reemplazar a los valores faltantes. Para este caso, se puede utilizar el <code>SimpleImputer</code> considerando una imputación de una variable.\n",
        "\n",
        "Alternativamente, se puede utilizar un método multivariado como <code>KNN</code> (vecinos cercanos) y comparar con la imputación simple."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AaWb5tT05DYY"
      },
      "source": [
        "De esta forma se tienen dos alternativas que pueden compararse para determinar cuál es mejor."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "30Bb5O_V4pVV"
      },
      "outputs": [],
      "source": [
        "# Se realiza la imputación simple de una sola variable utilizando la mediana:\n",
        "df_mean_imputed = simple_imputer_mean(df_cat_imputed.copy(), cont_columns)\n",
        "df_mean_imputed.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GAPdiG7U4zSD"
      },
      "outputs": [],
      "source": [
        "# Se realiza la imputación multivariada utilizando los vecinos cercanos (KNN):\n",
        "df_knn_imputed = knn_imputer(df_cat_imputed.copy(), cont_columns, n_neighbors=3)\n",
        "df_knn_imputed.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6gGEmglOJvbq"
      },
      "source": [
        "De esta forma se tienen dos opciones para comparar:\n",
        "`df_mean_imputed` y `df_knn_imputed`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E6_93Y8bVNzA"
      },
      "source": [
        "Creamos el paso para variables continuas:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oMEI8sGoVNzA"
      },
      "outputs": [],
      "source": [
        "cont_imputer = ('cont_missing_values_imptuer', SimpleImputer(strategy='mean'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "msaVPOAaPKdm"
      },
      "source": [
        "###### Análisis post-imputación"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ReJ6e1MDG4wR"
      },
      "source": [
        "Para entender cómo ha cambiado el dataset luego de las dos alternativas de imputación para los datos numéricos, se puede realizar una análisis de las estadísticas descriptivas del dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oQY9K9pVcdMs"
      },
      "outputs": [],
      "source": [
        "# Estadísticas descriptivas\n",
        "print(\"Estadísticas - dataset original:\")\n",
        "df.describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ale24dubcdMt"
      },
      "outputs": [],
      "source": [
        "print(\"Estadísticas - imputación (mediana):\")\n",
        "df_mean_imputed.describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "KerFNbLrPu-k"
      },
      "outputs": [],
      "source": [
        "print(\"Estadísticas - imputación (KNN):\")\n",
        "df_knn_imputed.describe()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f2_akPKgcdMt"
      },
      "source": [
        "<p><em>\n",
        "Como podemos observar, para ningún caso se produjo una variación grande de las estadísticas.\n",
        "</em></p>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qmgRVSiEcdMt"
      },
      "source": [
        "<strong>Análisis por variable</strong>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gStF8EwCRdhK"
      },
      "source": [
        "Se consideran solo las que tenían un gran porcentaje de valores faltantes (>10%)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3BsXUU-zRz0z"
      },
      "outputs": [],
      "source": [
        "# Sunshine (48%), Evaporation (43%), Cloud3pm (40%), Cloud9am (38%), Pressure9am (10%), Pressure3pm (10%)\n",
        "top_six = ['Sunshine', 'Evaporation', 'Cloud3pm', 'Cloud9am', 'Pressure9am', 'Pressure3pm']\n",
        "\n",
        "for variable in top_six:\n",
        "  plot_distributions(df, df_mean_imputed, df_knn_imputed, variable, figsize=(15, 4))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "war43mFPUTrV"
      },
      "source": [
        "<p><em>\n",
        "Como se puede observar en los gráficos, las distribuciones se han visto afectadas (en especial <code>Sunshine</code>) por las imputaciones.\n",
        "El caso de Sunshine es particular y debería ser analizado a mayor profundidad para entender que opciones pueden existir para eliminar el sesgo excesivo que se observa.\n",
        "En general, el método de imputación utilizando KNN es mejor que el método que se basa solamente en la mediana.\n",
        "</em></p>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5jtqV-tGVNzC"
      },
      "source": [
        "Finalmente, creamos los pasos y los agregamos al pipeline:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lDHldV4GVNzC"
      },
      "outputs": [],
      "source": [
        "missing_values_transf = ColumnTransformer(\n",
        "    [('cat_imputer', Pipeline([cat_imputer]), cat_columns + bool_columns),\n",
        "     ('cont_imputer', Pipeline([cont_imputer]), cont_columns)],\n",
        "    remainder='passthrough',\n",
        "    verbose_feature_names_out=False).set_output(transform='pandas')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R5y9CJ-rVNzC"
      },
      "source": [
        "Probamos el paso:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VPbtrrOzVNzD"
      },
      "outputs": [],
      "source": [
        "df = missing_values_transf.fit_transform(df)\n",
        "df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m_ReL6hEVNzD"
      },
      "source": [
        "Agregamos al pipeline:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yGOIW4taVNzD"
      },
      "outputs": [],
      "source": [
        "data_pipeline.steps.append(('missing_values_transf', missing_values_transf))\n",
        "data_pipeline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xdKyormU6ygE"
      },
      "source": [
        "#### 2.2.3. Codificar de variables categóricas"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dSRT2Zr3xu2X"
      },
      "source": [
        "A partir del análisis realizado hasta el momento encontramos las siguientes variables categóricas a codificar:\n",
        "\n",
        "- <code>Date</code>\n",
        "- <code>Location</code>\n",
        "- <code>WindGustDir</code>, <code>WindDir9am</code>, <code>WindDir3pm</code>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wKtPNakAx-Ee"
      },
      "source": [
        "##### 2.2.3.1. Variable \"Date\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4YIc3_5mG_kZ"
      },
      "source": [
        "Cada observación se registra con el día, mes y año. Sin embargo, tratar estos componentes como características independientes presenta varios inconvenientes:\n",
        "\n",
        "🌧️ Días del mes:\n",
        "- La representación numérica entera de los días no refleja la condición circular de anterioridad de los días altos de un mes respecto a los bajos del siguiente. Además, los meses tienen diferentes números de días por lo que aun si el modelo logra capturar esa circularidad, puede ser dificultoso entender que la distancia entre un determinado número de día y otro no es siempre la misma (ej. entre el 28 de un mes y el 1 del siguiente puede haber 1 o 4 días).\n",
        "- El valor numérico del día en un mes no informa directamente sobre la probabilidad de lluvia para el día siguiente.\n",
        "\n",
        "🌦️ Meses:\n",
        "- Tienen una moderadamente alta cardinalidad para ser representados mediante one-hot encoding. Además, esta técnica otorga la misma distancia euclideana a cada par de vectores que representan cada uno de los 12 meses.\n",
        "- Si se usan valores enteros del 1 al 12, se pierde la circularidad, complicando la interpretación de la distancia temporal entre los útlimos meses y los primeros.\n",
        "\n",
        "⛈️ Años:\n",
        "- Son informativos para variaciones climáticas anuales, pero pueden introducir problemas al generalizar a datos de años no vistos durante el entrenamiento.\n",
        "\n",
        "Por estas razones, codificamos la fecha como el número de día del año, utilizando coordenadas polares para reflejar su estructura circular. De este modo se traduce la información brindada por la fecha a su ubicación dentro de un año calendario, codificandola en dos nuevas variables."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rl_dWYPAyFue"
      },
      "outputs": [],
      "source": [
        "plot_day_of_year_in_unit_circle()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eOpONoigyKdE"
      },
      "source": [
        "<p><em>\n",
        "Codificar la fecha de esta manera cuenta con la ventaja de que indirectamente estamos incorporando la información de las estaciones del año, ya que para valores positivos de <font color='light-blue'>CosDay</font> y <font color='light-blue'>SinDay</font> nos encontramos con días de verano mientras que para valores negativos de ambos lo hacemos con días de invierno. En la diagonal opuesta sucede algo similar con días de primavera y otoño.\n",
        "</em></p>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AZ51iHPuVNzF"
      },
      "outputs": [],
      "source": [
        "df_date_encoded = encode_cyclical_date(df.copy())\n",
        "df_date_encoded[date_columns + ['DayCos', 'DaySin']]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vaF065J-yQ24"
      },
      "source": [
        "Vamos a entrenar un modelo de Regresión Logística utilizando únicamente las columnas creadas como features para predecir la variable objetivo <code>RainTomorrow</code>. Vamos a comparar los resultados obtenidos mediante esa codificación con la codificación ordinal del día y el mes para examinar el desempeño de la técnica utilizada."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7nku3ml6yQR2"
      },
      "outputs": [],
      "source": [
        "# Initialize lists to store results\n",
        "random_states = [10, 20, 30, 40, 50, 60, 70, 80, 90, 100]\n",
        "polar_results = []\n",
        "integer_results = []\n",
        "\n",
        "# Methodology 1: DayCos, DaySin\n",
        "df_model_cos_sin = df_date_encoded[[\"DayCos\", \"DaySin\", \"RainTomorrow\"]]\n",
        "df_model_cos_sin = df_model_cos_sin.dropna(how='any').reset_index(drop=True)\n",
        "\n",
        "# Methodology 2: Day, Month\n",
        "df_model_day_month = df_date_encoded[[\"Date\", \"RainTomorrow\"]]\n",
        "df_model_day_month[\"Day\"] = df_model_day_month[\"Date\"].dt.day\n",
        "df_model_day_month[\"Month\"] = df_model_day_month[\"Date\"].dt.month\n",
        "df_model_day_month = pd.get_dummies(df_model_day_month, columns=[\"Month\"], drop_first=True, dtype=int)\n",
        "df_model_day_month = df_model_day_month.drop(columns=\"Date\")\n",
        "df_model_day_month = df_model_day_month.dropna(how='any').reset_index(drop=True)\n",
        "\n",
        "for random_state in random_states:\n",
        "    # Methodology 1: DayCos, DaySin\n",
        "    X1 = df_model_cos_sin.drop(columns=\"RainTomorrow\")\n",
        "    y1 = df_model_cos_sin[\"RainTomorrow\"]\n",
        "    X1_train, X1_test, y1_train, y1_test = train_test_split(X1, y1, test_size=0.2, random_state=random_state)\n",
        "\n",
        "    scaler = StandardScaler()\n",
        "    X1_train_scaled = scaler.fit_transform(X1_train)\n",
        "    X1_test_scaled = scaler.transform(X1_test)\n",
        "\n",
        "    model1 = LogisticRegression(class_weight='balanced')\n",
        "    model1.fit(X1_train_scaled, y1_train)\n",
        "    y1_pred = model1.predict(X1_test_scaled)\n",
        "\n",
        "    acc1 = accuracy_score(y1_test, y1_pred)\n",
        "    f1_1 = f1_score(y1_test, y1_pred)\n",
        "\n",
        "    polar_results.append({\n",
        "        'RandomState': random_state,\n",
        "        'PolarAccuracy': round(acc1, 3),\n",
        "        'PolarF1': round(f1_1, 3)\n",
        "    })\n",
        "\n",
        "    # Methodology 2: Day, Month\n",
        "    X2 = df_model_day_month.drop(columns=\"RainTomorrow\")\n",
        "    y2 = df_model_day_month[\"RainTomorrow\"]\n",
        "    X2_train, X2_test, y2_train, y2_test = train_test_split(X2, y2, test_size=0.2, random_state=random_state)\n",
        "\n",
        "    scaler = StandardScaler()\n",
        "    X2_train_scaled = scaler.fit_transform(X2_train)\n",
        "    X2_test_scaled = scaler.transform(X2_test)\n",
        "\n",
        "    model2 = LogisticRegression(class_weight='balanced')\n",
        "    model2.fit(X2_train_scaled, y2_train)\n",
        "    y2_pred = model2.predict(X2_test_scaled)\n",
        "\n",
        "    acc2 = accuracy_score(y2_test, y2_pred)\n",
        "    f1_2 = f1_score(y2_test, y2_pred)\n",
        "\n",
        "    integer_results.append({\n",
        "        'RandomState': random_state,\n",
        "        'IntegerAccuracy': round(acc2, 3),\n",
        "        'IntegerF1': round(f1_2, 3)\n",
        "    })\n",
        "\n",
        "# Create DataFrames from results\n",
        "polar_df = pd.DataFrame(polar_results)\n",
        "integer_df = pd.DataFrame(integer_results)\n",
        "\n",
        "# Merge the DataFrames on 'RandomState'\n",
        "comparison_df = pd.merge(polar_df, integer_df, on='RandomState')\n",
        "\n",
        "# Display the comparison DataFrame\n",
        "comparison_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3rOa1rnQyb0e"
      },
      "source": [
        "<p><em>\n",
        "Observamos que los resultados obtenidos con la codificación de las fechas en coordenadas polares tuvo una performance superior para el F1 score para todos los valores de random state seleccionados. Si bien el accuracy de los modelos codificados con el número para el día y one-hot pára los meses es superior, recordamos que debido al desbalanceo entre clases, el accuracy se encuentra inflado para modelos que predicen mayormente que no llueve. Por otro lado, el F1 score refleja un balance entre la precisión y la sensibilidad del modelo.\n",
        "</em></p>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X14R5jhOhNmq"
      },
      "source": [
        "Finalmente, creamos el paso, lo probamos y lo agregamos al pipeline:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lqga8Z1fVNzG"
      },
      "outputs": [],
      "source": [
        "# Crear el FunctionTransformer\n",
        "cyclical_date_transformer = FunctionTransformer(\n",
        "    func=encode_cyclical_date,\n",
        "    kw_args={'date_column': 'Date'},\n",
        "    validate=False\n",
        ").set_output(transform='pandas')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zl3dEKeRVNzG"
      },
      "outputs": [],
      "source": [
        "# Probamos el paso\n",
        "df = cyclical_date_transformer.fit_transform(df)\n",
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E5JOvVaKVNzG"
      },
      "outputs": [],
      "source": [
        "# Agregamos el paso.\n",
        "data_pipeline.steps.append(('cyclical_date_transformer', cyclical_date_transformer))\n",
        "data_pipeline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LU-TcTRIyg-2"
      },
      "source": [
        "##### 2.2.3.2. Variable \"Location\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EU1vRzkPHOAw"
      },
      "source": [
        "Como evaluamos en el apartado anterior, la distancia entre estaciones meteorológicas es inversamente proporcional al coeficiente Phi para la variable <code>RainToday</code>. En decir, si en una estación se registra lluvia, es probable que en una estación cercana también se registre la misma condición.\n",
        "\n",
        "Vamos a codificar la locación a partir de las coordenadas de __Latitud__ y __Longitud__ obtenidas mediante geolocalización con *Open Street Map* 🌍. De este modo informamos al modelo con la relación espacial entre las estaciones. Además, evitamos la representación dispersa que implicara utilizar one hot encoding con una variable de alta cardinalidad."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cYVMI3aOVNzH"
      },
      "outputs": [],
      "source": [
        "encode_location_transformer = FunctionTransformer(\n",
        "    func=encode_location,\n",
        "    kw_args={'gdf_locations': gdf_locations},\n",
        "    validate=False\n",
        ").set_output(transform='pandas')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "location_pieline = Pipeline([('fix_location', FunctionTransformer(fix_location)), ('encode_location_transformer', encode_location_transformer)])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PTQcI5aSVNzH"
      },
      "outputs": [],
      "source": [
        "# Probamos el paso\n",
        "df = location_pieline.fit_transform(df)\n",
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wwXVLxuLVNzH"
      },
      "outputs": [],
      "source": [
        "# Agregamos el paso al pipeline\n",
        "data_pipeline.steps.append(('location_pieline', location_pieline))\n",
        "data_pipeline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B0-vILzyypVe"
      },
      "source": [
        "##### 2.2.3.3. Variables \"WindDir\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xJCGv4STHRSi"
      },
      "source": [
        "Al igual que sucede con la variable de fecha, las variables relacionadas con la dirección del viento también poseen un orden circular. En el caso de estas últimas variables, incluso, necesitamos un menor grado de abstracción ya que la dirección del viento puede representarse intuitivamente como la dirección de un vector en dos dimensiones $(x, y)$. El eje $x$ representa la dirección __Este-Oeste__ y el eje $y$ representa la dirección __Norte-Sur__."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MQqa76ViVNzI"
      },
      "outputs": [],
      "source": [
        "encode_wind_dir_transformer = FunctionTransformer(encode_wind_dir,\n",
        "                                                  validate=False).set_output(transform='pandas')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qk1pTx2SVNzI"
      },
      "outputs": [],
      "source": [
        "# Probamos el paso\n",
        "df = encode_wind_dir_transformer.fit_transform(df)\n",
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lsWdvo0-VNzI"
      },
      "outputs": [],
      "source": [
        "# Agregamos el paso al pipeline\n",
        "data_pipeline.steps.append(('encode_wind_dir_transformer', encode_wind_dir_transformer))\n",
        "data_pipeline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uf9PCj0DzN2v"
      },
      "source": [
        "Vamos a entrenar un modelo de *Regresión Logística* utilizando las columnas creadas como features para predecir la variable objetivo <code>RainTomorrow</code>, incluyendo las variables de velocidad del viento, ubicación de la estación meteorológica y la presencia o ausencia de lluvia para el día registrado. La incorporación de estás últimas variables se debe a que no se espera que los datos de la dirección del viento por si solo resulte informativa para predecir la lluvia del día siguiente. Vamos a comparar los resultados obtenidos mediante esa codificación con la codificación mediante one-hot encoding de la dirección del viento para examinar el desempeño de la técnica utilizada."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DlhlbpHhy1MG"
      },
      "outputs": [],
      "source": [
        "# Initialize lists to store results\n",
        "random_states = [10, 20, 30, 40, 50, 60, 70, 80, 90, 100]\n",
        "polar_results = []\n",
        "integer_results = []\n",
        "\n",
        "# Methodology 1: DayCos, DaySin\n",
        "df_model_cos_sin = df[\n",
        "    [\n",
        "        \"Lat\",\n",
        "        \"Lon\",\n",
        "        \"WindGustSpeed\",\n",
        "        \"WindSpeed9am\",\n",
        "        \"WindSpeed3pm\",\n",
        "        \"WindGustDirCos\",\n",
        "        \"WindGustDirSin\",\n",
        "        \"WindDir9amCos\",\n",
        "        \"WindDir9amSin\",\n",
        "        \"WindDir3pmCos\",\n",
        "        \"WindDir3pmSin\",\n",
        "        \"RainToday\",\n",
        "        \"RainTomorrow\",\n",
        "    ]\n",
        "]\n",
        "df_model_cos_sin = df_model_cos_sin.dropna(how=\"any\").reset_index(drop=True)\n",
        "\n",
        "# Methodology 2: Day, Month\n",
        "df_model_one_hot = df[\n",
        "    [\n",
        "        \"Lat\",\n",
        "        \"Lon\",\n",
        "        \"WindGustSpeed\",\n",
        "        \"WindSpeed9am\",\n",
        "        \"WindSpeed3pm\",\n",
        "        \"WindGustDir\",\n",
        "        \"WindDir9am\",\n",
        "        \"WindDir3pm\",\n",
        "        \"RainToday\",\n",
        "        \"RainTomorrow\",\n",
        "    ]\n",
        "]\n",
        "df_model_one_hot = df_model_one_hot.dropna(how=\"any\").reset_index(drop=True)\n",
        "\n",
        "df_model_one_hot = pd.get_dummies(\n",
        "    df_model_one_hot,\n",
        "    columns=[\"WindGustDir\", \"WindDir9am\", \"WindDir3pm\"],\n",
        "    drop_first=True,\n",
        "    dtype=int,\n",
        ")\n",
        "\n",
        "for random_state in random_states:\n",
        "    # Methodology 1: DayCos, DaySin\n",
        "    X1 = df_model_cos_sin.drop(columns=\"RainTomorrow\")\n",
        "    y1 = df_model_cos_sin[\"RainTomorrow\"]\n",
        "    X1_train, X1_test, y1_train, y1_test = train_test_split(\n",
        "        X1, y1, test_size=0.2, random_state=random_state\n",
        "    )\n",
        "\n",
        "    scaler = StandardScaler()\n",
        "    X1_train_scaled = scaler.fit_transform(X1_train)\n",
        "    X1_test_scaled = scaler.transform(X1_test)\n",
        "\n",
        "    model1 = LogisticRegression(class_weight=\"balanced\")\n",
        "    model1.fit(X1_train_scaled, y1_train)\n",
        "    y1_pred = model1.predict(X1_test_scaled)\n",
        "\n",
        "    acc1 = accuracy_score(y1_test, y1_pred)\n",
        "    f1_1 = f1_score(y1_test, y1_pred)\n",
        "\n",
        "    polar_results.append(\n",
        "        {\n",
        "            \"RandomState\": random_state,\n",
        "            \"PolarAccuracy\": round(acc1, 3),\n",
        "            \"PolarF1\": round(f1_1, 3),\n",
        "        }\n",
        "    )\n",
        "\n",
        "    # Methodology 2: Day, Month\n",
        "    X2 = df_model_one_hot.drop(columns=\"RainTomorrow\")\n",
        "    y2 = df_model_one_hot[\"RainTomorrow\"]\n",
        "    X2_train, X2_test, y2_train, y2_test = train_test_split(\n",
        "        X2, y2, test_size=0.2, random_state=random_state\n",
        "    )\n",
        "\n",
        "    scaler = StandardScaler()\n",
        "    X2_train_scaled = scaler.fit_transform(X2_train)\n",
        "    X2_test_scaled = scaler.transform(X2_test)\n",
        "\n",
        "    model2 = LogisticRegression(class_weight=\"balanced\")\n",
        "    model2.fit(X2_train_scaled, y2_train)\n",
        "    y2_pred = model2.predict(X2_test_scaled)\n",
        "\n",
        "    acc2 = accuracy_score(y2_test, y2_pred)\n",
        "    f1_2 = f1_score(y2_test, y2_pred)\n",
        "\n",
        "    integer_results.append(\n",
        "        {\n",
        "            \"RandomState\": random_state,\n",
        "            \"IntegerAccuracy\": round(acc2, 3),\n",
        "            \"IntegerF1\": round(f1_2, 3),\n",
        "        }\n",
        "    )\n",
        "\n",
        "# Create DataFrames from results\n",
        "polar_df = pd.DataFrame(polar_results)\n",
        "integer_df = pd.DataFrame(integer_results)\n",
        "\n",
        "# Merge the DataFrames on 'RandomState'\n",
        "comparison_df = pd.merge(polar_df, integer_df, on=\"RandomState\")\n",
        "\n",
        "# Display the comparison DataFrame\n",
        "comparison_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GUKoMnBkzWzQ"
      },
      "source": [
        "<p><em>\n",
        "La comparación realizada no arroja evidencia contundente de la superioridad de alguno de los métodos sobre el otro para la tarea que estamos realizando.\n",
        "\n",
        "Tanto la codificación de coordenadas polares como la codificación one-hot ofrecen distintas ventajas y potenciales riesgos como representación de la dirección del viento.\n",
        "\n",
        "La codificación de coordenadas polares, con su representación compacta de solo dos características (<font color='light-blue'>CosAngle</font> y <font color='light-blue'>SinAngle</font>), es computacionalmente eficiente y captura la naturaleza circular de la dirección del viento, lo que la hace adecuada para modelos que manejan datos continuos de manera efectiva. Además puede constribuir a la interpretabilidad si se entiende la relación entre los valores que asumen estas variables y los puntos cardinales. Se intuye que este tipo de codificación puede resultar beneficiosa para modelos como *SVM* que son computacionalmente muy costosos en altas dimensiones, y modelos como *KNN* que se ven favorecidos por una codificación que respeta la distancia espacial entre categorías. Por otro lado, los árboles de decisión trabajan sobre separaciones ortogonales, esto puede complejizar la tarea de aislar los observaciones correspondientes a una sóla de las categorías posibles. Sin embargo, no se descarta que resulte beneficioso la posibilidad de separar direcciones del viento similares entre si de forma eficiente.\n",
        "\n",
        "En cambio, la codificación one-hot garantiza la independencia de las variables creadas, lo que es especialmente beneficioso para modelos lineales como la *regresión logística*. Aumentando el espacio de características en 15 características binarias por cada una de las variables originales, preserva la granularidad de los datos categóricos sin asumir ningún orden inherente. Las nuevas variables creadas generan un dataset de entrenamiento altamente disperso lo que puede resultar problemático para modelos de redes neuronales. La representación categórica de los datos, incluso sin la necesidad de realizar one hot encoding, puede resultar beneficioso para modelos generados a partir de ensambles de árboles.\n",
        "\n",
        "Dadas las ligeras diferencias en las puntuaciones F1 observadas en nuestros experimentos y las diferentes fortalezas de cada método, no encontramos evidencia definitiva para elegir uno sobre el otro. Por lo tanto, continuaremos probando nuestros modelos utilizando ambas representaciones, seleccionando el método más apropiado en función de casos de uso específicos.\n",
        "</em></p>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wvOwr5y96ygF"
      },
      "source": [
        "#### 2.2.4. Evaluar importancia de las variables"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L16WFBHpzohP"
      },
      "source": [
        "Vamos a realizar un análisis de componentes principales para realizar una inspección visual e intuir la potencialidad de las variables elegidas para separar entre clases de la variable objetivo."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zXp3HoMgzpLg"
      },
      "outputs": [],
      "source": [
        "features_list = [\"DayCos\", \"DaySin\", \"Lat\", \"Lon\", \"MinTemp\", \"MaxTemp\", \"Rainfall\", \"Evaporation\", \"Sunshine\", \"WindGustDirCos\",\n",
        "                 \"WindGustDirSin\", \"WindGustSpeed\", \"WindDir9amCos\", \"WindDir9amSin\", \"WindSpeed9am\", \"WindDir3pmCos\", \"WindDir3pmSin\",\n",
        "                 \"WindSpeed3pm\", \"Humidity9am\", \"Humidity3pm\", \"Pressure9am\", \"Pressure3pm\", \"Cloud9am\", \"Cloud3pm\", \"Temp9am\",\n",
        "                 \"Temp3pm\", \"RainToday\" ]\n",
        "\n",
        "n_max_feat = len(max(features_list, key=len))\n",
        "\n",
        "df_model = df[features_list + [\"RainTomorrow\"]].copy()\n",
        "df_model = df_model.dropna(how='any').reset_index(drop=True)\n",
        "\n",
        "X = df_model[features_list]\n",
        "y = df_model[\"RainTomorrow\"]\n",
        "\n",
        "# Plot training data in two dimensions to visualize if captured features relate to labels.\n",
        "scaler_manual = StandardScaler()\n",
        "X_scaled = scaler_manual.fit_transform(X)\n",
        "\n",
        "# Apply PCA for dimensionality reduction\n",
        "pca = PCA(random_state=random_state)\n",
        "X_pca = pca.fit_transform(X_scaled)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X1Zi68cszr0h"
      },
      "outputs": [],
      "source": [
        "# Plot subsample\n",
        "random.seed(random_state)\n",
        "subsample_size = 5000\n",
        "subsample_indices = random.sample(range(len(y)), subsample_size)\n",
        "\n",
        "X_subsample = X_pca[subsample_indices]\n",
        "y_subsample = y[subsample_indices]\n",
        "\n",
        "plt.figure(figsize=(5, 5))\n",
        "\n",
        "plt.scatter(\n",
        "    X_subsample[y_subsample == 0, 0],\n",
        "    X_subsample[y_subsample == 0, 1],\n",
        "    c=spectral_palette[9],\n",
        "    label=\"Negativo para RainTomorrow\",\n",
        "    s=5,\n",
        "    alpha=0.4,\n",
        ")\n",
        "\n",
        "plt.scatter(\n",
        "    X_subsample[y_subsample == 1, 0],\n",
        "    X_subsample[y_subsample == 1, 1],\n",
        "    c=spectral_palette[1],\n",
        "    label=\"Positivo para RainTomorrow\",\n",
        "    s=5,\n",
        "    alpha=0.4\n",
        ")\n",
        "\n",
        "x_min, x_max = -10, 10\n",
        "y_min, y_max = -10, 10\n",
        "\n",
        "plt.xticks(fontsize=8)\n",
        "plt.yticks(fontsize=8)\n",
        "\n",
        "plt.xlim(x_min, x_max)\n",
        "plt.ylim(y_min, y_max)\n",
        "\n",
        "plt.xlabel('Componente principal 1')\n",
        "plt.ylabel('Componente principal 2')\n",
        "plt.title('PCA de las features para nuestros modelos', fontsize=10)\n",
        "\n",
        "# Set alpha=1 for the legend only\n",
        "legend = plt.legend(prop={'size': 8})\n",
        "for lh in legend.legend_handles:\n",
        "    lh.set_alpha(1)\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_YZ3RSR-0HXh"
      },
      "source": [
        "<p><em>\n",
        "Podemos observar que las dos componentes principales no son suficientes para esbozar una separación entre las clases de la variable objetivo. Sin embargo, podemos ver que efectivamente ambas clases se agrupan en respectivos clusters aunque estos se encuentren solapados. El reconocimiento de estos clusters sugiere que las variables elegidas poseen cierta habilidad para distinguir entre clases.\n",
        "</em></p>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kZuXxL7k0KBA"
      },
      "source": [
        "Vamos a analizar ahora la composición del componente principal 1 obtenido."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pTLCIiSS0LxF"
      },
      "outputs": [],
      "source": [
        "# Get the loadings for the first principal component\n",
        "loadings = pca.components_[0]\n",
        "pc1_loadings = pd.DataFrame(loadings, index=X.columns, columns=['PC1 Loading'])\n",
        "\n",
        "# Get the explained variance ratio to understand how much variance each component explains\n",
        "explained_variance = pca.explained_variance_ratio_\n",
        "\n",
        "# Print loadings for the first principal component\n",
        "print(\"Explained Variance by PC1:\", explained_variance[0], \"\\n\")\n",
        "print(pc1_loadings.sort_values(by='PC1 Loading', ascending=False).head(10))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kNeZtvud0P_F"
      },
      "source": [
        "<p><em>\n",
        "Analizar la composición de los componentes principales puede resultar útil para entender el aporte de cada una de las variables de entrada a la varianza del dataset. Sin embargo, en este caso, el componente principal 1 explica sólo el 24% de la varianza total por lo que el análisis resulta insuficiente para entender el aporte de las features a la estructura del dataset.\n",
        "</em></p>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gq1zPv900SkX"
      },
      "source": [
        "Para realizar una evaluación más precisa de la importancia de las variables del dataset en la tarea de predecir lluvia para el día siguiente, entrenamos un modelo de random forest y extraemos del mismo la contribución de cada una de las variables a la reducción de la impureza *Gini*. Para el entrenamiento del modelo no se dividirán los datos en entrenamiento y testeo, ya que no nos interesa utilizarlo para realizar inferencias."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gG6o9fEr0hTH"
      },
      "outputs": [],
      "source": [
        "# Check importance of each feature.\n",
        "rf_importance = RandomForestClassifier(random_state=42)\n",
        "rf_importance.fit(X, y)\n",
        "\n",
        "feature_importances = rf_importance.feature_importances_\n",
        "rf_feature_importances = pd.DataFrame(feature_importances, index=X.columns, columns=['Feature Importance'])\n",
        "\n",
        "print(rf_feature_importances.sort_values(by='Feature Importance', ascending=False))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pBMmWyJb6ygF"
      },
      "source": [
        "<p><em>\n",
        "Todas las variables oscilan en un rango de importancia comprendida entre 0.16 y 0.015, teniendo la mayoría de ellas valores cercanos al umbral mínimo mencionado.\n",
        "\n",
        "El análisis nos permite distinguir dos cuestiones de importancia:\n",
        "- En primer lugar que no encontramos variables con una importancia ínfima que sugiera su descarte, es decir, en mayor o menor medida todas las features resultan informativas para la tarea que llevamos a cabo.\n",
        "- En segundo lugar, la medida de Humidity3pm y de Sunshine se destacan por su importancia relevada respecto al resto de las variables. Esta situación verifica nuestra hipótesis de la importancia de ambas variables cuando analizamos el pairplot entre todas las variables numéricas.\n",
        "</em></p>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6k-E1-EyVNzL"
      },
      "source": [
        "#### 2.2.5. Eliminar columnas"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-xxBXfv1VNzL"
      },
      "source": [
        "Finalmente agregamos un paso en donde se eliminan las columnas que fueron codificadas:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mY2Qqw7fVNzL"
      },
      "outputs": [],
      "source": [
        "columnas_codificadas = [\"WindGustDir\",\n",
        "                        \"WindDir9am\", \"WindDir3pm\", \"Date\", \"Location\"]\n",
        "eliminar_columnas_transformer = FunctionTransformer(\n",
        "    eliminar_columnas, kw_args={'columnas_a_eliminar': columnas_codificadas}).set_output(transform='pandas')\n",
        "\n",
        "# Probamos el paso\n",
        "df = eliminar_columnas_transformer.fit_transform(df)\n",
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "clW4QqKmVNzL"
      },
      "outputs": [],
      "source": [
        "# Agregamos el paso al pipeline\n",
        "data_pipeline.steps.append(('eliminar_columnas_transformer', eliminar_columnas_transformer))\n",
        "data_pipeline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "garQoe1YSQhV"
      },
      "source": [
        "#### 2.2.6. Exportación de datos y pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5aYmnOvbgKjq"
      },
      "outputs": [],
      "source": [
        "# Mostramos como quedó el conjunto pre-procesado\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "48PENQ1MSkQY"
      },
      "outputs": [],
      "source": [
        "# Exportamos el dataset\n",
        "df.to_csv(\"./data/weather_processed.csv\", index=False)\n",
        "df_pre_processed.to_csv('./data/weather_pre_processed.csv', index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JlRmO5vtVNzM"
      },
      "outputs": [],
      "source": [
        "# Exportar el pipeline a un archivo\n",
        "with open('./data/data_pipeline.pkl', 'wb') as file:\n",
        "    pickle.dump(data_pipeline, file)\n",
        "with open('./data/target_pipeline.pkl', 'wb') as file:\n",
        "    pickle.dump(target_pipeline, file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Exportamos train y test... Por problema de Colab.\n",
        "# Separamos la variable objetivo del resto de las variables\n",
        "X = df_pre_processed.drop(columns=\"RainTomorrow\")\n",
        "y = df_pre_processed[\"RainTomorrow\"]\n",
        "\n",
        "# Separamos en entrenamiento y testing.\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=random_state)\n",
        "\n",
        "# Ejecutamos los pipelies de datos\n",
        "X_train = data_pipeline.fit_transform(X_train)\n",
        "X_test = data_pipeline.transform(X_test)\n",
        "y_train = target_pipeline.fit_transform(y_train)\n",
        "y_test = target_pipeline.transform(y_test)\n",
        "\n",
        "X_train.to_csv('./data/X_train.csv', index=False)\n",
        "X_test.to_csv('./data/X_test.csv', index=False)\n",
        "y_train.to_csv('./data/y_train.csv', index=False)\n",
        "y_test.to_csv('./data/y_test.csv', index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p2kkhkB16ygG"
      },
      "source": [
        "### 2.3. Ingeniería de modelos de aprendizaje automático (ML Model Engineering)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "97fgoVVUL6Gv"
      },
      "outputs": [],
      "source": [
        "# Importamos el conjunto pre-procesado\n",
        "file_name = './data/weather_pre_processed.csv'\n",
        "try:\n",
        "    df = pd.read_csv(file_name)\n",
        "    print('Dataset pre-procesado local')\n",
        "except:\n",
        "  raise Exception('Error al encontrar el archivo')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Configuraciones\n",
        "random_state = 42"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kSkzk_nh0czP"
      },
      "source": [
        "En esta etapa se prueban los modelos que son adecuados para este tipo de problemas (clasificación). Entre ellos se encuentran:\n",
        "\n",
        "- Regresión Logística\n",
        "- Naive-bayes\n",
        "- K-NN\n",
        "- Máquinas de Vectores de Soporte (SVM)\n",
        "- Árboles de Decisión\n",
        "- Bosques Aleatorios (Random Forests)\n",
        "- Naive Bayes\n",
        "- Gradient Boosting (GBM, XGBoost, LightGBM, CatBoost)\n",
        "- Discriminante Lineal y Cuadrático\n",
        "\n",
        "Además, existen variantes y combinaciones de estos modelos como:\n",
        "- Redes Neuronales Recurrentes para datos secuenciales\n",
        "- Ensambles de modelos (Stacking, Bagging, Boosting)\n",
        "\n",
        "Así como ajustar los hiperparámetros y técnicas de regularización."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1MPvwIOikg1g"
      },
      "source": [
        "#### 2.3.1. Modelado"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eDXr_AYPVNzP"
      },
      "outputs": [],
      "source": [
        "# Obtenemos los datos ya pre-procesados\n",
        "X_train = pd.read_csv('./data/X_train.csv')\n",
        "X_test = pd.read_csv('./data/X_test.csv')\n",
        "y_train = pd.read_csv('./data/y_train.csv').values.ravel()\n",
        "y_test = pd.read_csv('./data/y_test.csv').values.ravel()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OOaSVUxQrbqt"
      },
      "outputs": [],
      "source": [
        "df_algoritms = pd.DataFrame(columns=[\"Algorithm\", \"Sensibility\", \"Specificity\", \"Accuracy\", \"Balanced-Accuracy\", \"Precision\", \"F1 Score\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EMOYy0aMkn3R"
      },
      "source": [
        "##### 2.3.1.1 Estudio de la literatura y modelo base"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "faM52OHSk12f"
      },
      "source": [
        "A modo de baseline inicial para nuestro trabajo realizamos un modelo simple que predice lluvia para el día siguiente si el día de la observación también llovió, y evaluamos los resultados obtenidos."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "huQ65yYzlt2K"
      },
      "outputs": [],
      "source": [
        "# Construir un modelo que se fije en la columna \"RainToday\", si es verdadero (o sea, llovió hoy) predice que también lloverá mañana.\n",
        "y_pred = X_test[\"RainToday\"].astype(int)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Cv0LBGxTk4NK"
      },
      "outputs": [],
      "source": [
        "row = evaluate_predictions(y_test, y_pred, algorithm=\"Baseline\")\n",
        "df_algoritms.loc[len(df_algoritms)] = row"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WjlWp_w2qeR_"
      },
      "outputs": [],
      "source": [
        "pd.DataFrame([row])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fCH0G9eRnBFd"
      },
      "outputs": [],
      "source": [
        "conf_matrix = pd.crosstab(df[\"RainToday\"], df[\"RainTomorrow\"])\n",
        "phi, p = phi_coefficient(conf_matrix)\n",
        "\n",
        "print(f\"Phi Coefficient: {phi}\")\n",
        "print(f\"p-value: {p}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eRytMx5kn7Zd"
      },
      "source": [
        "<p><em>\n",
        "Observamos que el valor de accuracy se encuentra inflado por el desbalanceo en el dataset. De hecho, si predijeramos siempre ausencia de lluvia obtendríamos un valor de 0.78 en esa métrica. Claro que en ese caso obtendríamos un valor de 0 para precision y recall.\n",
        "\n",
        "Con este modelo inicial planteado obtenemos un F1 score de 0.47, con valores para precision y recall muy similares.\n",
        "\n",
        "Calculando el coeficiente Phi entre ambas variables binarias encontramos un valor de 0.3 (asociación entre variables moderada y positiva) con una significancia que nos permite rechazar la hipóteses de que la asociación encontrada puede deberse simplemente a la variabilidad de el muestreo aleatorio.</em></p>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X4LmoTZ45W7F"
      },
      "source": [
        "##### 2.3.1.2. Métricas de calidad"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_rS0tE7D5erF"
      },
      "source": [
        "Entre las métricas de calidad a utilizar, se encuentran:\n",
        "- Sensibilidad (**TPR**) ⇒ $ TPR = \\frac{TP}{P} = \\frac{TP}{TP + FN} = 1 - FNR $\n",
        "- Especificidad (**TNR**) ⇒ $ TNR = \\frac{TN}{N} = \\frac{TN}{TN + FP} = 1 - FPR $\n",
        "- Exactitud (**ACC**) ⇒ $ ACC = \\frac{TP + TN}{P + N} $\n",
        "- Exactitud balanceada (**BA**) ⇒ $ BA = \\frac{TPR + TNR}{2} $\n",
        "- Precisión ⇒ $ Precision = \\frac{TP}{TP + FP} $\n",
        "- F1-score o Fβ-score ⇒ $ F_β = (1 + β^2) * \\frac{precision ⋅ recall}{(β^2 ⋅ presicion) + recall}$\n",
        "\n",
        "\n",
        "<!-- <font color='red'>TODO: </font>\n",
        "<font color='red'>Analizar otras métricas también, como:\n",
        "- Robustness\n",
        "- Explainability\n",
        "- Scalability\n",
        "- Resource demand\n",
        "- Model compexity</font> -->\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CTZmlsOC1uE5"
      },
      "source": [
        "##### 2.3.1.3. Selección de modelos"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aPLZM2LL1zd4"
      },
      "source": [
        "Dentro de los modelos de aprendizaje automático, se eligen los siguientes modelos para este caso de estudio:\n",
        "\n",
        "⟶ [Regresión logística](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html)\n",
        "\n",
        "⟶ [Naive-bayes](https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.MultinomialNB.html#sklearn.naive_bayes.MultinomialNB)\n",
        "\n",
        "⟶ [K-NN](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html)\n",
        "\n",
        "⟶ [Decision Tree Classifier](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html)\n",
        "\n",
        "⟶ [Random Forest Classifier](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html)\n",
        "\n",
        "⟶ [Support Vector Machines](https://scikit-learn.org/stable/modules/svm.html)\n",
        "\n",
        "⟶ [Gradient Boosting Classifier](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingClassifier.html)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jU7z6DKE2F56"
      },
      "source": [
        "##### 2.3.1.4. Entrenamiento de modelos"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2yytB1HD3Dtq"
      },
      "source": [
        "###### Regresión logística"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4jJO_6u9PfFS"
      },
      "source": [
        "<strong>Asunciones de la regresión logística:</strong>\n",
        "- Variable de salida binaria (o multinominal)\n",
        "- Independencia en variables de entrada\n",
        "- Baja multicolinearidad entre variables de entrada\n",
        "- Grandes tamaños de muestra"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X9BEwcpwfwuc"
      },
      "source": [
        "<strong>Entrenamiento:</strong>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jFcmHqfIYdV7"
      },
      "outputs": [],
      "source": [
        "# Estadarizar los datos\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Le agrego las columnas\n",
        "X_train_scaled = pd.DataFrame(X_train_scaled, columns=X_train.columns)\n",
        "X_test_scaled = pd.DataFrame(X_test_scaled, columns=X_test.columns)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k2uMeU05ZBKH"
      },
      "outputs": [],
      "source": [
        "X_train_scaled.describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FAWFAyhdZUUt"
      },
      "outputs": [],
      "source": [
        "# Entrenamos el modelo\n",
        "model = LogisticRegression(class_weight=\"balanced\", random_state=random_state)\n",
        "model.fit(X_train_scaled, y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RG5f4nfPa2Gy"
      },
      "outputs": [],
      "source": [
        "# Realizamos la matriz de confusión\n",
        "y_pred = model.predict(X_test_scaled)\n",
        "row = evaluate_predictions(y_test, y_pred, algorithm=\"Logistic Regression\")\n",
        "df_algoritms.loc[len(df_algoritms)] = row"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plot_roc(y_test, model.predict_proba(X_test_scaled)[:, -1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RXlrWQQUtEUj"
      },
      "outputs": [],
      "source": [
        "pd.DataFrame([row])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GJGI851Sy3Sw"
      },
      "source": [
        "###### Naive-bayes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6kz6hIh4y3Sy"
      },
      "source": [
        "<strong>Asunciones de naive bayes:</strong>\n",
        "- Independencia entre los atributos, dado el valor de la variable de clase\n",
        "- Distribución Gaussiana (para el Naive-bayes Gaussiano de Scikit Learn)\n",
        "- Para Naive Bayes Multinominal, se asume que las características son conteos o frecuencias.\n",
        "- Variables Bernouli (para Naive-Bayes Bernoulli)\n",
        "- Independencia de la frecuencia de las características (Naive-Bayes Multinominal)\n",
        "- Baja correlación entre las variables de entrada\n",
        "- Grandes conjuntos"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CcO2oq_Sy3S0"
      },
      "source": [
        "<strong>Entrenamiento:</strong>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ig_BF5Sgy3S1"
      },
      "outputs": [],
      "source": [
        "# Estadarizar los datos\n",
        "scaler = MinMaxScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Le agrego las columnas\n",
        "X_train_scaled = pd.DataFrame(X_train_scaled, columns=X_train.columns)\n",
        "X_test_scaled = pd.DataFrame(X_test_scaled, columns=X_test.columns)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mmwtzDy9y3S2"
      },
      "outputs": [],
      "source": [
        "X_train_scaled.describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qHOof_HLy3S3"
      },
      "outputs": [],
      "source": [
        "# Entrenamos el modelo\n",
        "model = MultinomialNB()\n",
        "model.fit(X_train_scaled, y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iuAcj9nay3S4"
      },
      "outputs": [],
      "source": [
        "# Realizamos la matriz de confusión\n",
        "y_pred = model.predict(X_test_scaled)\n",
        "row = evaluate_predictions(y_test, y_pred, algorithm=\"Naive Bayes\")\n",
        "df_algoritms.loc[len(df_algoritms)] = row"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plot_roc(y_test, model.predict_proba(X_test_scaled)[:, -1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u2yjYcugy3S4"
      },
      "outputs": [],
      "source": [
        "pd.DataFrame([row])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5NaKMwY7faGn"
      },
      "source": [
        "###### K-NN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rXpBRgZBeDRL"
      },
      "source": [
        "<strong>Asunciones de K-NN:</strong>\n",
        "- Similaridad local\n",
        "- Escala y métrica de distancia\n",
        "- Distribución uniforme\n",
        "- Cantidad adecuada de vecinos\n",
        "- Densidad de datos"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g4HFupW6ePM1"
      },
      "source": [
        "<strong>Entrenamiento:</strong>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SsfY-h0ieKHN"
      },
      "outputs": [],
      "source": [
        "model = KNeighborsClassifier(n_neighbors=23, metric=\"minkowski\", p=2)\n",
        "model.fit(X_train, y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O43tgN09e58a"
      },
      "outputs": [],
      "source": [
        "# Realizamos la matriz de confusión\n",
        "y_pred = model.predict(X_test)\n",
        "row = evaluate_predictions(y_test, y_pred, algorithm=\"K-NN\")\n",
        "df_algoritms.loc[len(df_algoritms)] = row"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "pd.DataFrame([row])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "###### Decision Tree"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<strong>Asunciones de Descision Tree:</strong>\n",
        "- Divisibilidad del estpacio de features (no lineales)\n",
        "- Homogeniedad"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<strong>Entrenamiento:</strong>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model = DecisionTreeClassifier(criterion='entropy', splitter='best',\n",
        "                               max_depth=None, min_samples_split=2, min_samples_leaf=1,\n",
        "                               random_state=42)\n",
        "model.fit(X_train, y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Realizamos la matriz de confusión\n",
        "y_pred = model.predict(X_test)\n",
        "row = evaluate_predictions(y_test, y_pred, algorithm=\"DecisionTreeClassifier\")\n",
        "df_algoritms.loc[len(df_algoritms)] = row"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plot_roc(y_test, model.predict_proba(X_test)[:, -1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "pd.DataFrame([row])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "###### Random Forest"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<strong>Asunciones de Random Forest:</strong>\n",
        "- Independencia entre arblos de decisión\n",
        "- Reducción del sobreajuste.\n",
        "- Robusto"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<strong>Entrenamiento:</strong>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model = RandomForestClassifier(n_estimators=25, random_state=random_state)\n",
        "model.fit(X_train, y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Realizamos la matriz de confusión\n",
        "y_pred = model.predict(X_test)\n",
        "row = evaluate_predictions(y_test, y_pred, algorithm=\"RandomForestClassifier\")\n",
        "df_algoritms.loc[len(df_algoritms)] = row"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plot_roc(y_test, model.predict_proba(X_test)[:, -1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "pd.DataFrame([row])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "###### SVM (linear)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<strong>Asunciones de Suport Vector Machines</strong>\n",
        "- Los datos están escalados\n",
        "- Puede sobreajustar cuando el número de características es mayor al de las observaciones\n",
        "- Dependiendo de la linearidad de los datos es el kernel a utilizar (lineal, etc.)\n",
        "- No funciona bien con clases desbalanceadas\n",
        "- Sensible a datos ruidosos y outliers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<strong>Entrenamiento:</strong>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Estadarizar los datos\n",
        "scaler = MinMaxScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Le agrego las columnas\n",
        "X_train_scaled = pd.DataFrame(X_train_scaled, columns=X_train.columns)\n",
        "X_test_scaled = pd.DataFrame(X_test_scaled, columns=X_test.columns)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "X_train_scaled.describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model = SVC(kernel='linear', random_state=random_state)\n",
        "model.fit(X_train_scaled, y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Realizamos la matriz de confusión\n",
        "y_pred = model.predict(X_test_scaled)\n",
        "row = evaluate_predictions(y_test, y_pred, algorithm=\"SVM-linear\")\n",
        "df_algoritms.loc[len(df_algoritms)] = row"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "pd.DataFrame([row])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "###### SVM (rbf)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<strong>Entrenamiento:</strong>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Estadarizar los datos\n",
        "scaler = MinMaxScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Le agrego las columnas\n",
        "X_train_scaled = pd.DataFrame(X_train_scaled, columns=X_train.columns)\n",
        "X_test_scaled = pd.DataFrame(X_test_scaled, columns=X_test.columns)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "X_train_scaled.describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model = SVC(kernel='rbf', random_state=random_state)\n",
        "model.fit(X_train_scaled, y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Realizamos la matriz de confusión\n",
        "y_pred = model.predict(X_test_scaled)\n",
        "row = evaluate_predictions(y_test, y_pred, algorithm=\"SVM-rbf\")\n",
        "df_algoritms.loc[len(df_algoritms)] = row"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "pd.DataFrame([row])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "###### XGBoost"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<strong>Asunciones de XGBoost:</strong>\n",
        "- No linearidad (puede capturar relaciones complejas y no lineales)\n",
        "- Independencia entre observaciones (multicolinearidad extrema)\n",
        "- Datos sin ruido exesivo\n",
        "- Gran cantidad de datos\n",
        "- Caracteristicias relevantes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<strong>Entrenamiento:</strong>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Estadarizar los datos\n",
        "scaler = MinMaxScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Le agrego las columnas\n",
        "X_train_scaled = pd.DataFrame(X_train_scaled, columns=X_train.columns)\n",
        "X_test_scaled = pd.DataFrame(X_test_scaled, columns=X_test.columns)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "X_train_scaled.describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model = GradientBoostingClassifier(n_estimators=100, learning_rate=0.1, max_depth=3, random_state=random_state)\n",
        "model.fit(X_train_scaled, y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Realizamos la matriz de confusión\n",
        "y_pred = model.predict(X_test_scaled)\n",
        "row = evaluate_predictions(y_test, y_pred, algorithm=\"XGBoost\")\n",
        "df_algoritms.loc[len(df_algoritms)] = row"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "pd.DataFrame([row])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yea-kJDv6ygH"
      },
      "source": [
        "### 2.4 Evaluación de modelos de aprendizaje automático (ML Model Evaluation)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Luego de entrenado los modelos seleccionados, obtenemos las siguientes métricas para dichos modelos:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_algoritms.sort_values(by='Accuracy', ascending=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Finalmente, guardamos la ejecución de los modelos sin optimizar:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
        "file_name = f'df_algorithms_{timestamp}.csv'\n",
        "\n",
        "os.makedirs('results', exist_ok=True)\n",
        "\n",
        "filepath = os.path.join('results', file_name)\n",
        "df_algoritms.to_csv(filepath, index=False)\n",
        "\n",
        "print(f'DataFrame guardado en {filepath}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### 2.4.1 Optimización de los modelos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_algoritms_optimized = pd.DataFrame(columns=[\"Algorithm\", \"Sensibility\", \"Specificity\", \"Accuracy\", \"Balanced-Accuracy\", \"Precision\", \"F1 Score\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Dados estos mejores modelos en métricas, se los selecciona para la optimización:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### 2.4.1.1 Gradient Boosting Classifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "scaler = MinMaxScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "model = GradientBoostingClassifier(random_state=random_state)\n",
        "\n",
        "param_grid = {\n",
        "    'n_estimators': [50, 100, 200],\n",
        "    'learning_rate': [0.01, 0.1, 0.2],\n",
        "    'max_depth': [3, 4, 5],\n",
        "    'min_samples_split': [2, 5, 10],\n",
        "    'min_samples_leaf': [1, 2, 4],\n",
        "    'subsample': [0.8, 1.0]\n",
        "}\n",
        "\n",
        "grid_search = GridSearchCV(estimator=model, param_grid=param_grid,\n",
        "                           cv=5, n_jobs=-1, verbose=2, scoring='accuracy')\n",
        "\n",
        "grid_search.fit(X_train_scaled, y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"Mejores parámetros:\", grid_search.best_params_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Evaluación\n",
        "model = grid_search.best_estimator_\n",
        "y_pred = model.predict(X_test_scaled)\n",
        "row = evaluate_predictions(y_test, y_pred, algorithm=\"XGBoost-Optimizado\")\n",
        "df_algoritms_optimized.loc[len(df_algoritms_optimized)] = row"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "pd.DataFrame([row])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### 2.4.1.2 Random Forest Classifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model = RandomForestClassifier(random_state=random_state)\n",
        "\n",
        "param_grid = {\n",
        "    'n_estimators': [100, 200, 300],\n",
        "    'max_features': ['auto', 'sqrt', 'log2'],\n",
        "    'max_depth': [10, 20, 30, None],\n",
        "    'min_samples_split': [2, 5, 10],\n",
        "    'min_samples_leaf': [1, 2, 4],\n",
        "    'bootstrap': [True, False]\n",
        "}\n",
        "\n",
        "grid_search = GridSearchCV(estimator=model, param_grid=param_grid,\n",
        "                           cv=5, n_jobs=-1, verbose=2, scoring='accuracy')\n",
        "\n",
        "grid_search.fit(X_train, y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"Mejores parámetros:\", grid_search.best_params_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Evaluación\n",
        "model = grid_search.best_estimator_\n",
        "y_pred = model.predict(X_test)\n",
        "row = evaluate_predictions(y_test, y_pred, algorithm=\"RandomForest-Optimizado\")\n",
        "df_algoritms_optimized.loc[len(df_algoritms_optimized)] = row"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "pd.DataFrame([row])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### 2.4.1.2 SVM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "model = SVC(random_state=random_state)\n",
        "\n",
        "param_grid = {\n",
        "    'C': [0.1, 1, 10, 100],\n",
        "    'gamma': [1, 0.1, 0.01, 0.001],\n",
        "    'kernel': ['linear', 'rbf']\n",
        "}\n",
        "\n",
        "grid_search = GridSearchCV(estimator=model, param_grid=param_grid,\n",
        "                           cv=5, n_jobs=-1, verbose=2, scoring='accuracy')\n",
        "\n",
        "grid_search.fit(X_train_scaled, y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"Mejores parámetros:\", grid_search.best_params_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Evaluación\n",
        "model = grid_search.best_estimator_\n",
        "y_pred = model.predict(X_test)\n",
        "row = evaluate_predictions(y_test, y_pred, algorithm=\"SVM-Optimizado\")\n",
        "df_algoritms_optimized.loc[len(df_algoritms_optimized)] = row"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "pd.DataFrame([row])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### 2.4.2. Seleccion del modelo optimizado"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "El modelo seleccionado, luego de la optimización es:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_algoritms_optimized.sort_values(by='Accuracy', ascending=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Sin embargo, un punto fuerte a destacar el la alta sensibilidad de la *Regresión Logística*. En este problema, nos parece bastante importante esta métrica, dado que es preferible llevar un paraguas a algún lugar y que no llueva, a que no llevar y que llueva... O sea, creemos que sería importante prestar atención a este punto. Es por eso que también opmizamos dicho caso, para luego probar en producción la satifacción de los \"clientes\" para ambos casos. Un modelo con una alta exactitud o un modelo con una álta sensibilidad."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### 2.4.2.1 Optimización de Regresión Logística"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "model = LogisticRegression(random_state=random_state, max_iter=10000)\n",
        "\n",
        "param_grid = {\n",
        "    'C': [0.01, 0.1, 1, 10, 100],\n",
        "    'solver': ['newton-cg', 'lbfgs', 'liblinear'],\n",
        "    'penalty': ['l2']\n",
        "}\n",
        "\n",
        "# Notar \"Recall\" como métrica a optimizar\n",
        "grid_search = GridSearchCV(estimator=model, param_grid=param_grid,\n",
        "                           cv=5, n_jobs=-1, verbose=2, scoring='recall')\n",
        "\n",
        "grid_search.fit(X_train_scaled, y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"Mejores parámetros:\", grid_search.best_params_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Evaluación\n",
        "model = grid_search.best_estimator_\n",
        "y_pred = model.predict(X_test)\n",
        "row = evaluate_predictions(y_test, y_pred, algorithm=\"LogisticRegression-Optimizado\")\n",
        "df_algoritms_optimized.loc[len(df_algoritms_optimized)] = row"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "pd.DataFrame([row])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Evaluación:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_algoritms_optimized.sort_values(by='Sensibility', ascending=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Guardamos los resultados optimizados\n",
        "timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
        "file_name = f'df_algoritms_optimized_{timestamp}.csv'\n",
        "\n",
        "os.makedirs('results', exist_ok=True)\n",
        "\n",
        "filepath = os.path.join('results', file_name)\n",
        "df_algoritms_optimized.to_csv(filepath, index=False)\n",
        "\n",
        "print(f'DataFrame guardado en {filepath}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aA8GlTHC6ygI"
      },
      "source": [
        "### 2.5 Despliegue del modelo (Model Deployment)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AZ3j_IftAKqp"
      },
      "source": [
        "🔮 Futuras versiones 🔮"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mb63TL_06ygI"
      },
      "source": [
        "### 2.6 Monitoreo y mantenimiento del modelo (Model Monitoring and Maintenance)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fsw3YH8nAKqp"
      },
      "source": [
        "🔮 Futuras versiones 🔮"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SBHtocWZ6ygK"
      },
      "source": [
        "## 3. Mejora continua"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fC19d38ZAKqp"
      },
      "source": [
        "> Mejoras planteadas en un futuro:\n",
        "> - Fases que faltaron implementar o mejorar\n",
        "> - Mejorar los procesos\n",
        "> - Mejorar la documentación de los procesos\n",
        "> - Mejorar las referencias\n",
        "> - Mejorar el archivo de código auxiliar (principalmente las improtaciones)\n",
        "> - Refactorizar el código actual en código auxiliar\n",
        "> - Tratamiento de valores a-típicos\n",
        "> - Mejorar documentos de salida. Refactorizar y especificar los documentos de salidas."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s9tJtWPx6ygL"
      },
      "source": [
        "## 4. Referencias"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pD9wt3NJAKqp"
      },
      "source": [
        "- https://en.wikipedia.org/wiki/List_of_extreme_temperatures_in_Australia\n",
        "- https://ml-ops.org/content/crisp-ml\n",
        "- https://www.researchgate.net/publication/369194767_A_Different_Traditional_Approach_for_Automatic_Comparative_Machine_Learning_in_Multimodality_Covid-19_Severity_Recognition\n",
        "- https://arxiv.org/pdf/2003.05155.pdf"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oJaF_tor6ygL"
      },
      "source": [
        "## 5. Apendices"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a-j8_w-wAKqp"
      },
      "outputs": [],
      "source": [
        "with open(\"utils.py\") as f:\n",
        "  print(f.read())"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
